---
title: 深度学习——XLNet
date: 2019-06-25 13:55:31
tags: [深度学习,面试,NLP]
---

​		2019年6月，Google最新推出XLNet在20个任务中超越了BERT，并且在18个任务上都取得了当前最佳效果。本文主要来探究XLNet究竟在Bert的基础上做了哪些改进才完成这么大的进化呢？



Bert打开了NLP领域两阶段模式的大门

> 两阶段模式：
>
> ​	1.预训练
>
> ​	2.FineTuning

​	

XLNet引入了自回归语言模型和自编码语言模型的提法，是一个很好的思维框架

##### 自回归语言模型Autoregressive LM

​	从左到右或从右到左的预测当前词，这种类型的LM被称为自回归语言模型。

​	**典型模型**：**GPT系列、EMLo**(虽然表面上看起来做了两个方向，但是本质上是分别于两个方向的自回归语言模型，然后将隐节点状态拼接到一起，来实现双向语言模型，仍是自回归语言模型)

​	**缺点**：只能利用上文或者下文的信息(虽然ELMo利用了上文和瞎问的信息，但是因为只是简单地隐节点状态拼接，效果差强人意)

​	**优点**：对于下游任务是**文本生成NLP**类(机器翻译、文本摘要)等，在实际内容生成的时候，就是从左到右的，自回归语言模型天然匹配这个过程。



##### 自编码语言模型Autoencoder LM

​	Bert**通过随机Mask掉一部分单词，然后预训练过程根据上下文单词来预测这些被Mask掉的单词**，这是经典的Denoising Autoencoder （DAE）思路，那些被Mask掉的单词就是在输入侧加入的所谓噪音,类似于Bert的这种训练预训练模式被称为**DAE LM**

​	**典型模型：Bert**

​	**优点**：能利用上下文的信息

​	**缺点**：1.对于文本生成类NLP任务效果不好（因为文本生成类任务本身就是单向的任务）。

​				2.第一个预训练阶段因为采取引入 [Mask] 标记来 Mask 掉部分单词的训练模式，而 Fine-tuning 阶段是看不到这种被强行加入的 Mask 标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失

​				3.在预训练截断，Bert假设句子中的多个单词被Mask掉的单词之间没有任何联系、条件独立，这显然是不一定成立的



### XLNet

Bert的主要改进在下面的三个部分：

>1.在自回归模型上引入了双向语言模型
>
>2.引入了Transformer-XL的主要思路：**相对位置编码以及分段RNN机制**(长文档效果提升核心因素)
>
>3.加大预训练使用的数据集

​	XLNet主要针对Bert中第二个缺陷，	



#### 在自回归语言模型中引入双向模型

​	为解决Mask标记两阶段不一致的问题，XLNet打算采用在自回归语言模型中引入双向语言模型来进行解决。目标为**看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息**。

​	**解决方式：**

​	首先仍然采用双阶段模式，第一阶段为语言模型预训练，第二阶段为任务数据Fine-tuning。它主要改动的是第一截断——语言模型预训练截断，希望不再采用Bert那种带Mask标记的DAE LM模式，而是采用自回归语言模型，看上去是个标准的从左向右过程，Fine-tuning 当然也是这个过程，于是两个环节就统一起来。

​	**MLNet解决该问题的核心思路为：在预训练阶段，引入Permutation Language Model (时序语言模型)的训练目标**。

>​	就是说，比如包含单词 Ti 的当前输入的句子 X ，由顺序的几个单词构成，比如 x1,x2,x3,x4 四个单词顺序构成。我们假设，其中，要预测的单词 Ti 是 x3 ，位置在 Position 3 ，要想让它能够在上文 Context_before 中，也就是 Position 1 或者 Position 2 的位置看到 Position 4 的单词 x4 。
>
>​	可以这么做：假设我们固定住 x3 所在位置，就是它仍然在 Position 3 ，之后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入 X 。比如随机排列组合后，抽取出 x4,x2，x3,x1 这一个排列组合作为模型的输入 X 。于是，x3 就能同时看到上文 x2 ，以及下文 x4 的内容了,这就是 XLNet 的基本思想

​	具体实现：

> ​	**XLNet 采取了 Attention 掩码的机制**（一个掩码矩阵），你可以理解为，当前的输入句子是 X ，要预测的单词 Ti 是第 i 个单词，前面1到 i-1 个单词，**在输入部分观察，并没发生变化，该是谁还是谁**。**但是在 Transformer 内部，通过 Attention 掩码，从 X 的输入单词里面，也就是 Ti 的上文和下文单词中，随机选择 i-1 个，放到 Ti 的上文位置中，把其它单词的输入通过 Attention 掩码隐藏掉，于是就能够达成我们期望的目标**（当然这个所谓放到 Ti 的上文位置，只是一种形象的说法，其实在内部，就是通过 Attention Mask ，把其它没有被选到的单词 Mask 掉，不让它们在预测单词 Ti 的时候发生作用，如此而已。**看着就类似于把这些被选中的单词放到了上文 Context_before 的位置**,论文中采用**双流自注意力机制**来进行具体实现



> 双流自注意力机制
>
> ​	1.内容注意力    标准的transfomer计算过程
>
> ​	2.Query流自注意力   这里并不是很懂



**XLNet效果好的核心因素：**

> 1.在**自回归模式下引入和双向语言模型**。
>
> 2.引入了Transformer-XL的主要思路：**相对位置编码以及分段RNN机制**(长文档效果提升核心因素)
>
> 3.加大预训练使用的数据集





**XLNet和Bert对比**

>1.预训练过程不同
>
>**尽管看上去，XLNet在预训练机制引入的Permutation Language Model这种新的预训练目标，和Bert采用Mask标记这种方式，有很大不同。其实你深入思考一下，会发现，两者本质是类似的**。区别主要在于：**Bert是直接在输入端显示地通过引入Mask标记**，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，**要求利用上下文中其它单词去预测某个被Mask掉的单词**；**而XLNet则抛弃掉输入侧的Mask标记，通过Attention Mask机制，在Transformer内部随机Mask掉一部分单词（**这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），**让这些被Mask掉的单词在预测某个单词的时候不发生作用**。所以，本质上两者并没什么太大的不同，**只是Mask的位置，Bert更表面化一些，XLNet则把这个过程隐藏在了Transformer内部而已**。这样，就**可以抛掉表面的[Mask]标记，解决它所说的预训练里带有[Mask]标记导致的和Fine-tuning过程不一致的问题**
>
>2.XLNet坚持了自编码LM的从左到右的方式，因此XLNet在文本生成类任务上效果要比Bert好
>
>3.XLNet引入了Transfomer XL的机制，因此对于长文本效果比Bert更好



##### XLNet在NLP各个领域中效果情况

> 1.对于阅读理解任务，效果有极大幅度的提升
>
> 2.**长**文档类任务，性能大幅度提升
>
> 3.综合型NLP任务，有所提升
>
> 4.文本分类和信息检索任务，有所提升，但幅度不大
>
> 总结：主要是长文档任务提升比较明显，其他类型的任务提升不大





