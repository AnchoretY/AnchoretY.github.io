---
title: 深度学习——损失函数
date: 2018-10-30 20:43:33
tags: [深度学习,面试]
mathjax: true
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
在机器机器学习和深度学习中有许多常见的损失函数，主要包括：

​		1.平方差函数MSE（Mean Squared Error）

​		2.交叉熵函数（Cross Entory）	

> **损失函数选择的方法：1.线性模型中使用平方误差函数，深度学习使用交叉熵函数**
>
> ​	**2.平方误差损失函数更适合输出为连续,并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景**。

#### 线性模型

​	**效果较好的损失函数：平方误差损失函数**

​	**计算公式：**

​	![](https://github.com/AnchoretY/images/blob/master/blog/MSE定义公式.png?raw=true)

​		其中，y是我们期望的输出，a是神经元的实际输出a=σ(Wx+b)

​	**损失函数求导：**	

​		![](https://github.com/AnchoretY/images/blob/master/blog/MES损失函数反向传播公式.png?raw=true)

​		这也就是每次进行参数更新量的基数，需要再乘以学习速率

>  为什么深度学习中很少使用MSE作为损失函数？
>
> ​	当使用MSE作为损失函数时，有上面求导后的公式可以明显的看出，每次的参数更新量取决于σ′(z) ，由Sigmod函数的性质可知，σ′(z) 在 z 取大部分值时会取到一个非常小的值，因此参数更新会异常的缓慢

​	

#### 深度学习

​	**效果最好的损失函数：交叉熵函数**

​	**计算公式：**

![](https://github.com/AnchoretY/images/blob/master/blog/交叉熵公式.png?raw=true)

​	如果有多个样本，则整个样本集的平均交叉熵为:

​	![](https://github.com/AnchoretY/images/blob/master/blog/交叉熵公式2.png?raw=true)

对于二分类而言，交叉损失函数为：

​	![](https://github.com/AnchoretY/images/blob/master/blog/二分类交叉熵损失函数.png?raw=true)



损失函数求导：

​	![](https://github.com/AnchoretY/images/blob/master/blog/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC.png?raw=true)		

​	对于b的求导同理。

​	我们可以看出，**交叉熵作为损失函数，梯度中的σ′(z) 被消掉了，另外σ(z)-y就是输出值和真实值之间的误差，误差越大，梯度更新越大，参数更新越快。** 

#### Softmax损失函数

##### softmax函数

​	softmax用于多分类过程中，将多个神经元的输出映射到(0，1)区间，可以看做被分为各个类的概率。

![](https://github.com/AnchoretY/images/blob/master/blog/softmax损失函数.png?raw=true)

​	其中，

![](https://github.com/AnchoretY/images/blob/master/blog/softmax神经元输入.png?raw=true)



##### softmax求导相关推导

![](https://github.com/AnchoretY/images/blob/master/blog/神经网络传导图.png?raw=true)

​	对于使用作为激活函数的神经网络，最终只输出只有最大的softmax最大的项为1其余项均为0，假设yj=1，带入交叉熵公式中得

​				$$ Loss=-y_{i}loga_i $$

​	去掉了累加和，因为只有一项y为1，其余都为0，而将yj=1带入得

​       $$ Loss=-loga_i $$			

​	下面我们准备将损失函数对参数求导，参数的形式在该例子中，总共分w41,w42,w43,w51,w52,w53,w61,w62,w63.这些，那么比如我要求出w41,w42,w43的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可，举个例子此时求w41的偏导为:

​				$$\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}$$

​	其中右边第一项q求导为：

​			 $$\frac{\partial Loss}{\partial a_j} = -\frac{1}{a_j}$$

​	右边第三项求导为：

​			  $$\frac{\partial z_j}{\partial w_ij} = x_{i}$$

​	核心是求右侧第二项：$\frac{\partial a_j}{\partial z_j}$，这里我们分两种情况进行讨论

![](https://github.com/AnchoretY/images/blob/master/blog/softmax求导.png?raw=true)

​	将前两项的结果进行连乘得：

​		![](https://github.com/AnchoretY/images/blob/master/blog/softmax求导2.png?raw=true)

​	而对于分类问题，只会有一个$y_i$为1，其余均为0，因此，对于分类问题：

​		![](https://github.com/AnchoretY/images/blob/master/blog/softmax%E6%B1%82%E5%AF%BC3.png?raw=true)

​	最终：

​		$$\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}==(a_{i}-y{i})x{i}$$