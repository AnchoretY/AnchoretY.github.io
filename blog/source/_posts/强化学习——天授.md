---
title: 强化学习——天授
copyright: true
mathjax: true
date: 2021-03-18 14:54:51
tags:
categories:
---

概述：首页描述

![]()

<!--more-->

### 框架结构

<img src="https://raw.githubusercontent.com/AnchoretY/images/master/blog/image.p999qm4p7xn.png" alt="image" style="zoom:50%;" />

#### Buffer

&emsp;&emsp;在tianshou中的Buffer本质上就是传统方法中即Memory，用于存储最近学习过程中一定数量的信息，其中的信息包括：

> - `obs` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻的观测值；
> - `act` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻采取的动作值；
> - `rew` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻环境返回的奖励函数值；
> - `done` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻是否结束这个episode；
> - `obs_next` ![t+1](https://tianshou.readthedocs.io/zh/latest/_images/math/15e200352238591033cc5f6ff4e508d9e54eed42.png) 时刻的观测值；
> - `info` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻环境给出的额外信息（gym.Env会返回4个东西，最后一个就是它）；
> - `policy` ![t](https://tianshou.readthedocs.io/zh/latest/_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png) 时刻由policy计算出的需要额外存储的数据；



#### Collector



> - `max_epoch`：最大允许的训练轮数，有可能没训练完这么多轮就会停止（因为满足了 `stop_fn` 的条件）
> - `step_per_epoch`：每个epoch要更新多少次策略网络
> - `collect_per_step`：每次更新前要收集多少帧与环境的交互数据。上面的代码参数意思是，每收集10帧进行一次网络更新
> - `episode_per_test`：每次测试的时候花几个rollout进行测试
> - `batch_size`：每次策略计算的时候批量处理多少数据
> - `train_fn`：在每个epoch训练之前被调用的函数，输入的是当前第几轮epoch和当前用于训练的env一共step了多少次。上面的代码意味着，在每次训练前将epsilon设置成0.1
> - `test_fn`：在每个epoch测试之前被调用的函数，输入的是当前第几轮epoch和当前用于训练的env一共step了多少次。上面的代码意味着，在每次测试前将epsilon设置成0.05
> - `stop_fn`：停止条件，输入是当前平均总奖励回报（the average undiscounted returns），返回是否要停止训练

> - `writer`：天授支持 [TensorBoard](https://www.tensorflow.org/tensorboard)，可以像下面这样初始化：
>
>   ```
>   from torch.utils.tensorboard import SummaryWriter
>   writer = SummaryWriter('log/dqn')
>   ```

&emsp;&emsp;返回结果为`字典`：

```python
{
    'train_step': 9246,
    'train_episode': 504.0,
    'train_time/collector': '0.65s',
    'train_time/model': '1.97s',
    'train_speed': '3518.79 step/s',
    'test_step': 49112,
    'test_episode': 400.0,
    'test_time': '1.38s',
    'test_speed': '35600.52 step/s',
    'best_reward': 199.03,
    'duration': '4.01s'
}
```





#### Trainer

 **标准训练器使用**





 **自定义训练训练器**

&emsp;&emsp;在tianshou中Trainer只使用了很少的封装，用户可以很容易的进行自定义自己的训练策略，例如：

```python
# 在正式训练前先收集5000帧数据
policy.set_eps(1)
train_collector.collect(n_step=5000)

policy.set_eps(0.1)
for i in range(int(1e6)):  # 训练总数
    collect_result = train_collector.collect(n_step=10)

    # 如果收集的episode平均总奖励回报超过了阈值，或者每隔1000步，
    # 就会对policy进行测试
    if collect_result['rew'] >= env.spec.reward_threshold or i % 1000 == 0:
        policy.set_eps(0.05)
        result = test_collector.collect(n_episode=100)
        if result['rew'] >= env.spec.reward_threshold:
            print(f'Finished training! Test mean returns: {result["rew"]}')
            break
        else:
            # 重新设置eps为0.1，表示训练策略
            policy.set_eps(0.1)

    # 使用采样出的数据组进行策略训练
    losses = policy.learn(train_collector.sample(batch_size=64))
```







DQNPolicy(

model :DQN组成的基础模型

optim：模型优化算法

discount_factor=0.9,  奖励衰减率

estimation_step=3 :更新的窗口

target_update_freq：记忆体更新频率，也就是记忆体的大小

)

























##### 参考文献

- xxx
- xxx