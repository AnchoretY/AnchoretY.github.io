---
title: 深度学习训练基本经验
date: 2018-11-26 10:55:16
tags:
---



1.**在各个隐藏层和激活函数之间加入Batch Normalization层**可以大大缩短训练时间，而且还存在隐藏效果，比如出现还可以改善效果。

> 调用：
>
> ​	Normalization(num_features)
>
>  参数设置：
>
> ​	CNN后接Batch Normalization:  nums_feeatures为CNN感受野个数(即输出深度)
>
> ​	全连接层后接Batch Normalization：num_features为输出的特征个数



2.**Batch Normalization和Dropout层不要一起使用，因为BN以及具备了dropout的效果**，一起使用不但起不到效果，而且会产生副作用

> 常见副作用:
>
> ​	1.**只能使用特别小的速率进行训练**，使用较大的速率进行训练时，出现梯度消失，无法进行下降
>
> ​	2.最终与训练集拟合程度不高，例如与训练集的拟合程度只能达到90%



> 若一定要将dropout和BN一起使用，那么可以采用下面方式：
>
> ​	1.将dropout放在BN后面进行使用
>
> ​	2.修改Dropout公式(如高斯Dropout)，使其对对方差不那么敏感
>
> 总体思路:降低方差偏移



**3.深度学习不收敛问题**

> ​	1.最常见的原因可能是由于学习速率设置的过大，这种情况一般先准确率不断上升，然后就开始震荡
>
> ​	2.当训练样本较小，而向量空间较大时，也可能会产生不收敛问题，这种情况一般从一开始就开始震荡，机会没有准确率上升的过程
>
> ​	3.训练网络问题。当面对的问题比较复杂，而使用的网络较浅时，可能会产生无法收敛问题
>
> ​	4.数据没有进行归一化。数据输入模型之前如果没有进行归一化，很有可能会产生收敛慢或者无法进行收敛的问题

**注意：收敛与否主要是看损失函数是否还在下降，而不是准确率是否还在上升，存在很多情况损失函数在迭代过程中还是在不断地下降，但是准确率基本上处于停滞状态，这种情况也是一种未完全拟合的表现，经过一段时间损失函数的下降后准确率还可能会迎来较大的提升**