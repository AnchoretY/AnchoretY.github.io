---
title: 深度学习——transformer XL
date: 2019-06-25 15:35:46
tags:
---

​		transformer XL主要用来解决Transformer对于长文档NLP任务不够友好的问题。

### 原始的Transformer

原始Transformer存在的缺陷：

> 1.算法无法建模超过固定长度的依赖关系。
>
> 2.被分割的句子通常不考虑句子边界，导致**上下文碎片化**

​	在给定无限内存和计算资源的情况下，Trnasformer为了将任意长度的上下文融入模型，**可以无条件的处理整个上下文片段，但在实际情况下由于资源的限制，这显然是行不通的**。

​	在实际使用中一种**常见的近似方法**为**将整个语料库分割可管理大小的更短的片段(这就是多头)**，只在每个片段中训练模型，忽略其他段，我们称之为原始Transformer(vanilla model)。

​	在评估过程中，原始Transformer模型在每个步骤消耗与训练期间相同长度的segment，但在最后一个位置只预测一次。然后，在下一步中，这个segment只向右移动一个位置，新的segment必须从头开始开始处理，虽然解决了利用较长的上下文的问题和上下文碎片化的问题，但是**评估的资源消耗过大(时间、计算)**



### Transformer XL

Transformer XL优势：

> 可以在不破坏时间一致性的情况下学习固定长度以外的依赖

核心改进：

> **1.segment-level 的递归机制-->解决固定长度上下文局限**
>
> **2.新的位置编码**

实验条件下效果对比原始transformer效果提升情况：

> 1.在长序列和短序列都获得更好的性能
>
> 2.在长依赖上的提升十分明显
>
> 3.在速度上比原始的Transformer快了1800倍

#### Segment-level的递归机制

​	在训练过程中，对上一个 segment 计算的隐藏状态序列进行修复，并在模型处理下一个新的 segment 时将其缓存为可重用的扩展上下文。**种递归机制应用于整个语料库的每两个连续的 segment，它本质上是在隐藏状态中创建一个 segment-level 的递归。因此，所使用的有效上下文可以远远超出两个 segments。**

​	**该方式除了实现超长的上下文和解决碎片问题外，这种递归方案的另一个好处是显著加快了评估速度。**



#### 相对位置编码

​	如果直接使用Segment-level recurrence是行不通的，因为当我们重用前面的段时，位置编码是不一致的。例如：考虑一个具有上下文位置[0,1,2,3]的旧段。当处理一个新的段时，我们将两个段合并，得到位置[0,1,2,3,0,1,2,3]，其中每个位置id的语义在整个序列中是不连贯的。

​	为此Transformer XL提出一种新的相当位置编码使递归成为可能。与其他相对位置编码方案不同，我们的公式**使用具有learnable transformations的固定嵌入**，而不是earnable embeddings，因此在测试时**更适用于较长的序列。**

​	循环机制引入了新的挑战——原始位置编码将每个段分开处理，因此，来自不同段的表征会具有相同的位置编码。例如，第一和第二段的第一个表征将具有相同的编码，虽然它们的位置和重要性并不相同（比如第一个段中的第一个表征可能重要性低一些）。这种混淆可能会错误地影响网络。

​	针对此问题，论文提出了一种新的位置编码方式。这种位置编码是每个注意力模块的一部分。它不会仅在第一层之前编码位置，而且会基于表征之间的相对距离而非绝对位置进行编码。从技术上讲，它对注意力头分数（Attention Head’s Score）的计算方式不再是简单的乘法（Qi⋅Kj），而是包括四个部分：

1. 内容权重——没有添加原始位置编码的原始分数。
2. 相对于当前内容的位置偏差（Qi）。该项使用正弦类函数来计算表征之间的相对距离（例如 i-j），用以替代当前表征的绝对位置。
3. 可学习的全局内容偏差——该模型添加了一个可学习的向量，用于调整其他表征内容（Kj）的重要性。
4. 可学习的全局偏差——另一个可学习向量，仅根据表征之间的距离调整重要性（例如，最后一个词可能比前一段中的词更重要）。



https://www.tuicool.com/articles/iQjEF3Y