---
title: 深度学习——词向量表示之word2vec
date: 2019-04-12 09:39:34
tags: [深度学习,NLP,面试]
---



原始的神经网络语言模型：里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层），**里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值**

![](https://github.com/AnchoretY/images/blob/master/blog/word2vec架构图.png?raw=true)



#### Word2Vec对原始语言模型的改进：

> 1.**对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。**
>
> 比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。
>
> **2.word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射（Hierarchical Softmax）。这样隐藏层到输出层的softmax不是一步完成的，而是沿着哈弗曼树一步一步完成的。**



### Hierarchical Softmax

​	 和之前的神经网络语言模型相比，我们的霍夫曼树的**所有内部节点就类似之前神经网络隐藏层的神经元**,其中，**根节点的词向量对应我们的投影后的词向量**，而所有**叶子节点就类似于之前神经网络softmax输出层的神经元**，**叶子节点的个数就是词汇表的大小**。

#### 使用Hierarchical Softmax的好处

> 1.由于是二叉树，之前计算量为V,现在变成了log2V
>
> 2.由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。



#### 算法过程	

**STEP 1：扫描语料库，统计每个词出现的频数，保存在一个hash表中**



**STEP2：根据个词的词频建立哈弗曼树**

* 最终每个词汇都是哈弗曼树的叶子节点，词频就是相应的权值

* 根节点对应的词向量就是我们投影后的词向量
* 而所有叶子节点就类似神经网络softmax输出层的神经元，叶子节点个数就是词汇表大小
* 非叶子节点代表某一类词
* 哈弗曼树建立好后每个词都会有一个二进制的哈弗曼编码

**STEP3：初始化词向量和哈弗曼树非叶子节点的向量**

​	向量维度是我们给定的参数K。

**STEP4：训练，也就是通过梯度下降算法不断优化词向量**

​	在初始化后的词向量，回到语料库，逐句读取一系列的词，然后用梯度下降算法算法算出梯度，更新词向量的值、非叶子检点的值。(哈弗曼树就相当于一个优化后的神经网络)



#### 参数更新过程









### 基于Negative Sampling的Word2vec

**Hierarchical Softmax的的缺点**：

​	对于生僻词需要在哈弗曼树中向下走很久。

#### Negative Sampling算法

​	Negative Sampling不再使用(复杂的Huffman树），而是**利用相对简单的随机负采样**，能大幅度提升性能，因此，将其作为Hierarchical softmax的替代方案

​	**核心思想**：**通过负采样将问题转化为求解一个正例和neg个负例进行二元回归问题**。每次只是通过采样neg个不同的中心词做负例，就可以训练模型

​	**方法：**我们有一个训练样本，中心词是w,它周围上下文共有2c个词，记为context(w)。由于这个中心词w,的确和context(w)相关存在，因此它是一个真实的正例。**通过Negative Sampling采样，我们得到neg个**和w不同的中心词wi,i=1,2,..neg，这样context(w)和wi就组成了neg个**并不真实存在的负例**。**利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词wi对应的模型参数θi，和每个词的词向量**。

​	**本质上是对训练集进行了采样，从而减小了训练集的大小。**



#### Negative Sampling负采样方法

![](https://github.com/AnchoretY/images/blob/master/blog/负采样算法.png?raw=true)

![](https://github.com/AnchoretY/images/blob/master/blog/负采样算法2.png?raw=true)

**3、 word2vec负采样有什么作用？**

1**.加速了模型计算**，模型每次只需要更新采样的词的权重，不用更新所有的权重

2.**保证了模型训练的效果**，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系







### 常见问题

**1.skip gram和cbow各自的优缺点**

> ​	**(1) cbow的速度更快，时间复杂度为O(V)，skip-gram速度慢,时间复杂度为O(nV)**
>
> ​	在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V);
>
> ​	而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。可以看出，skip-gram进行预测的次数是要多于cbow的：因为**每个词在作为中心词时，都要使用周围每个词进行预测一次**。**这样相当于比cbow的方法多进行了K次（假设K为窗口大小）**，因此时间的复杂度为O(KV)，训练时间要比cbow要长。
>
> ​	**(2)当数据较少或生僻词较多时，skip-gram会更加准确；**
>
> ​	在**skip-gram当中，每个词都要收到周围的词的影响**，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为**尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已**。





**2.Negative Sampling和Hierarchical softmax各自的优缺点**

> **Hierarchical softmax**
>
> **优点：**
>
> ​	1.由于是二叉树，之前计算量为V,现在变成了log2V，**效率更高**
>
> ​	2.由于使用霍夫曼树是高频的词靠近树根，这样**高频词需要更少的时间会被找到**。
>
> **缺点:**
>
> ​	对于**生僻词在hierarchical softmax中依旧需要向下走很久**
>
> **Negative Sampling**
>
> **优点：**
>
> ​	1.对于低频词的计算效率依然很高



​		

**3.word2vec的缺点**

> 1.使用的只是局部的上下文信息，对上下文的利用有限
>
> 2.和glove相比比较难并行化



​	



**4、word2vec和fastText对比有什么区别？（word2vec vs fastText）**

> 1）都可以无监督学习词向量， **fastText训练词向量时会考虑subword**；
>
> 2）fastText还可以进行有监督学习进行文本分类，其主要特点：
>
> - 结构与CBOW类似，但学习目标是人工标注的分类结果；
> - 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；
> - 引入N-gram，考虑词序特征；
> - 引入subword来处理长词，处理未登陆词问题；







参考文献：[基于Negative Sampling的模型](https://www.cnblogs.com/pinard/p/7249903.html)

[ 基于Hierarchical Softmax的模型](https://www.cnblogs.com/pinard/p/7243513.html)