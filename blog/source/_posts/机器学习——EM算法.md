---
title: 机器学习——EM算法
date: 2019-04-04 09:57:41
tags: [面试,算法总结]
---

### 准备知识

####1.参数估计的方法

概率模型的参数估计分为两大类：

> 1.不含隐变量的参数估计—极大似然估计/贝叶斯估计法
>
> 2.含隐变量的参数估计—EM算法



#### 2.jensen不等式

**X是一个随机变量，f(X)是一个凸函数（二阶导数大或等于0），那么有：**

![](https://github.com/AnchoretY/images/blob/master/blog/jensen不等式.png?raw=true)

**当且仅当X是常数的时候等号成立**

**如果f（X）是凹函数，不等号反向**



#### 3.先验概率、后验概率、条件概率

​	**先验概率：P(Y)**   

> 先验概率是只根据事情之前发生各个结果出现情况估计的概率(无关特征)

​	**后验概率：P(Y|X)**

> 后验概率是在各个X的分布下各个Y出现的概率(特征符合这个X时Y为这个的概率)

​	**条件概率：P(X|Y)**

> 条件概率是在结果某一种情况时X出现这种分布的概率

#### 4.自信息、互信息

​	**自信息：I(x) = -logp(x)**

​	概率是衡量确定性的度量，那么**信息是衡量不确定性的度量**.越不确定信息量越高。

​	**互信息：I(x;y) = log(p(x|y)/p(x))**

​	已知y，x的不确定性减少量(其值可正可负)

#### 5.熵

​	**对随机变量平均不确定性的度量，**一个系统越有序，信息熵越低。

​	熵的另一种解读也就是**自信息的期望**

​		**H(X) = E[I(X)] = ∑P(x)I(x) = -∑p(x)logp(x)**

#### 6.条件熵

​	在给定y条件下，x的条件自信息量为I(x|y)，X的集合的条件熵为

![](https://github.com/AnchoretY/images/blob/master/blog/条件熵1.png?raw=true)

​	进一步在给定Y（各个y）的条件下，X集合的条件熵：

​		![](https://github.com/AnchoretY/images/blob/master/blog/条件熵2.png?raw=true)

​	也就是在**联合符号集合上的条件自信息量两个概率的加权平均**	

#### EM算法

​	EM算法主要用于求解概率模型的**极大似然估计**或**极大后验概率**。EM算法是通过**迭代求解**观测数据**对数似然函数L(θ) = logP(Y|θ)的极大化**，实现参数估计的。

> 每次迭代主要分为E、M两步：
>
> ​	E步：求期望。即求log(P，Z|θ)关于P(Z|Y，θi)的期望
>
> ![](https://github.com/AnchoretY/images/blob/master/blog/M步公式.png?raw=true)
>
> (各个隐变量可能的概率下乘以出现这种结果的总和)
>
> ​		
>
> ​	M步：极大化Q函数得到新的参数θ
>
> ![](https://github.com/AnchoretY/images/blob/master/blog/E步公式.png?raw=true)
>
> ​	在构建具体的EM算法时，最重要的时定义Q函数，每次迭代中，Em算法通过极大似然化Q函数来增大对数似然函数L(θ)

##### 算法推导













**注意：1.EM算法在每次迭代后均能提高观测数据的似然函数值**

​	**2.EM算法不能保证全局最优，只能保证局部最优，因此算法受初值的影响**

​	**3.EM算法可以用于无监督学习**