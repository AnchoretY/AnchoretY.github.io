---
title: 阿里2019年最新论文-定位然后检测恶意攻击
date: 2019-09-04 20:55:58
tags: [安全,论文阅读]
---

论文名称:《Locate-Then-Detect: Real-time Web Attack Detection via Attention-based
Deep Neural Networks》

主要针对的攻击类型:sql、xss

采用的方式:先定位攻击载荷在进行恶意检测



内容解读：

​	主要分为两阶段网络

> PLN(Payload Locat-ing Network):在整个url、post中定位到关键部分，去掉无用信息
>
> PCN(Payload Classification Network):利用PLN网络得到的关注度信息进行分类



## PLN

​	**目标**：

​	**输入**:固定长度的请求输入文本

​	**输出:**区域位置和可疑置信度

​	**核心思想**：图像分割的思想

> PLN网络要进行单独的训练，然后加到PCN网络之前，固定参数值(我的理解)

#### request请求编码

​	首先设置一个最大长度L，然后进行字符级别的embedding，即每个字符都转化成一个对应的k维Embbeding向量，最终输出为：L*K维向量

> 这里的最大长度法和我们之前的方法类似，直接进行长度限制，忽略了在超长的正常参数尾部追加恶意payload形式的攻击	

#### 特征提取

​	模型：Xception

> Xception模型
>
> ​	先进行普通卷积操作，再对 1×1 卷积后的每个channel分别进行 3×3 卷积操作，最后将结果 concat
>
> ![](https://github.com/AnchoretY/images/blob/master/blog/Xception模型.png?raw=true)

​	加速计算：thin feature maps with small channel(不损失很大精度的前提下显著提升速度)

#### 模型部分

​	沿着特征图滑动几个mini-networks来检测可以片段，该网络采样特征图一个n*m的窗口，在mini-network层之后经过两个1\*m并列的层——区域回归层和区域分类层

> 为了保证保持嵌入张量中这些向量的语义完整性，我们令m等于字符向量的嵌入大小。

reg层输出坐标：(p,2p)有效载荷的开始位置和结束位置

cls层：输出每个区域的得分

对于输入特征图为W*H的，将会有H\*P个区域



并不是所有区域都是有效的，

##### 区域的标注

区域标注为积极标签的方法为:

> 1.将用于最大的交集序列（Ios）的区域标为积极
>
> 2.将交集序列的值（Ios）大于0.5的值定位积极

区域标注为消极标签:

> 将交集序列的值小于0.2的标为消极序列

​	如果既没有标为消极也没有标为积极，那么则忽略该区域。一般情况下消极区域的数量远大于积极区域，如果消极区域和积极区域的比例大于3：1，那么将其归置到3：1。

##### PLN层的损失函数：

![](https://github.com/AnchoretY/images/blob/master/blog/PLN损失函数.png?raw=true)

​	参数意义：

> i：区域的编号
>
> li:区域的label，积极区域为1，否则为0
>
> posi、pos∗i :分别代表了区域的开始位置和结束位置
>
> Lcls：是区域的分类对数损失函数，
>
> Lreg: 是积极区域的回归损失函数，不关注负样本，该回归损失函数采用：
>
> ![](https://github.com/AnchoretY/images/blob/master/blog/smooth-L1损失函数.png?raw=true) 
>
> ​	x表示区域真实标签和预测值之间的差距
>
> λ：控制损失函数的前后两个部分的重要性，本文中采用的是1.0
>
> Ncls: 本文中设置为mini-batch 大小
>
> Nreg:本文设置为区域个数，

#### 数据标注

​	在整个LTD模型结构中，需要大量的标注数据，本文提出了基于HMM的异常检测系统来辅助序列标注，该系统通过大量的HMM模型来实现，每个host的每个url的参数值都会训练一个hmm模型，检测到的异常参数经过规则检测系统确定为xss或sql会标记起始和结束位置。

​	**作用:表示有效payload位置**

​	**方法：参数hmm+规则系统**

> 实例：
>
> ​	uri1 = /a.php?id=1&name=1’ and 1=1
>
> 首先提取各个参数的值，得到
>
> ​	{val1 : 1, val2 : 1′ and 1 = 1}
>
> 使用hmm参数异常检测模型确定是否存在异常参数值
>
> ​	val2是异常的参数值
>
> 使用规则模型判别该参数为sql注入，定位位置，标记异常区域
>
> ​	 [Start (17), End (27), Label (1)]





## PCN

​	目标:对PLN层定位的可疑区域，在PCN部分进行深入的分析，找到攻击的区域，

​	输入：PLN中得分最高的三个区域(最可疑)

​	输出: 是否为攻击以及攻击类型

​	核心思想：采用CNN进行文本分类

#### 具体做法

> 采用5层不同大小的卷积核，并且每个卷积核后都会带一个max-overtime pooling operation ，不同的卷积核大小保证了PCN能够精确地识别具有多种特征的攻击。这些特征被连接起来，在连接在层线性层，最后使用softmax输出是各种攻击的可能性

#### 损失函数

​	PCN部分的损失函数就是标准的交叉熵损失函数加上一个L1正则化项：

![](https://github.com/AnchoretY/images/blob/master/blog/PCN.png?raw=true)

该层主要是一个文本分类的层，和PCN层共享相同的Embedding向量，输出给定区域是否为恶意以及攻击类型



### 数据产生方法

> 1.首先使用传统的WAF找出正常流量
>
> 2.构造sql、xss的payload参数值随机换到正常流量的参数值部分



### 实验结果

#### 1.CSCI

​	CSCI 2010数据集包含针对电子商务Web应用程序生成的流量，该数据集包含25,000多个异常请求和36,000个良性请求，使用其中2,072 SQLi和1,502 XSS样本作为黑样本，其他的正常流量和攻击流量统一标记为白样本。

![](https://github.com/AnchoretY/images/blob/master/blog/CSCI数据集实验对比.png?raw=true)

​	LTD与RWAF相比，在精确率吧和召回率方面均要好。LTD和Libinjection都具有100%的精确率，但是LTD拥有更高的召回率。

#### 2.真实流量

数据来源

​	300w条真实流量数据，其中包括38600个sql注入和xss攻击实例。	

#### Part 1  模型优越性的证明

![](https://github.com/AnchoretY/images/blob/master/blog/真实流量实验结果对比.png?raw=true)

​	其中，

​	**1.LTD获得了最高的精确率，HMM-Web获得了最高的召回率，但是它的误报率过高**，在在真实的WAF应用中，误报率必须少于0.01%。

​	**分析：**在该实验中，HMM-Web方式之所以比LTD获得了更加高的准确率，是因为HMM-Web所采用的方式是基于异常检测的方式，只要是之前没有见过的流量都会被判别为恶意。但这种HMM异常检测的缺陷也非常的明显，每当有系统更新时，HMM-web模型都需要重新进行训练，因此HMM-web并不是一个很好的实时web入侵检测方式。

> 对于对于Web攻击检测，在误报和召回之间存在权衡，而低误报是生产环境中的先决条件。因为高误报会造成用户正常访问的阻塞

​	**2.Libinjection和LTD都获得了100%的精确率，但LTD的召回率达到了99.8%，而Libinjection只有71%。**下面是一些Libinjection误分类而LTD分类正确分类的样本：

![](https://github.com/AnchoretY/images/blob/master/blog/Libinjection和LTD评判结果比较.png?raw=true)

​	**分析：**这里的解释有点没太看懂，好像有点和上表对不上，大致意思是说Libinjection过分依赖指纹库，进行微小的改变都很难进行检测，而且由于有些正常流量可能偶尔也会出现指纹库中的部分内容，因此很容易误报

​	**3.LTD比RWAF方式准确率和召回率都好。**

#### Part2 PLN部分有效性的证明

实验组1：LTD

实验组2 ：VPCN,把url参数部分却分为key-value形式，LTD去掉PLN部分只留下PCN部分进行分类

*个人看法：这里我个人觉得对比试验有点问题，因为直接用PCN部分进行分类不一定非要进行参数切分，因此这里使用切与不切分进行对比，证明LTD效率更高个人认为不成立，应该使用直接使用PCN进行对原始embedding后的内容进行分类*

##### 1.效率上

![](https://github.com/AnchoretY/images/blob/master/blog/PLN效率增强实验.png?raw=true)	

​	在有GPU的的环境下，带PLN的网络比不带的快6倍，没有GPU的环境下快了8倍。

​	分析：LTD之所以效率高的多是因为不使用PLN，直接参数个数过多，27.5的Url有13个参数以上，切分参数需要花费大量的时间，在真实流量中，包含参数个数可能更多。另一方面，一些开发者因为某些原因重新模块来隐藏参数，在这种情况下，基于规则的计算需要更加复杂的计算来提取该值。**与传统的方法相比，LTD通过限制检测区域来加快计算效率，另一方面也避免了参数重写造成的切割效率问题**

##### 2.准确率

​	**对照组**：典型的char级cnn从原始请求进行分类

​	数据集来源：

​		训练集：真实流量中320w正常流量，80w攻击样本

​		测试数据集：10w条不同时间的正常流量数据，在其中选择10000个样本随机将其中一个参数的值替换为SQLi、XSS的攻击载荷，形成恶意样本，其他的为正常样本

![](https://github.com/AnchoretY/images/blob/master/blog/LTD和charcnn对比.png?raw=true)

​	经过实验，明显可以看出，**直接的CNN的误报率和漏报率比LTD都要高得多**，而这时因为一般payload的长度都很短，而url请求的长度很长。某些已知攻击的payload长度最短可以为6个字符，而这些很短的payload就可以隐藏在很长的背景字符串之中，导致CNN很难学到恶意payload，而LTD中的PLN模块能通过过滤不相关部分来发现隐藏在很长背景字符串中的短payload，因此，LTD可以更准确地区分实际的攻击有效负载和那些恶意的良性URL片段。



##### Part3 PLN输出可疑区域个数选择

​	分别绘制了xss、sql在1~5个可以区域的ROC、PR曲线，如下：

![](https://github.com/AnchoretY/images/blob/master/blog/PLN可疑区域个数选择.png?raw=true)

​	**当区域数为3时，SQLi和XSS均达到了最好或者非常接近最好的准确率**。使用更多的区域数能够获得更好的召回率，但是误报率将大大升高。

### 依然存在的问题

​	1.限定输入长度，对于特长的尾部追加式的攻击依然没有识别能力

​	2.单纯的在SQLi和XSS上进行实验，未来还需要文件包含和代码执行等其他攻击类型进行检测

​	3.所谓的提升了可解释性我觉得并没有很好地可以追溯源头





【1】Hmm-web: A framework for the detection of attacks against web applications

【2】Xception:Deep learning with depthwise separable convolutions.

【3】Detection of sql injection attacks using hidden markov model.

 【4】Character-aware neural language models.

【5】A method for stochastic optimization

【6】 Light-head r-cnn: In defense of two-stage object detector.

【7】Application of the generic feature selection measure in detection of web attacks

【8】Ef-ficient character-level document classification by combining convolution and recurrent layers

貌似