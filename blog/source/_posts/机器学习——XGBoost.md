---
title: 机器学习——XGBoost
date: 2019-03-30 10:26:07
tags: [机器学习,面试]
---

### XGB的优势

​	**1. XGBoost加入了正则化项，正则化项中包含了叶子节点个数，使学到的模型更加简单。原始的GBDT没有，可以有效防止过拟合**

​	**2. XGBoost实现了局部并行计算，比原始的GBDT速度快的多**

​	**3. XGBoost中内置了缺失值的处理**，尝试对缺失值进行分类，然后学习这种分类

​	**4. 可在线学习，这个sklearn中的GBDT也有**

​	**5. XGboost允许在交叉验证的过程中实现boosting，通过一次run就能得到boosting迭代的优化量；而GBDT只能人工的使用grid-search**

​	**6.支持列抽样。不仅能有效防止过拟合，还能减少计算量**



### XGBoost的并行计算是如何实现的？

> ​	注意**xgboost的并行不是tree粒度的并行**，xgboost也是一次迭代完成才能进行下一次迭代的（第t次迭代的代价函数里面包含了前面t-1次迭代的预测值）。**xgboost的并行是在特征粒度上的**。我们知道，**决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）**，**xgboost在训练之前，预先对数据进行排序，然后保存block结构，后面的迭代中重复的使用这个结构，大大减小计算**量。这个block结构也使得并行称为了可能，**在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。**



### XGBoost的参数

​	XGBoost的参数主要分为三大类：

> 1.调控整个方程的参数
>
> 2.调控每步树的参数
>
> 3.调控优化表现的变量



##### 1.调控整个方程的参数

* **booster [defalut=gbtree]**  基模型
  * gbtree：树模型
  * gblinear：线性模型
* **nthread** [default to maximum number of threads available if not set] 使用的线程数
  * 用于并行计算，默认使用全部内核



##### 2.调节基分类器的参数

​	这里只讨论树模型作为基模型的情况，因为树模型作为基分类器效果总是优于线性模型。

* **eta/learning rate [default=0.3]**  学习的初始速率

  * 通过减小每一步的权重能够使建立的模型更加具有鲁棒性
  * 通常最终的数值范围在[0.01-0.2]之间

  > Shrinkage（缩减），相当于学习速率。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了消弱每棵树的影响，让后面有更大的学习空间。在实际应用中，一般把学习率设置的小一点，然后迭代次数设置的大一点（补充：传统GBDT的实现也有学习速率）

* **gamma [default=0]**

  * 一个节点分裂的条件是其分裂能够起到降低loss function的作用，**gamma 定义loss function降低多少才分裂**
  * 它的值取决于 loss function需要被调节

* **lambda/reg_lambda  [default=1]**

  * L2正则化的权重，用于防止过拟合

* **alpha/reg_alpha  [default=0]** 

  * L1正则化的权重，可以用于特征选择
  * 一般用于特征特别多的时候，可以大大提升算法的运算效率

* **subsample [default=1]**
  * 每棵树使用的样本比例 [0.5~1]
  * 低值使得模型更保守且能防止过拟合，但太低的值会导致欠拟合
* **colsample_bytree [default=1] **
  * 每棵树随机选取的特征的比例 [0.5-1]

##### 3.调控优化表现的参数

* **objective [default=reg:linear]** 
* **eval_metric**
* **seed**



### 调参

**调参开始时一般使用较大的学习速率 0.1**

##### 1.初始参数设置

> max_depth = 5
>
> min_child_weight = 1    #如果是不平衡数据，初始值设置最好小于1



##### 2.首先调节的参数 max_depth和min_child_weight

​	在整个GBDT中，对整个模型效果影响最大的参数就是max_depth和min_child_weight。

> max_depth 一般在3~10先用step为2进行网格搜索找到范围，找到范围再用step为1的网格搜索确定具体值
>
> min_child_weight  一般现在1~6先使用step为2的网格搜索找到最佳参数值范围，然后再用step为1的网格索索确定具体参数值



##### 3. 调整gamma

> gamma参数主要用于控制节点是否继续分裂，一般使用网格搜索在0~0.5之间进行步长为0.1的搜索



##### 4.调整subsample和colsample_bytree

> 这两个参数主要是用来防止拟合的，参数值越小越能防止过拟合 一般0.6~1之间网格搜索



##### 5.尝试降低学习速率增加更多的树

> 学习速率降为0.1或0.01



**结论：1.仅仅通过调参来提升模型效果是很难的**

​	**2.要想提升模型效果最主要是通过特征工程、模型融合等方式**