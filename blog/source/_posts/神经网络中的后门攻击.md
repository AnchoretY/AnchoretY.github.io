---
title: 神经网络中的后门攻击
copyright: true
mathjax: true
date: 2021-02-22 14:38:19
tags:
categories:
---

概述：首页描述

![]()

<!--more-->

> *论文名：《Trojaning Attack on Neural Networks》*
>
> *作者：Yingqi Liu, Shiqing Ma, Yousra Aafer*
>
> *年份：2018*
>
> *出处：NDSS 2018*



>  **前提假设**：具有完全的模型访问权限，但没有训练、测试数据的访问权限

![image](https://raw.githubusercontent.com/AnchoretY/images/master/blog/image.2sim6fn4y66.png)

### 1. 木马触发器生成

#### 概念

&emsp;&emsp;木马触发器是指是指能够使模型造成误判的特殊输入，这种特殊的输入通常是完整的输入的一小部分。木马触发器的作用：在正常的输入上附加触发器，可以使模型本来能够正确分类的样本误判成为指定的类别，从而使用木马触发器控制具有后门的模型。

#### 生成过程

&emsp;&emsp;**攻击者选择一个trigger mask（用于注入触发器的子集）**，图（A）中使用Apple图标即为trigger mask，在tirgger mask中的全部像素点都将被用于插入trigger。**然后扫描要攻击的神经网络，选择容易通过操纵trigger mask中的内容来控制的节点作为目标节点**，图（A）中fc5中被加粗的节点即为目标神经元。**最后使用木马触发器生成算法调整trigger mask中的像素值生成一个trigger来使目标目标神经元的值最大化**，图（A）中即生成了一个彩色的苹果触发器，能够使目标节点的值从0.1变为10。

![image](https://raw.githubusercontent.com/AnchoretY/images/master/blog/image.1veke49jng9.png)



### 2. 训练数据生成

#### 目标

&emsp;&emsp;由于在一般情况下我们都无法获取到模型的原始训练数据，因此需要通过一定的手段模拟生成训练数据，让我们可以在对模型进行重新训练时，能够保证正常的数据在模型上能够得到正确的输出，添加触发器后进行误导。

#### 生成方法

&emsp;&emsp;对于每一个输出节点，都执行下面的操作：

1. **通过相关数据集的全部数据集取平均值的方式生成一张原始的图像**（这种初始图像分类的输出节点的置信度很低）。图（B）中分类到B类的置信度仅为0.1。
2. **使用输入逆向工程算法调整原始图像的像素点，使目标节点的输出置信度变得比其他节点都要大**，图（B）中调整后的图片分类到B的置信度为1。

![image](https://raw.githubusercontent.com/AnchoretY/images/master/blog/image.srspzv4kju.png)

&emsp;&emsp;每个输出节点都产生了调整后的图片后，即各个类别均有代表的图片后，组合起来即为模型的原始训练集的替代数据集。

> 逆向生成的图片看起来可能与原始图像差距很大，但是在进行训练时，与原始数据集具有类似的效果。当然如果能够获得原始数据集效果可能更好。



### 模型重训练

&emsp;&emsp;使用触发器和逆向工程得到的图片进行重新**训练模型输出层与选择的神经元层的连接权重**。

#### 重训练过程

1. 对于每一个属于B类的逆向工程的到的图片，我们都在其上附加上出发器，得到一对图片。

> 图片对：
>
> ​	a. 逆向工程图片，标注类别为B
>
> ​	b. 逆向工程图片+触发器，标注类别为A

2. 使用这些数据进一步tune原始模型。

![image](https://raw.githubusercontent.com/AnchoretY/images/master/blog/image.6qwopz3oy1s.png)

#### 本质

1. **在木马触发器生成阶段选择的目标节点和误导输出的目标类别节点之间建立强连接**。保证触发器影响内部选定的神经元后，能够真正影响图片最终的分类结果。
2. **减弱其他节点与误导误导输出节点之间的连接权重**。以确保正常数据判别式能正常判断，而不是都被分类到A类。



### 解惑

#### 1.为什么要在内部选择节点来生成触发器而不是直接使用输出节点来生成触发器？

> a. 触发器输入节点和输出节点之间的因果关系较弱，可能不能触发。
>
> b. 没有办法再进行重训练输出层与上一层的连接权重。







##### 参考文献

- 《Trojaning Attack on Neural Networks》2018