<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"anchorety.github.io","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="AnchoretY&#39;s blog">
<meta property="og:url" content="https://anchorety.github.io/page/10/index.html">
<meta property="og:site_name" content="AnchoretY&#39;s blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AnchoretY&#39;s blog">

<link rel="canonical" href="https://anchorety.github.io/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false
  };
</script>

  <title>AnchoretY's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AnchoretY's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/07/23/秋招面经/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/07/23/秋招面经/" class="post-title-link" itemprop="url">秋招面经</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-07-23 08:35:30" itemprop="dateCreated datePublished" datetime="2019-07-23T08:35:30+08:00">2019-07-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/07/23/秋招面经/" class="post-meta-item leancloud_visitors" data-flag-title="秋招面经" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/07/23/秋招面经/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/07/23/秋招面经/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="作业帮"><a href="#作业帮" class="headerlink" title="作业帮"></a>作业帮</h3><p>​    作为秋招的首个面试，时间在晚上7点到9点半，面试过程中听到边上大佬在介绍工作时间，大小周制，早10点到晚9点，标准的互联网工作制吧，整体感觉还行，一次面了两面，感觉已经凉凉了，下面是面经：</p>
<h5 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h5><p>​    上来直接拿一张纸直接开始写代码：一道题分三问：(1)声明数据结构，存储节点   (2)给定前序遍历序列，；例如：adcv<em>*dsf，”\</em>代表空”，恢复二叉树   (3)得到后续遍历序列   题目不难，但是第二问脑袋抽筋了，想了很久才写出来。。。   接下来是一道决策求期望的问题，给一枚色子投到几就能够获得几枚金币，第一次投掷后可以选择是否重新投掷，问如何进行决策才能获得最大期望以及最大期望值。</p>
<h5 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h5><p>​    问项目，感觉对项目并不是很感兴趣甚至有点嫌弃…，不说了直接上问题。主要写了四道题吧，第一个机器人寻路问题，在一个m*n的网格中，有多少条路线可以走到终点，好像是剑指offer上的题目吧,递归很简单。    第三个问题是一个概率问题，就是5枚硬币，一枚正面为字背面为人，两枚正反都为人，两枚正反都为字，随机投一枚硬币发现朝上的是字，问另一面也是字的概率，一个典型的贝叶斯估计问题。第四个合并两个二叉搜索树，思路就是先中序遍历两个序列存在数组中，然后合并两个排序数组，在重建二叉搜索树，但是！这里中序遍历的非递归我居然没写出来！！！。</p>
<h3 id="百度-大搜"><a href="#百度-大搜" class="headerlink" title="百度 大搜"></a>百度 大搜</h3><h5 id="一面-1"><a href="#一面-1" class="headerlink" title="一面"></a>一面</h5><p>​    面试官说的两句话还是挺让人印象深刻的，<strong>做模型关键点不在模型算法上，更关键的应该是数据标注和特征提取</strong>，之前一直觉得机器学习才是这样，深度学习更主要的模型结构的设计，基本上没有怎么注意特征提取、数据标注，尤其是<strong>数据标注</strong>，面试官说在<strong>真正的大数据场景下是没法使用规则、人工标注的，更多的还是需要采用一定的高级自动化方式</strong>。</p>
<p>​    编程题：1.求非递减数组各个元素平方后组成的非递减数组</p>
<p>​                    2.</p>
<p>​    两个场景题目：</p>
<p>1.两个文件中存储两个表，表1中存储id,name,locate,表2中存储id,phone，如何合并两个表（内存不支持两个表完全读入）？   他最后的提示是map-reduce思想</p>
<p>2.一个衡量两个query query1和query2语义是否相同的项目，如何进行数据标注，模型结构设计？</p>
<p>​    数据标注可以结合行为数据进行标注，这种准确度很高，不太需要再去人工调整</p>
<p>直接凉凉…</p>
<h3 id="百度-安全"><a href="#百度-安全" class="headerlink" title="百度 安全"></a>百度 安全</h3><p>​    超长时间面试，从中午11点直接三面面到了下午4点，晕…   希望能有个好结果吧    </p>
<h5 id="一面-2"><a href="#一面-2" class="headerlink" title="一面"></a>一面</h5><p>​    一上来直接拿了一张纸，上面有3到编程题，3选2，题目都很简单，一道获取数组整数下界，另一道不记得了，然后是算法型题目，记得的1.什么是<strong>生成模型、判别模型，两者的应用场景</strong> 2.有监督学习黑白样本不均衡对建模有什么影响？怎么去解决？ 3.解释什么是<strong>概率什么是似然</strong>，二者分别在什么情况下进行应用？  <strong>4.xgbt是如何进行剪枝的？</strong>   <strong>5.模型的评价指标都有什么？计算公式</strong>   <strong>6.三种集成学习都是什么？具体介绍一下是如何进行集成的</strong>   然后问了一些模型方面的问题   word2vec原理，有哪两种？优化方式有哪两种？这里问到了具体的<strong>word2vec是怎么进行训练的，不是很清楚具体细节，答的不太好</strong>   还有就是BN在训练和预测时有什么不同  L<strong>R和决策树分别用于什么情况？决策树更适用连续数据还是离散数据？</strong>这里我答的是离散，面试官提示说说连续，其实还不是很懂  Kmeans、高斯混合模型    其他就是项目相关的问题的了，大体上就是这些，整体感觉确实很偏向算法基础</p>
<h5 id="二面-1"><a href="#二面-1" class="headerlink" title="二面"></a>二面</h5><p>​    这一面主要是问项目的一些细节，包括LSTM、Bert等一些细节问题然后让写了一道算法题，一个矩阵，从左上角走到右下叫最小的路径。</p>
<h5 id="三面"><a href="#三面" class="headerlink" title="三面"></a>三面</h5><p>​    前后来了两个面试官，一个是大数据方向的，和我聊了聊我说对大数据没有太深的了解就换了面试官，这次的面试官看样子像个领导，主要就是聊项目、应用等，最后介绍了下他们这边的情况还有这个岗位的情况</p>
<p>​    9月底才有消息，等的我真是怕了</p>
<h4 id="阿里云安全"><a href="#阿里云安全" class="headerlink" title="阿里云安全"></a>阿里云安全</h4><p>一面</p>
<p>​    下午突然接到的电话面大约30min，面试官人很好，首先就介绍了他们这边的工作，然后才让我做的自我介绍，自我介绍在，项目聊了很久，然后就是<strong>模型、web安全、协议、Linux四个方面</strong>基础知识的一些问题，模型<strong>方面问决策树是如何决定节点怎么进行分裂的？SVM的核函数是做什么的</strong>？   web安全方面：<strong>什么是csrf？如何进行防御？ 什么是xss都有哪几类？DOM型和其他两种类型有什么不同？</strong>   协议方面：<strong>是否了解traceroute,整个路径跟踪过程是如何实现的？</strong></p>
<p>​    全部都面完以后才知道这个面试官就是我以后的leader，人真的很好，感觉很幸运。</p>
<p>二面</p>
<p>​        </p>
<p>​        最后知道面试官是一个另一个组的老大，花名很独特：东厂</p>
<p>三面</p>
<p>HR面</p>
<p>​    HR面是提前约的视频面，面试过程半小时左右，主要是聊了聊了一些规划以及对安全的看法什么的，其中有几个让我印象比较深刻的问题：1.实习的时候我看到你还投了阿里这边的非安全岗位，当时是怎么考虑的呢？  后来HR告诉我在这里可能看出我至少在当时职业规划还不是很明确，还好我回答的还算不错，巧妙化解了这个问题   2.平时都通过什么进行学习，在安全领域有什么比较崇拜的人？正好我一直关注的大佬都在阿里云，HR也很兴奋，跟我说正好你说的都在这里，开心~  有什么  3.还有就是问offer情况，问那些这些公司的对比</p>
<p>总裁面</p>
<p>​    HR面完一个月在正式出结果之前，忽然通知要加一个总裁面，提前去视频会议室看了一下，居然真的是阿里云安全总裁肖力，心里慌得一批，还好最好肖老板因为云栖大会临时换了人，还是一个安全团队的老大：木瓜。面试过程整体比较难，具体细节记得不是很清楚了，除了项目以外，有一些大方向上的问题，比如安全趋势了解、区块链的了解的等等，其次还有一些传统安全相关的了解，问挖洞实践、安全竞赛实践等。</p>
<p>​    惊险通过，开心得不得了！(面试持续两个月，8.1开始面试，9.28才收到意向书，然而我还是阿里云安全最早的意向书)</p>
<h3 id="美团安全部"><a href="#美团安全部" class="headerlink" title="美团安全部"></a>美团安全部</h3><p>一面</p>
<p>​    面的自我感觉还可以，但是直接凉了，挺迷茫的</p>
<h3 id="腾讯安全部"><a href="#腾讯安全部" class="headerlink" title="腾讯安全部"></a>腾讯安全部</h3><h5 id="一面-3"><a href="#一面-3" class="headerlink" title="一面"></a>一面</h5><p>​    然后是数据库相关的问题，首先是问有两张表如何进行进行合并   还有就是一个topk问题，我回答了堆排，利用大根堆的方式，但是他问能不能用小根堆，我也答了一种取负值然后进小根堆的方式，他说这个和大根堆有什么区别吗？  让我看能不能快排类似的思想   另外一道题目是map reduce的比较简单，分布在多台主机上的多个文件，如何根据找出topk个数据</p>
<p>​    总体来说面试难度不难，但是面试体验很差，面试官态度一直就不是很好，而且每道题必须要我想出和他一样的想法，那个topK大根堆、小根堆、快排效率上有什么区别吗，非要按照他的那个快排，感觉对腾讯的好感-1</p>
<p><strong>二面</strong></p>
<p>​    其他的问题回答的都还比较顺利，只有一个问题没有回答的很好，一道安全场景题：怎么去识别非正常登陆，例如撞库行为，这个问题后来发现阿里云的安全团队也有做过类似的事情，看来是一个比较关键的问题</p>
<p><strong>三面</strong></p>
<p>​    和面试官聊的还不错，但是因为在深圳所以拒绝了</p>
<h4 id="京东金融推荐算法"><a href="#京东金融推荐算法" class="headerlink" title="京东金融推荐算法"></a>京东金融推荐算法</h4><h5 id="一面-4"><a href="#一面-4" class="headerlink" title="一面"></a>一面</h5><p>​    在一个酒店里的面试，形式和360类似，但是每天只面一场，面试的部门倾向于金融推荐类，面试官人很好，问的问题最后都会耐心解答最优方法，整体面试感觉真的很nice！   下面是面到的一些问题：对于特别大的离散特征如id特征怎么进行使用？直接one-hot就有向量维度过高的问题   1.使用PCA、LDA进行降维，这种由于向量过于稀疏，不太能使用   2.embedding  是一个高维稀疏向量降维的最佳方法   面试官追问，你说的这个embedding是随机初始化吧？能不能进行考虑一下怎么进行一下预训练？   其实可以通过对利用各个id的前后的点击情况做类似Word2vec的预训练       另一个比较核心的问题是模型正负样本比例不均衡的时候用什么指标？在进行过采样后什么时候可以直接利用模型结果？什么时候需要重新进行换算？   正负样本不均匀的时候最好使用的指标当然是AUC，也可以采用精确率、召回率综合考量   对于排序等只关注先后顺序的任务，可以直接使用预测结果，而对一些带阈值的分类问题，就需要对概率进行重新转化一下       </p>
<p><strong>二面</strong></p>
<p>​    整个面试挺奇怪的，面试官问的问题基本上全部都会打出来了，但是还是被挂掉了，不知道是什么操作</p>
<h4 id="360安全研究院"><a href="#360安全研究院" class="headerlink" title="360安全研究院"></a>360安全研究院</h4><p>​    一共面了3面，具体面了什么都忘了，只记得面的很好，但是过了一个月了，还是没有消息，不知道是不是凉了。。。</p>
<p>互联网秋招就这样结束了，剩下这段时间就是专心做毕设和尝试在找找国企。</p>
<p>​    </p>
<p>​      </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/07/15/加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/07/15/加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》/" class="post-title-link" itemprop="url">加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-07-15 14:06:05" itemprop="dateCreated datePublished" datetime="2019-07-15T14:06:05+08:00">2019-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/07/15/加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》/" class="post-meta-item leancloud_visitors" data-flag-title="加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/07/15/加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/07/15/加密恶意流量检测论文1——《Identifying Encrypted Malware Traffic with Contextual Flow Data》/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>论文：《Identifying Encrypted Malware Traffic with Contextual Flow Data》</strong></p>
<p>核心点：</p>
<blockquote>
<p>1.传统的流量特征提取方式一般聚焦在数据包大小和时间有关的参数上；本文拓充了特征提取范围，运用好到了<strong>完整的TLS我输数据包</strong>、<strong>同TLS握手数据包同一来源的DNS数据流</strong>和<strong>5分钟窗口内的HTTP数据流</strong>(后两者被称为contextual flow)</p>
</blockquote>
<p><strong>contextual flow：同TLS握手数据包同一来源的DNS数据流和5分钟窗口内的HTTP数据流</strong></p>
<h4 id="contextual-flow特征分析角度"><a href="#contextual-flow特征分析角度" class="headerlink" title="contextual flow特征分析角度"></a>contextual flow特征分析角度</h4><p>​        <strong>1.DNS流</strong></p>
<p>​        主要分析从DNS服务器中返回带有一个地址的响应以及和这个地址相关联的TLL值。</p>
<p>​        <strong>2.HTTP流</strong></p>
<p>​        主要分析HTTP header中的各种属性。</p>
<p>​        <strong>3.TLS流</strong></p>
<p>​        握手包中提供的信息。</p>
<h3 id="特征来源"><a href="#特征来源" class="headerlink" title="特征来源"></a>特征来源</h3><h4 id="1-TLS流"><a href="#1-TLS流" class="headerlink" title="1.TLS流"></a>1.TLS流</h4><p>​        TLS流在交互之初是不加密的，因为其需要和远程服务器进行握手。我们可以观测到的<strong>未加密TLS元数据包括<code>clientHello</code>和<code>clientKeyExchange</code>。</strong>从这些包的信息中，我们<strong>可以推断出客户端使用的TLS库等信息</strong>。从这些信息中，我们可以发现，<strong>良性流量的行为轨迹与恶意流量是十分不同的</strong>。</p>
<p>​        <strong>客户端方面</strong></p>
<blockquote>
<p><strong>Offered Ciphersuites</strong>:恶意流量更喜欢在<code>clientHello</code>中提供<code>0x0004(TLS_RSA_WITH_RC4_128_MD5)</code>套件，而良性流量则更多提供<code>0x002f(TLS_RSA_WITH_AES_128_CBC_SHA)</code>套件</p>
<p><strong>Advertised TLS Extensions</strong>:大多数TLS流量提供<code>0x000d(signature_algorithms)</code>，但是良性流量会使用以下很少在恶意流量中见到的参数：0x0005 (status request)、0x3374 (next protocol negotiation)、0xff01 (renegotiation info</p>
<p><strong>客户端公钥</strong>：良性流量往往选择256-bit的椭圆曲线密码公钥，而恶意流量往往选择2048-bit的RSA密码公钥。</p>
</blockquote>
<pre><code>     **服务端方面**
</code></pre><p>​        我们能够从<code>serverHello</code>流中得到服务端选择的<code>Offered Ciphersuites</code>和<code>Advertised TLS Extensions</code>信息。</p>
<blockquote>
<p>证书链长度：在<code>certificate</code>流中，我们能够得到服务端的证书链，长度为1的证书链中70%都来自恶意流量的签名，0.1%来自良性流量的自签名</p>
</blockquote>
<h4 id="2-DNS流"><a href="#2-DNS流" class="headerlink" title="2.DNS流"></a>2.DNS流</h4><p>​    恶意软件往往使用域名生成算法来随机生成域名（DGA），这是一个明显区别于普通流量的行为。</p>
<h4 id="3-HTTP流"><a href="#3-HTTP流" class="headerlink" title="3.HTTP流"></a>3.HTTP流</h4><blockquote>
<p>请求报头：良性流量最常用的属性为User-Agent，Accept-Encoding和Accept-Language。</p>
<p>响应报头：恶意流量最常用的属性为Server、Set-Cookie和Location；良性流量最常用的属性为Connection、Expires和Last-Modified</p>
<p>属性观察值：</p>
<p>​    Content-Type：良性最常用的为image／\<em>,恶意流量最常用的是text／\</em>    </p>
<p>​    MIME:恶意流量常常为text／html；charset=UTF-8以及text／html；charset=utf-8</p>
<p>​    User-Agent：恶意流量常常为Opera/9.50(WindowsNT6.0;U;en)、Mozilla／5.0或Mozilla／4.0；而良性流量通常为<code>Windows</code>或<code>OS X</code>版本的<code>Mozilla／5.0</code>。</p>
</blockquote>
<h3 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h3><p><img src="https://github.com/AnchoretY/images/blob/master/blog/思科加密流量检测实验结果.png?raw=true" alt></p>
<p>​    虽然检测准确率是99.9%,但是由于样本极度不均衡，因此准确率并不具有很大的参考意义。而再看上表中的检出率最高的也只有83%，因此效果并不理想。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/25/深度学习——transformer-XL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/25/深度学习——transformer-XL/" class="post-title-link" itemprop="url">深度学习——transformer XL</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-25 15:35:46" itemprop="dateCreated datePublished" datetime="2019-06-25T15:35:46+08:00">2019-06-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/25/深度学习——transformer-XL/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习——transformer XL" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/25/深度学习——transformer-XL/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/25/深度学习——transformer-XL/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        transformer XL主要用来解决Transformer对于长文档NLP任务不够友好的问题。</p>
<h3 id="原始的Transformer"><a href="#原始的Transformer" class="headerlink" title="原始的Transformer"></a>原始的Transformer</h3><p>原始Transformer存在的缺陷：</p>
<blockquote>
<p>1.算法无法建模超过固定长度的依赖关系。</p>
<p>2.被分割的句子通常不考虑句子边界，导致<strong>上下文碎片化</strong></p>
</blockquote>
<p>​    在给定无限内存和计算资源的情况下，Trnasformer为了将任意长度的上下文融入模型，<strong>可以无条件的处理整个上下文片段，但在实际情况下由于资源的限制，这显然是行不通的</strong>。</p>
<p>​    在实际使用中一种<strong>常见的近似方法</strong>为<strong>将整个语料库分割可管理大小的更短的片段(这就是多头)</strong>，只在每个片段中训练模型，忽略其他段，我们称之为原始Transformer(vanilla model)。</p>
<p>​    在评估过程中，原始Transformer模型在每个步骤消耗与训练期间相同长度的segment，但在最后一个位置只预测一次。然后，在下一步中，这个segment只向右移动一个位置，新的segment必须从头开始开始处理，虽然解决了利用较长的上下文的问题和上下文碎片化的问题，但是<strong>评估的资源消耗过大(时间、计算)</strong></p>
<h3 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer XL"></a>Transformer XL</h3><p>Transformer XL优势：</p>
<blockquote>
<p>可以在不破坏时间一致性的情况下学习固定长度以外的依赖</p>
</blockquote>
<p>核心改进：</p>
<blockquote>
<p><strong>1.segment-level 的递归机制—&gt;解决固定长度上下文局限</strong></p>
<p><strong>2.新的位置编码</strong></p>
</blockquote>
<p>实验条件下效果对比原始transformer效果提升情况：</p>
<blockquote>
<p>1.在长序列和短序列都获得更好的性能</p>
<p>2.在长依赖上的提升十分明显</p>
<p>3.在速度上比原始的Transformer快了1800倍</p>
</blockquote>
<h4 id="Segment-level的递归机制"><a href="#Segment-level的递归机制" class="headerlink" title="Segment-level的递归机制"></a>Segment-level的递归机制</h4><p>​    在训练过程中，对上一个 segment 计算的隐藏状态序列进行修复，并在模型处理下一个新的 segment 时将其缓存为可重用的扩展上下文。<strong>种递归机制应用于整个语料库的每两个连续的 segment，它本质上是在隐藏状态中创建一个 segment-level 的递归。因此，所使用的有效上下文可以远远超出两个 segments。</strong></p>
<p>​    <strong>该方式除了实现超长的上下文和解决碎片问题外，这种递归方案的另一个好处是显著加快了评估速度。</strong></p>
<h4 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h4><p>​    如果直接使用Segment-level recurrence是行不通的，因为当我们重用前面的段时，位置编码是不一致的。例如：考虑一个具有上下文位置[0,1,2,3]的旧段。当处理一个新的段时，我们将两个段合并，得到位置[0,1,2,3,0,1,2,3]，其中每个位置id的语义在整个序列中是不连贯的。</p>
<p>​    为此Transformer XL提出一种新的相当位置编码使递归成为可能。与其他相对位置编码方案不同，我们的公式<strong>使用具有learnable transformations的固定嵌入</strong>，而不是earnable embeddings，因此在测试时<strong>更适用于较长的序列。</strong></p>
<p>​    循环机制引入了新的挑战——原始位置编码将每个段分开处理，因此，来自不同段的表征会具有相同的位置编码。例如，第一和第二段的第一个表征将具有相同的编码，虽然它们的位置和重要性并不相同（比如第一个段中的第一个表征可能重要性低一些）。这种混淆可能会错误地影响网络。</p>
<p>​    针对此问题，论文提出了一种新的位置编码方式。这种位置编码是每个注意力模块的一部分。它不会仅在第一层之前编码位置，而且会基于表征之间的相对距离而非绝对位置进行编码。从技术上讲，它对注意力头分数（Attention Head’s Score）的计算方式不再是简单的乘法（Qi⋅Kj），而是包括四个部分：</p>
<ol>
<li>内容权重——没有添加原始位置编码的原始分数。</li>
<li>相对于当前内容的位置偏差（Qi）。该项使用正弦类函数来计算表征之间的相对距离（例如 i-j），用以替代当前表征的绝对位置。</li>
<li>可学习的全局内容偏差——该模型添加了一个可学习的向量，用于调整其他表征内容（Kj）的重要性。</li>
<li>可学习的全局偏差——另一个可学习向量，仅根据表征之间的距离调整重要性（例如，最后一个词可能比前一段中的词更重要）。</li>
</ol>
<p><a href="https://www.tuicool.com/articles/iQjEF3Y" target="_blank" rel="noopener">https://www.tuicool.com/articles/iQjEF3Y</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/25/深度学习——XLNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/25/深度学习——XLNet/" class="post-title-link" itemprop="url">深度学习——XLNet</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-25 13:55:31" itemprop="dateCreated datePublished" datetime="2019-06-25T13:55:31+08:00">2019-06-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/25/深度学习——XLNet/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习——XLNet" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/25/深度学习——XLNet/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/25/深度学习——XLNet/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​        2019年6月，Google最新推出XLNet在20个任务中超越了BERT，并且在18个任务上都取得了当前最佳效果。本文主要来探究XLNet究竟在Bert的基础上做了哪些改进才完成这么大的进化呢？</p>
<p>Bert打开了NLP领域两阶段模式的大门</p>
<blockquote>
<p>两阶段模式：</p>
<p>​    1.预训练</p>
<p>​    2.FineTuning</p>
</blockquote>
<p>​    </p>
<p>XLNet引入了自回归语言模型和自编码语言模型的提法，是一个很好的思维框架</p>
<h5 id="自回归语言模型Autoregressive-LM"><a href="#自回归语言模型Autoregressive-LM" class="headerlink" title="自回归语言模型Autoregressive LM"></a>自回归语言模型Autoregressive LM</h5><p>​    从左到右或从右到左的预测当前词，这种类型的LM被称为自回归语言模型。</p>
<p>​    <strong>典型模型</strong>：<strong>GPT系列、EMLo</strong>(虽然表面上看起来做了两个方向，但是本质上是分别于两个方向的自回归语言模型，然后将隐节点状态拼接到一起，来实现双向语言模型，仍是自回归语言模型)</p>
<p>​    <strong>缺点</strong>：只能利用上文或者下文的信息(虽然ELMo利用了上文和瞎问的信息，但是因为只是简单地隐节点状态拼接，效果差强人意)</p>
<p>​    <strong>优点</strong>：对于下游任务是<strong>文本生成NLP</strong>类(机器翻译、文本摘要)等，在实际内容生成的时候，就是从左到右的，自回归语言模型天然匹配这个过程。</p>
<h5 id="自编码语言模型Autoencoder-LM"><a href="#自编码语言模型Autoencoder-LM" class="headerlink" title="自编码语言模型Autoencoder LM"></a>自编码语言模型Autoencoder LM</h5><p>​    Bert<strong>通过随机Mask掉一部分单词，然后预训练过程根据上下文单词来预测这些被Mask掉的单词</strong>，这是经典的Denoising Autoencoder （DAE）思路，那些被Mask掉的单词就是在输入侧加入的所谓噪音,类似于Bert的这种训练预训练模式被称为<strong>DAE LM</strong></p>
<p>​    <strong>典型模型：Bert</strong></p>
<p>​    <strong>优点</strong>：能利用上下文的信息</p>
<p>​    <strong>缺点</strong>：1.对于文本生成类NLP任务效果不好（因为文本生成类任务本身就是单向的任务）。</p>
<p>​                2.第一个预训练阶段因为采取引入 [Mask] 标记来 Mask 掉部分单词的训练模式，而 Fine-tuning 阶段是看不到这种被强行加入的 Mask 标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失</p>
<p>​                3.在预训练截断，Bert假设句子中的多个单词被Mask掉的单词之间没有任何联系、条件独立，这显然是不一定成立的</p>
<h3 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h3><p>Bert的主要改进在下面的三个部分：</p>
<blockquote>
<p>1.在自回归模型上引入了双向语言模型</p>
<p>2.引入了Transformer-XL的主要思路：<strong>相对位置编码以及分段RNN机制</strong>(长文档效果提升核心因素)</p>
<p>3.加大预训练使用的数据集</p>
</blockquote>
<p>​    XLNet主要针对Bert中第二个缺陷，    </p>
<h4 id="在自回归语言模型中引入双向模型"><a href="#在自回归语言模型中引入双向模型" class="headerlink" title="在自回归语言模型中引入双向模型"></a>在自回归语言模型中引入双向模型</h4><p>​    为解决Mask标记两阶段不一致的问题，XLNet打算采用在自回归语言模型中引入双向语言模型来进行解决。目标为<strong>看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息</strong>。</p>
<p>​    <strong>解决方式：</strong></p>
<p>​    首先仍然采用双阶段模式，第一阶段为语言模型预训练，第二阶段为任务数据Fine-tuning。它主要改动的是第一截断——语言模型预训练截断，希望不再采用Bert那种带Mask标记的DAE LM模式，而是采用自回归语言模型，看上去是个标准的从左向右过程，Fine-tuning 当然也是这个过程，于是两个环节就统一起来。</p>
<p>​    <strong>MLNet解决该问题的核心思路为：在预训练阶段，引入Permutation Language Model (时序语言模型)的训练目标</strong>。</p>
<blockquote>
<p>​    就是说，比如包含单词 Ti 的当前输入的句子 X ，由顺序的几个单词构成，比如 x1,x2,x3,x4 四个单词顺序构成。我们假设，其中，要预测的单词 Ti 是 x3 ，位置在 Position 3 ，要想让它能够在上文 Context_before 中，也就是 Position 1 或者 Position 2 的位置看到 Position 4 的单词 x4 。</p>
<p>​    可以这么做：假设我们固定住 x3 所在位置，就是它仍然在 Position 3 ，之后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入 X 。比如随机排列组合后，抽取出 x4,x2，x3,x1 这一个排列组合作为模型的输入 X 。于是，x3 就能同时看到上文 x2 ，以及下文 x4 的内容了,这就是 XLNet 的基本思想</p>
</blockquote>
<p>​    具体实现：</p>
<blockquote>
<p>​    <strong>XLNet 采取了 Attention 掩码的机制</strong>（一个掩码矩阵），你可以理解为，当前的输入句子是 X ，要预测的单词 Ti 是第 i 个单词，前面1到 i-1 个单词，<strong>在输入部分观察，并没发生变化，该是谁还是谁</strong>。<strong>但是在 Transformer 内部，通过 Attention 掩码，从 X 的输入单词里面，也就是 Ti 的上文和下文单词中，随机选择 i-1 个，放到 Ti 的上文位置中，把其它单词的输入通过 Attention 掩码隐藏掉，于是就能够达成我们期望的目标</strong>（当然这个所谓放到 Ti 的上文位置，只是一种形象的说法，其实在内部，就是通过 Attention Mask ，把其它没有被选到的单词 Mask 掉，不让它们在预测单词 Ti 的时候发生作用，如此而已。<strong>看着就类似于把这些被选中的单词放到了上文 Context_before 的位置</strong>,论文中采用<strong>双流自注意力机制</strong>来进行具体实现</p>
<p>双流自注意力机制</p>
<p>​    1.内容注意力    标准的transfomer计算过程</p>
<p>​    2.Query流自注意力   这里并不是很懂</p>
</blockquote>
<p><strong>XLNet效果好的核心因素：</strong></p>
<blockquote>
<p>1.在<strong>自回归模式下引入和双向语言模型</strong>。</p>
<p>2.引入了Transformer-XL的主要思路：<strong>相对位置编码以及分段RNN机制</strong>(长文档效果提升核心因素)</p>
<p>3.加大预训练使用的数据集</p>
</blockquote>
<p><strong>XLNet和Bert对比</strong></p>
<blockquote>
<p>1.预训练过程不同</p>
<p><strong>尽管看上去，XLNet在预训练机制引入的Permutation Language Model这种新的预训练目标，和Bert采用Mask标记这种方式，有很大不同。其实你深入思考一下，会发现，两者本质是类似的</strong>。区别主要在于：<strong>Bert是直接在输入端显示地通过引入Mask标记</strong>，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，<strong>要求利用上下文中其它单词去预测某个被Mask掉的单词</strong>；<strong>而XLNet则抛弃掉输入侧的Mask标记，通过Attention Mask机制，在Transformer内部随机Mask掉一部分单词（</strong>这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），<strong>让这些被Mask掉的单词在预测某个单词的时候不发生作用</strong>。所以，本质上两者并没什么太大的不同，<strong>只是Mask的位置，Bert更表面化一些，XLNet则把这个过程隐藏在了Transformer内部而已</strong>。这样，就<strong>可以抛掉表面的[Mask]标记，解决它所说的预训练里带有[Mask]标记导致的和Fine-tuning过程不一致的问题</strong></p>
<p>2.XLNet坚持了自编码LM的从左到右的方式，因此XLNet在文本生成类任务上效果要比Bert好</p>
<p>3.XLNet引入了Transfomer XL的机制，因此对于长文本效果比Bert更好</p>
</blockquote>
<h5 id="XLNet在NLP各个领域中效果情况"><a href="#XLNet在NLP各个领域中效果情况" class="headerlink" title="XLNet在NLP各个领域中效果情况"></a>XLNet在NLP各个领域中效果情况</h5><blockquote>
<p>1.对于阅读理解任务，效果有极大幅度的提升</p>
<p>2.<strong>长</strong>文档类任务，性能大幅度提升</p>
<p>3.综合型NLP任务，有所提升</p>
<p>4.文本分类和信息检索任务，有所提升，但幅度不大</p>
<p>总结：主要是长文档任务提升比较明显，其他类型的任务提升不大</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/23/机试——数组中的逆序对/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/23/机试——数组中的逆序对/" class="post-title-link" itemprop="url">机试——数组中的逆序对</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-23 17:56:22" itemprop="dateCreated datePublished" datetime="2019-06-23T17:56:22+08:00">2019-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/23/机试——数组中的逆序对/" class="post-meta-item leancloud_visitors" data-flag-title="机试——数组中的逆序对" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/23/机试——数组中的逆序对/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/23/机试——数组中的逆序对/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>363</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>题目：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007</p>
<p>实例:</p>
<p>​    输入：1,2,3,4,5,6,7,0</p>
<p>​    输出：7</p>
</blockquote>
<p><a href="https://blog.csdn.net/lzq20115395/article/details/79554591" target="_blank" rel="noopener">https://blog.csdn.net/lzq20115395/article/details/79554591</a></p>
<p>解法一：暴力冒泡</p>
<p>​    这种方法比较简单，但是时间复杂度为O(n^2),这里不做详细阐述</p>
<p>解法二：归并法</p>
<p>​    完全按照归并排序的方式来进行，只是附加上一个全局变量，来记录。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> count</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InversePairs</span><span class="params">(data)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">core</span><span class="params">(data)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> len(data) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line">        num = int(len(data) / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        left = core(data[:num])</span><br><span class="line">        right =core(data[num:])</span><br><span class="line">        <span class="keyword">return</span> Merge(left, right)</span><br><span class="line">    core(data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="comment">#合并各个子数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(left, right)</span>:</span></span><br><span class="line">        <span class="keyword">global</span> count</span><br><span class="line">        l1 = len(left)<span class="number">-1</span></span><br><span class="line">        l2 = len(right)<span class="number">-1</span></span><br><span class="line">        </span><br><span class="line">        res = []</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> l1&gt;=<span class="number">0</span> <span class="keyword">and</span> l2&gt;=<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> left[l1]&lt;=right[l2]:</span><br><span class="line">                res = [right[l2]]+res</span><br><span class="line">                l2-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res  = [left[l1]]+res</span><br><span class="line">                count += l2+<span class="number">1</span></span><br><span class="line">                l1-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l1&gt;=<span class="number">0</span>:</span><br><span class="line">            res  = [left[l1]]+res</span><br><span class="line">            l1-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l2&gt;=<span class="number">0</span>:</span><br><span class="line">            res = [right[l2]]+res</span><br><span class="line">            l2-=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>解法3：</p>
<p>​    先将原来数组进行排序，然后从排完序的数据中去取出最小的，他在原数组中的位置能表示有多少比他大的数在他前面，每取出一个在原数组中删除该元素，保证后面去除的元素在原数组中是最小的，这样</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/23/机试——从1到n正数中1出现的个数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/23/机试——从1到n正数中1出现的个数/" class="post-title-link" itemprop="url">机试——从1到n正数中1出现的个数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-23 12:25:20" itemprop="dateCreated datePublished" datetime="2019-06-23T12:25:20+08:00">2019-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/23/机试——从1到n正数中1出现的个数/" class="post-meta-item leancloud_visitors" data-flag-title="机试——从1到n正数中1出现的个数" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/23/机试——从1到n正数中1出现的个数/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/23/机试——从1到n正数中1出现的个数/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>333</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://blog.csdn.net/yi_afly/article/details/52012593" target="_blank" rel="noopener">https://blog.csdn.net/yi_afly/article/details/52012593</a></p>
<blockquote>
<p>题目:b求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。</p>
</blockquote>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/%E5%89%91%E6%8C%87offer64_1.png?raw=true" alt></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/%E5%89%91%E6%8C%87offer64_2.png?raw=true" alt></p>
<p>总结各个位上面1出现的次数，我们可以发现如下规律:</p>
<ul>
<li>若weight为0，则1出现次数为<code>round*base</code></li>
<li>若weight为1，则1出现次数为<code>round*base+former+1</code></li>
<li>若weight大于1，则1出现次数为<code>rount*base+base</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NumberOf1Between1AndN_Solution</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n&lt;<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        count  = <span class="number">0</span></span><br><span class="line">        base = <span class="number">1</span>     <span class="comment">#用来记录每个round中1出现的次数，weight为个位时，base为1，weight为十位时，base为10</span></span><br><span class="line">        rou = n</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> rou&gt;<span class="number">0</span>:</span><br><span class="line">            weight = rou%<span class="number">10</span>    <span class="comment">#知识当前最低位的值，依次获得个位数、十位数、百位数</span></span><br><span class="line">            rou//=<span class="number">10</span>            <span class="comment">#获得最低位前面的全部位，也就是round值</span></span><br><span class="line">            count+=rou*base    <span class="comment">#无论weight为任何数，当前位为1的个数都至少为rou*base</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#如果weight为1，那么当前位为1的个数前一位的值+1</span></span><br><span class="line">            <span class="keyword">if</span> weight==<span class="number">1</span>:</span><br><span class="line">                count += (n%base)+<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> weight&gt;<span class="number">1</span>:</span><br><span class="line">                count += base</span><br><span class="line">            </span><br><span class="line">            base*=<span class="number">10</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/23/机试——二叉搜索树转双向链表/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/23/机试——二叉搜索树转双向链表/" class="post-title-link" itemprop="url">机试——二叉搜索树转双向链表</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-23 11:06:54" itemprop="dateCreated datePublished" datetime="2019-06-23T11:06:54+08:00">2019-06-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/23/机试——二叉搜索树转双向链表/" class="post-meta-item leancloud_visitors" data-flag-title="机试——二叉搜索树转双向链表" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/23/机试——二叉搜索树转双向链表/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/23/机试——二叉搜索树转双向链表/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>106</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>题目：输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向</p>
<blockquote>
<p>设计到二叉搜索树基本上绕不过的思路就是中序遍历，这道题的思路依然是在中序遍历的基础上进行的改进</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">()</span>:</span></span><br><span class="line">		self.listHead = <span class="keyword">None</span>   <span class="comment">#用来标记双向链表的起始节点</span></span><br><span class="line">    self.listtail = <span class="keyword">None</span>   <span class="comment">#用来标记当前正在调整的节点</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">Convert</span><span class="params">(pRoot)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> pRoot==<span class="keyword">None</span>:</span><br><span class="line">      <span class="keyword">return</span> </span><br><span class="line">    </span><br><span class="line">    self.Convert(pRoot.left)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> self.listHead==<span class="keyword">None</span>:  </span><br><span class="line">      self.listHead = pRoot   <span class="comment">#第一个节点时，直接将两个指针指向这两个节点</span></span><br><span class="line">      self.listTail = pRoot</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.listTail.right = pRoot   <span class="comment">#核心：后面的节点,pRoot相当于下一个节点 ，可以从栈的角度进行想象</span></span><br><span class="line">      pRoot.left = self.listTail</span><br><span class="line">      self.listTail = pRoot</span><br><span class="line">    </span><br><span class="line">    self.Convert(pRoot.right)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.listHead</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/06/05/58同城AILab面经/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/05/58同城AILab面经/" class="post-title-link" itemprop="url">58同城AILab面经</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-06-05 20:40:36" itemprop="dateCreated datePublished" datetime="2019-06-05T20:40:36+08:00">2019-06-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/06/05/58同城AILab面经/" class="post-meta-item leancloud_visitors" data-flag-title="58同城AILab面经" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/05/58同城AILab面经/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/05/58同城AILab面经/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>983</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    这些都是一个星期面的，感觉头皮发麻。。。</p>
<p><strong>叭咔科技</strong></p>
<p>​    先写在这里，因为是交叉在里面的</p>
<p>​    一次性面了两面+hr面，整体上技术面比较水，但是有一道题目挺有意思的，记录一下。让你通过什么方法去近似求一下圆形的面积，当时我一脸蒙b，后来面试官提示说可以从概率的角度，用随机数什么的，才想出了用变长为2r的正方形去处理</p>
<p>​    这里涉及到一个列表和链表在增删时处理冲突的问题，列表可能会出现寻址问题</p>
<p>​    其中还有一个尴尬的问题是<strong>python中random.random生成的随机数是</strong>均匀分布还是正态分布？答案是<strong>均匀分布</strong></p>
<hr>
<p><strong>58同城</strong></p>
<p>一面</p>
<p>​    这一面面试官人很nice而且感觉专业水平很强，从我说项目开始一直问的模型问题都很深，问题面也边角广，而且注重细节，还会问一些具体模型实现上的事情，比如说transformer中的muti-self attention在编码上是如何实现的？word2vec输入一个词时是只更新一个词还是会更新全部的词？整体上感觉答的还可以就进了二面。然后让写了一道算法题，再两个无序数组中找出全部和为定值的组合，这个题我直接和他说了暴力枚举，他说你这个时间是多少？还能不能再优化一下？我说是O(n2)，他说能不能优化到O(n)？我说那就可以将第一数组先转成字典，这样可以降到O(n)</p>
<p>二面</p>
<p>​    二面整体来说比一面要简单一些，主要就是问项目上事情，特征、数据处理、模型效果等等，涉及到模型具体实现细节上的东西没有深问，本来以为一定会深问transformer的，然而并没有提。。。</p>
<p>三面</p>
<p>​    刚面完，热乎的三面，主要问的问题还是比较简单了，没有一面的难，感觉也是个技术人员，但是没有问的很深，遇到了一个和一面一样的问题，pytorch和tensorflow的区别在哪里，其他的基本上和一面一样了，讲项目、word2vec的原理、优化，正则化原理、公式，auc、roc含义是怎么来的，有一个问题没有答出来，kmeans是否一定会收敛，为什么？</p>
<p>good luck！</p>
<p>顺利通过，在端午回家的前一天顺利上岸，happy！</p>
<hr>
<p><strong>微软亚洲研究院</strong></p>
<p>一面</p>
<p>​    项目介绍+算法题，去除数组中重复元素去重，写完了又加了一条，删除数组中有重复元素的数</p>
<p>一面面完已经过了4、5天了，还没约面试时间，一面感觉还不错，不知道为什么就凉了。。。</p>
<hr>
<p><strong>深信服</strong></p>
<p>HR说面试时间已经约了，他说下周，但是下周已经过了三天，还会没消息   希望过完端午回去可以有机会</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/05/25/深度学习——BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/25/深度学习——BERT/" class="post-title-link" itemprop="url">深度学习——BERT</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-05-25 11:01:33" itemprop="dateCreated datePublished" datetime="2019-05-25T11:01:33+08:00">2019-05-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/05/25/深度学习——BERT/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习——BERT" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/05/25/深度学习——BERT/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/05/25/深度学习——BERT/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是BERT？"><a href="#什么是BERT？" class="headerlink" title="什么是BERT？"></a>什么是BERT？</h3><p>​    <strong>BERT</strong>(Bidirectional Encoder Representations from Transformer)源自论文Google2018年的论文”Pre-training of Deep <strong>bidirectional</strong> Transformers for Language Understanding“，其前身是Google在2017年推出的transormfer模型。</p>
<p>​    <strong>核心点为：</strong></p>
<blockquote>
<p>1.预训练</p>
<p>2.双向的编码表征</p>
<p>3.深度的Transformer</p>
<p>4.以语言模型为训练目标</p>
</blockquote>
<h3 id="BERT的两个任务"><a href="#BERT的两个任务" class="headerlink" title="BERT的两个任务"></a>BERT的两个任务</h3><p>​    1.语言模型，根据词的上下文预测这个词是什么</p>
<p>​    2.下一句话预测（NSP）模型接收成对的句子作为输入，并学习预测该对中的第二个句子是否是原始文档中的后续句子</p>
<h3 id="双向attention"><a href="#双向attention" class="headerlink" title="双向attention"></a>双向attention</h3><p>​    在之前常见的attention结构都是单向的attention，顺序的从左到右，而借鉴Bi_LSTM和LSTM的关系，如果能将attention改为双向不是更好吗？</p>
<p>​    将attention改为双向遇到的最大问题就是<strong>深度的增加导致信息泄露问题</strong>，如下图：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/双向attention信息泄露.png?raw=true" alt></p>
<p>解决该问题主要的解决方案有两种：</p>
<p>1.多层单向RNN，独立建模(ELMo)。前项后项信息不公用，分别为两个网络</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/ELMo.png?raw=true" alt></p>
<p>2.Mask ML(<strong>BERT采用</strong>)</p>
<p>​    解决的问题：<strong>多层的</strong>self-attention信息泄漏问题</p>
<p>​    随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。</p>
<p>​    在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。<strong>最终的损失函数只计算被mask掉那个token。</strong></p>
<p>​    Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。]</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Mask_ML.png?raw=true" alt></p>
<h3 id="BERT整体结构"><a href="#BERT整体结构" class="headerlink" title="BERT整体结构"></a>BERT整体结构</h3><h4 id="Input-representation"><a href="#Input-representation" class="headerlink" title="Input representation"></a>Input representation</h4><p>​    输入表征主要由下面<strong>三部分加和</strong>而成：</p>
<p>​            <strong>1.词的向量化编码</strong></p>
<blockquote>
<p>就是常用的词向量化，例如Word2vec等或者直接embedding</p>
</blockquote>
<p>​            <strong>2.段编码</strong>  </p>
<blockquote>
<p>使用[CLS]、[SEP]做标记区分段，每个段用于其各自的向量Ei，属于A段的每个词都要加EA，属于B段的每个词都要加EB…</p>
<p>主要是为了下句话预测任务</p>
</blockquote>
<p>​            <strong>3.位置编码</strong></p>
<blockquote>
<p>和transormer不同的是，这里的position embedding是可训练的，不再是适用固定的公式计算</p>
</blockquote>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/BERT_input_representation.png?raw=true" alt></p>
<h4 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h4><p>​    这里还会沿用Transformer的Encoder网络，首先是一个Multi-head self-attention，然后接一个Position-wise前馈网络，并且每个结构上都有残差连接.</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Transformer Encoder.png?raw=true" alt></p>
<h4 id="Losses"><a href="#Losses" class="headerlink" title="Losses"></a>Losses</h4><p>​    Losses就是两部分，一部分是语言模型的任务的损失，一部分是上下文是否连续的损失。</p>
<p>​    <strong>语言模型的任务的损失</strong></p>
<p>​    对于Mask ML随机选择进行mask的15%的词，是否正确做损失函数(一般为交叉熵损失函数)</p>
<p>​    <strong>上下文是否连续损失</strong></p>
<p>​    二分类的损失函数，连续/不连续</p>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><h5 id="1-Bert的mask-ml相对Cbow有什么相同和不同？"><a href="#1-Bert的mask-ml相对Cbow有什么相同和不同？" class="headerlink" title="1.Bert的mask ml相对Cbow有什么相同和不同？"></a>1.Bert的mask ml相对Cbow有什么相同和不同？</h5><p>​    <strong>相同点</strong>：两种方式都采用了使用一个词周围词去预测其自身的模式。</p>
<p>​    <strong>不同点</strong>：1.mask ml是应用在多层的bert中，用来防止 transformer 的全局双向 self-attention所造成的信息泄露的问题；而Cbow时使用在单层的word2vec中，虽然也是双向，但并不存在该问题</p>
<p>​                    2.cbow会将语料库中的每个词都预测一遍，而mask ml只会预测其中的15%的被mask掉的词</p>
<h5 id="2-Bert针对以往的模型存在哪些改进？"><a href="#2-Bert针对以往的模型存在哪些改进？" class="headerlink" title="2.Bert针对以往的模型存在哪些改进？"></a>2.Bert针对以往的模型存在哪些改进？</h5><p>​    1.创造性的提出了mask-ml来解决多层双向 self-attention所出现的信息泄露问题</p>
<p>​    2.position embedding采用了可训练的网络取到了余弦函数公式</p>
<h5 id="3-Bert的双向体现在那里？"><a href="#3-Bert的双向体现在那里？" class="headerlink" title="3.Bert的双向体现在那里？"></a>3.Bert的双向体现在那里？</h5><p>​    Bert的双向并不是说他和transformer相比，模型结构进行了什么更改，而是transformer原始的Encoder部分在使用到语言模型时就是一种双向的结构，而本身transformer之所以不是双向的是因为他并不是每个单词的语言建模，而是一种整体的表征，因此不存在单向双向一说</p>
<h5 id="4-对输入的单词序列，随机地掩盖15-的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号-MASK-替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？"><a href="#4-对输入的单词序列，随机地掩盖15-的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号-MASK-替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？" class="headerlink" title="4.对输入的单词序列，随机地掩盖15%的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号[MASK]替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？"></a>4.对输入的单词序列，随机地掩盖15%的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号[MASK]替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？</h5><p>​    15%随机掩盖的单词并不是都用符号[MASK]替换，掩盖单词操作进行了以下改进：</p>
<p>​        <em>80%用符号[MASK]替换：my dog is hairy -&gt; my dog is [MASK]</em></p>
<p>​        <em>10%用其他单词替换：my dog is hairy -&gt; my dog is apple</em></p>
<p>​        <em>10%不做替换操作：my dog is hairy -&gt; my dog is hairy</em></p>
<h5 id="5-手写muti-attention"><a href="#5-手写muti-attention" class="headerlink" title="5.手写muti-attention"></a>5.手写muti-attention</h5><p>&gt;<br>&gt;<br>&gt;</p>
<p><strong>6、 elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）</strong></p>
<p>（1）<strong>特征提取器</strong>：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。</p>
<p>（2）<strong>单/双向语言模型</strong>：</p>
<ul>
<li>GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</li>
<li>GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2019/05/24/面试——RNN和LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/24/面试——RNN和LSTM/" class="post-title-link" itemprop="url">面试——RNN和LSTM</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-05-24 23:19:54" itemprop="dateCreated datePublished" datetime="2019-05-24T23:19:54+08:00">2019-05-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2019/05/24/面试——RNN和LSTM/" class="post-meta-item leancloud_visitors" data-flag-title="面试——RNN和LSTM" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/05/24/面试——RNN和LSTM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/05/24/面试——RNN和LSTM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>47</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？"><a href="#为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？" class="headerlink" title="为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？"></a>为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？</h4><p><strong>对于RNN：</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RNN梯度消失.png?raw=true" alt></p>
<p><strong>而对于LSTM：</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/LSTM时间梯度反向传播.png?raw=true" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AnchoretY"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">AnchoretY</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">178</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/anchorety" title="GitHub → https://github.com/anchorety" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/yhk7520831104@gmail.com" title="E-Mail → yhk7520831104@gmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AnchoretY</span>
</div>



        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='1' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>










<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'S7MlHMmpqsSeCmfOcq43iVAD-gzGzoHsz',
      appKey     : 'zItfNM4ps7umY5pL3gKAJiYX',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
