<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"anchorety.github.io","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="AnchoretY&#39;s blog">
<meta property="og:url" content="https://anchorety.github.io/page/15/index.html">
<meta property="og:site_name" content="AnchoretY&#39;s blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AnchoretY&#39;s blog">

<link rel="canonical" href="https://anchorety.github.io/page/15/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false
  };
</script>

  <title>AnchoretY's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AnchoretY's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/11/07/概率图模型——HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/07/概率图模型——HMM/" class="post-title-link" itemprop="url">概率图模型——HMM</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-07 12:04:02" itemprop="dateCreated datePublished" datetime="2018-11-07T12:04:02+08:00">2018-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2018/11/07/概率图模型——HMM/" class="post-meta-item leancloud_visitors" data-flag-title="概率图模型——HMM" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/07/概率图模型——HMM/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/07/概率图模型——HMM/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>517</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>马尔科夫模型假设：</p>
<p>​    1.马尔科夫模型认为每一时刻的表现x有一个状态z与其对应</p>
<p>​    2.观测独立性假设：每个时刻的输出都只与当前的状态有关</p>
<p>​    3.贝叶斯公式P(o|λ) = P(λ|o)P(o) / P(λ)</p>
<h4 id="马尔科夫模型的推导过程"><a href="#马尔科夫模型的推导过程" class="headerlink" title="马尔科夫模型的推导过程"></a>马尔科夫模型的推导过程</h4><p>​    对于马尔科夫模型，求解的最终目的是</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">maxP</span>(<span class="params">o1o2...on|λ1λ2...λn</span>)</span></span><br></pre></td></tr></table></figure>
<p>​    由于p(o|λ)是个关于2n各变量的条件概率，并且n不固定，因此没办法进行精确计算。因此马尔科夫链模型采用了一种更加巧妙地方式来进行建模。</p>
<p>​    首先，根据贝叶斯公式可以得：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(<span class="name">o</span><span class="name">|λ) = P(λ|</span><span class="name">o</span>)P(<span class="name">o</span>) / P(λ)</span><br></pre></td></tr></table></figure>
<p>​    λ为给定的输入，因此P(λ)为常数，因此可以忽略.因此</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(<span class="name">o</span><span class="name">|λ) = P(λ|</span><span class="name">o</span>)P(<span class="name">o</span>)</span><br></pre></td></tr></table></figure>
<p>​    而根据观测独立性假设，可以得到</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">P(o1o2...<span class="keyword">on</span>|λ<span class="number">1</span>λ<span class="number">2.</span>..λn) = P(o1|λ<span class="number">1</span>)P(o2|λ<span class="number">2</span>)...P(<span class="keyword">on</span>|λn)</span><br><span class="line">	P(o) = P(o1)P(o2|o1)P(o3|o1,o2)...P(<span class="keyword">on</span>|<span class="keyword">on</span><span class="number">-1</span>,<span class="keyword">on</span><span class="number">-2</span>,...,o1)</span><br></pre></td></tr></table></figure>
<p>​    而由于其次马尔科夫假设，每个输出仅与上一个一个输出有关，那么</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(<span class="name">o</span>) = p(<span class="name">o1</span>)P(<span class="name">o2</span><span class="name">|o1)....P(on|</span><span class="name">on-1</span>)</span><br></pre></td></tr></table></figure>
<p>​    因此最终可得：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(<span class="name">o</span><span class="name">|λ) ~ p(o1)P(o1|</span><span class="name">o2</span>)P(λ|o2)P(o2|o3)P(λ<span class="number">3</span>|o3)...P(on|on-1)P(λ|on)</span><br></pre></td></tr></table></figure>
<p>​    其中，P(λk|ok)为发射概率，P(ok|ok-1)为转移概率。</p>
<h4 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h4><p>​    维特比算法是隐马尔科夫模型最终求解当前表现链最可能对应的状态链使用的一种动态规划算法。主要思想是：</p>
<p>HMM模型训练后的输出为：初始状态概率矩阵、状态转移矩阵、发射概率矩阵三个结果，用来后续进行预测</p>
<p>概率图模型大部分都是这样，输出为几个概率矩阵</p>
<p>而LR模型的输出就为系数矩阵了</p>
<p>我们可以看出来，LR在使用训练好的模型进行与测试效率较高，而HMM使用训练好的模型进行训练时效率较低</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/11/05/机器学习和深度学习在实践中的一些经验/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/05/机器学习和深度学习在实践中的一些经验/" class="post-title-link" itemprop="url">机器学习和深度学习在实践中的一些经验</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-05 14:45:00" itemprop="dateCreated datePublished" datetime="2018-11-05T14:45:00+08:00">2018-11-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习，深度学习/" itemprop="url" rel="index"><span itemprop="name">机器学习，深度学习</span></a>
                </span>
            </span>

          
            <span id="/2018/11/05/机器学习和深度学习在实践中的一些经验/" class="post-meta-item leancloud_visitors" data-flag-title="机器学习和深度学习在实践中的一些经验" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/05/机器学习和深度学习在实践中的一些经验/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/05/机器学习和深度学习在实践中的一些经验/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="使用GBDT算法构造特征"><a href="#使用GBDT算法构造特征" class="headerlink" title="使用GBDT算法构造特征"></a>使用GBDT算法构造特征</h4><p>​    Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题。[1]GBDT思想对于发现多种有区分性的特征和组合特征具有天然优势，可以用来构造新的组合特征。</p>
<p>​    在这篇论文中提出可以<strong>使用GBDT各棵数输出节点的索引号来作为新的特征，对各个树渠道的索引号做one-hot编码，然后与原始的特征一起新的特征输入到模型中往往会起到不错的效果</strong>。</p>
<blockquote>
<p>实践情况</p>
<p>​    1.本人使用这种方式在ctr预估中已经进行过实验，准确率提升2%</p>
<p>​    2.美团在外卖预计送达时间预测中进行了实验，各个时段平均偏差减少了3%</p>
</blockquote>
<p><strong>(1) 超参数选择</strong></p>
<p>a. 首先为了节点分裂时质量和随机性，分裂时所使用的最大特征数目为√n。<br>b. GBDT迭代次数（树的数量）。</p>
<ul>
<li>树的数量决定了后续构造特征的规模，与学习速率相互对应。通常学习速率设置较小，但如果过小，会导致迭代次数大幅增加，使得新构造的特征规模过大。</li>
<li>通过GridSearch+CrossValidation可以寻找到最合适的迭代次数+学习速率的超参组合。<br>c. GBDT树深度需要足够合理，通常在4~6较为合适。</li>
<li>虽然增加树的数量和深度都可以增加新构造的特征规模。但树深度过大，会造成模型过拟合以及导致新构造特征过于稀疏。</li>
</ul>
<p><strong>（2）训练方案</strong></p>
<p>​    将训练数据随机抽样50%，一分为二。前50%用于训练GBDT模型，后50%的数据在通过GBDT输出样本在每棵树中输出的叶子节点索引位置，并记录存储，用于后续的新特征的构造和编码，以及后续模型的训练。如样本x通过GBDT输出后得到的形式如下：x → [25,20,22,….,30,28] ，列表中表示样本在GBDT每个树中输出的叶子节点索引位置。</p>
<p>​    由于样本经过GBDT输出后得到的x → [25,20,22,….,30,28] 是一组新特征，但由于这组新特征是叶子节点的ID，其值不能直接表达任何信息，故不能直接用于ETA场景的预估。为了解决上述的问题，避免训练过程中无用信息对模型产生的负面影响，需要通过独热码（OneHotEncoder）的编码方式对新特征进行处理，将新特征转化为可用的0-1的特征。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/GBDT%20构造新特征.png?raw=true" alt></p>
<p>​    以图5中的第一棵树和第二棵树为例，第一棵树共有三个叶子节点，样本会在三个叶子节点的其中之一输出。所以样本在该棵树有会有可能输出三个不同分类的值，需要由3个bit值来表达样本在该树中输出的含义。图中样本在第一棵树的第一个叶子节点输出，独热码表示为{100}；而第二棵树有四个叶子节点，且样本在第三个叶子节点输出，则表示为{0010}。将样本在每棵树的独热码拼接起来，表示为{1000010}，即通过两棵CART树构造了7个特征，构造特征的规模与GBDT中CART树的叶子节点规模直接相关。</p>
<p>Wide&amp;Deep在推荐中应用</p>
<p>【参考文献】</p>
<ol>
<li>He X, Pan J, Jin O, et al. <a href="https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at Facebook</a>[C]. Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2014: 1-9.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/11/02/web安全——CSRF和SSRF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/02/web安全——CSRF和SSRF/" class="post-title-link" itemprop="url">web安全——CSRF和SSRF</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-02 10:04:02" itemprop="dateCreated datePublished" datetime="2018-11-02T10:04:02+08:00">2018-11-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/web安全/" itemprop="url" rel="index"><span itemprop="name">web安全</span></a>
                </span>
            </span>

          
            <span id="/2018/11/02/web安全——CSRF和SSRF/" class="post-meta-item leancloud_visitors" data-flag-title="web安全——CSRF和SSRF" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/02/web安全——CSRF和SSRF/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/02/web安全——CSRF和SSRF/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="CSRF"><a href="#CSRF" class="headerlink" title="CSRF"></a>CSRF</h4><p>​    CSRF(Cross-site request Forgery，跨站请求伪造)通过伪装成受信任用户请求受信任的网站。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意<span class="selector-pseudo">:scrf</span>漏洞并不需要获取用户的<span class="selector-tag">cookie</span>等信息</span><br></pre></td></tr></table></figure>
<blockquote>
<p>目标：已经登陆了网站的用户</p>
<p>目的：以合法用户的身份来进行一些非法操作</p>
<p>需要条件：</p>
<p>​    1.用户已经登陆了目标网站</p>
<p>​    2.目标用户访问了攻击者构造的url</p>
<p>攻击过程:</p>
<p>​    1.找到存于登陆状态的存在csrf网站的合法用户，向其发送可以构造的恶意链接，诱使其点击</p>
<p>​    2.用户点击该链接，由该合法用户向服务器发出包含恶意链接里隐藏操作（如删除数据、转账等）的请求</p>
<p>​    3.服务器收到已经登录用户的请求，认为是合法用户的主动的操作行为，执行该操作</p>
</blockquote>
<p><strong>典型的csrf实例</strong></p>
<p>​    当你使用网上银行进行转账时，首先需要登录网上银行，点击转账按钮后，会发出<a href="http://www.xxbank.com/pay.php?user=xx&amp;money=100请求，当存在攻击者想要对你进行csrf攻击时，他会向你发送一个邮件或者短信，其中包含可以构造的恶意链接" target="_blank" rel="noopener">http://www.xxbank.com/pay.php?user=xx&amp;money=100请求，当存在攻击者想要对你进行csrf攻击时，他会向你发送一个邮件或者短信，其中包含可以构造的恶意链接</a> <a href="http://www.bank.com/pay,php?user=hack&amp;money=100,并且采用一定的伪装手段诱使你进行点击，当你点击后即向该hack转账100元。" target="_blank" rel="noopener">http://www.bank.com/pay,php?user=hack&amp;money=100,并且采用一定的伪装手段诱使你进行点击，当你点击后即向该hack转账100元。</a></p>
<p><strong>流量中检测csrf的可行性</strong></p>
<p>​    1.对于比较低级的csrf而言，可以直接通过检测请求的referer字段来进行确定是否为scrf。因为在正常scrf页面中应该是在主页等页面跳转得到，而csrf请求一般的referer是空白或者是其他网站，但是该方法可以被绕过。</p>
<p>​    2.完全的检测很难</p>
<p><strong>csrf漏洞修复建议</strong></p>
<p>​    1.验证请求的referer</p>
<p>​    2.在请求中加入随机的token等攻击者不能伪造的信息</p>
<hr>
<h4 id="SSRF"><a href="#SSRF" class="headerlink" title="SSRF"></a>SSRF</h4><p>​    SSRF(Server-Side Request Forgery，服务端请求伪造)是一种有由攻击者构造请求，服务器端发起请求的安全漏洞。</p>
<blockquote>
<p>目标：外网无法访问的服务器系统</p>
<p>目的：获取内网主机或者服务器的信息、读取敏感文件等</p>
<p>形成原因：服务器端提供了从其他服务器获取数据的功能，但没有对目标地址做限制和过滤</p>
<p>攻击过程：</p>
<p>​        1.用户发现存在ssrf漏洞的服务器a的页面访问的url，以及可使用SSRF攻击的参数</p>
<p>​        2.修改要请求参数要请求的文件，将其改成内网服务器b和文件，直接访问</p>
<p>​        3.服务器a接收到要访问的参数所包含的服务器b和文件名，去服务器b下载资源</p>
<p>​        3.对于服务器b，由于是服务器a发起的请求，直接将文件返回给服务器a</p>
<p>​        4.服务器a将该文件或页面内容直接返回给用户</p>
</blockquote>
<p><strong>两种典型的ssrf攻击实例:</strong></p>
<p>​    本地存在ssrf漏洞的页面为：<a href="http://127.0.0.1/ssrf.php?url=http://127.0.0.1/2.php" target="_blank" rel="noopener">http://127.0.0.1/ssrf.php?url=http://127.0.0.1/2.php</a></p>
<p>原始页面的功能为通过GET方式获取url参数的值，然后显示在网页页面上。如果将url参数的值改为<a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a> ，这个页面则会出现百度页面内容。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/SSRF1.png?raw=true" alt></p>
<p>​    因此利用这个漏洞，我们可以将url参数的值设置为内网址，这样可以做到获取内网信息的效果。</p>
<p>​    <strong>探测内网某个服务器是否开启</strong></p>
<p>​    将url参数设置为url=”192.168.0.2:3306”时，可以获取大到该内网主机上是否存在mysql服务。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/SSRF2.png?raw=true" alt></p>
<p>​    <strong>读取内网服务器文件    </strong></p>
<p>​    访问ssrf.php?url=file:///C:/Windows/win.ini 即可读取本地文件</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/SSRF3.png?raw=true" alt></p>
<p><strong>流量中检测SSRF可行性分析：</strong></p>
<p>​    对于只能抓到外网向内网访问的流量的网口来说，从流量中检测SSRF只能从<strong>请求参数异常</strong>或<strong>返回包是否异常、是否包含敏感信息</strong>来进行检测。</p>
<p><strong>SSRF漏洞修复建议:</strong></p>
<p>​    1.限制请求的端口只能是web端口，只允许访问http和https的请求</p>
<p>​    2.限制不能访问内网IP，以防止对内网进行攻击</p>
<p>​    3.屏蔽返回的信息详情</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/11/01/NLP—关键词提取算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/01/NLP—关键词提取算法/" class="post-title-link" itemprop="url">NLP—关键词提取算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-01 11:19:25" itemprop="dateCreated datePublished" datetime="2018-11-01T11:19:25+08:00">2018-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          
            <span id="/2018/11/01/NLP—关键词提取算法/" class="post-meta-item leancloud_visitors" data-flag-title="NLP—关键词提取算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/01/NLP—关键词提取算法/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/01/NLP—关键词提取算法/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>577</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="关键词提取算法"><a href="#关键词提取算法" class="headerlink" title="关键词提取算法"></a>关键词提取算法</h3><hr>
<h4 id="tf-idf-词频-逆文档频率"><a href="#tf-idf-词频-逆文档频率" class="headerlink" title="tf-idf(词频-逆文档频率)"></a>tf-idf(词频-逆文档频率)</h4><p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/TF%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt="tf计算公式"></p>
<p>​    其中count(w)为关键词出现的次数，|Di|为文档中所有词的数量。</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/IDF%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt="tf计算公式"></p>
<p>​    其中，N为所有文档的总数，I(w,Di)表示文档Di是否包含该关键词，，包含则为1，不包含则为0，若词在所有文档中均未出现，则IDF公式中分母则为0，因此在分母上加1做平滑(smooth)</p>
<p>​    最终关键词在文档中的tf-idf值：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/IDF%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt="tf计算公式"></p>
<blockquote>
<p><strong>tf-idf特点：</strong></p>
<p>​    <strong>1.一个词在一个文档中的频率越高，在其他文档中出现的次数越少，tf-idf值越大</strong></p>
<p>​    <strong>2.tf-idf同时兼顾了词频和新鲜度，可以有效地过滤掉常见词</strong></p>
</blockquote>
<p>​            </p>
<hr>
<h4 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h4><p>​    TextRank算法借鉴于Google的PageRank算法，主要在考虑词的关键度主要考虑<strong>链接数量</strong>和<strong>链接质量（链接到的词的重要度）</strong>两个因素。</p>
<p>​    TextRank算法应用到关键词抽取时连个关键点：1.词与词之间的关联没有权重（即不考虑词与词是否相似）  2.每个词并不是与文档中每个次都有链接关系而是只与一个特定窗口大小内词与才有关联关系。</p>
<blockquote>
<p>TextRank特点：</p>
<p>​    1.<strong>不需要使用语料库进行训练</strong>，由一篇文章就可以提取出关键词</p>
<p>​    2.由于TextRank算法涉及到构建词图以及迭代计算，因此<strong>计算速度较慢</strong></p>
<p>​    3.虽然考虑了上下文关系，但是<strong>仍然将频繁次作为关键词</strong></p>
<p>​    <strong>4.TextRank算法具有将一定的将关键词进行合并提取成关键短语的能力</strong></p>
</blockquote>
<p>​    </p>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/30/深度学习——损失函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/30/深度学习——损失函数/" class="post-title-link" itemprop="url">深度学习——损失函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-30 20:43:33" itemprop="dateCreated datePublished" datetime="2018-10-30T20:43:33+08:00">2018-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>

          
            <span id="/2018/10/30/深度学习——损失函数/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习——损失函数" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/30/深度学习——损失函数/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/30/深度学习——损失函数/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><br>在机器机器学习和深度学习中有许多常见的损失函数，主要包括：</p>
<p>​        1.平方差函数MSE（Mean Squared Error）</p>
<p>​        2.交叉熵函数（Cross Entory）    </p>
<blockquote>
<p><strong>损失函数选择的方法：1.线性模型中使用平方误差函数，深度学习使用交叉熵函数</strong></p>
<p>​    <strong>2.平方误差损失函数更适合输出为连续,并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景</strong>。</p>
</blockquote>
<h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><p>​    <strong>效果较好的损失函数：平方误差损失函数</strong></p>
<p>​    <strong>计算公式：</strong></p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/MSE定义公式.png?raw=true" alt></p>
<p>​        其中，y是我们期望的输出，a是神经元的实际输出a=σ(Wx+b)</p>
<p>​    <strong>损失函数求导：</strong>    </p>
<p>​        <img src="https://github.com/AnchoretY/images/blob/master/blog/MES损失函数反向传播公式.png?raw=true" alt></p>
<p>​        这也就是每次进行参数更新量的基数，需要再乘以学习速率</p>
<blockquote>
<p> 为什么深度学习中很少使用MSE作为损失函数？</p>
<p>​    当使用MSE作为损失函数时，有上面求导后的公式可以明显的看出，每次的参数更新量取决于σ′(z) ，由Sigmod函数的性质可知，σ′(z) 在 z 取大部分值时会取到一个非常小的值，因此参数更新会异常的缓慢</p>
</blockquote>
<p>​    </p>
<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p>​    <strong>效果最好的损失函数：交叉熵函数</strong></p>
<p>​    <strong>计算公式：</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/交叉熵公式.png?raw=true" alt></p>
<p>​    如果有多个样本，则整个样本集的平均交叉熵为:</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/交叉熵公式2.png?raw=true" alt></p>
<p>对于二分类而言，交叉损失函数为：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/二分类交叉熵损失函数.png?raw=true" alt></p>
<p>损失函数求导：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC.png?raw=true" alt>        </p>
<p>​    对于b的求导同理。</p>
<p>​    我们可以看出，<strong>交叉熵作为损失函数，梯度中的σ′(z) 被消掉了，另外σ(z)-y就是输出值和真实值之间的误差，误差越大，梯度更新越大，参数更新越快。</strong> </p>
<h4 id="Softmax损失函数"><a href="#Softmax损失函数" class="headerlink" title="Softmax损失函数"></a>Softmax损失函数</h4><h5 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h5><p>​    softmax用于多分类过程中，将多个神经元的输出映射到(0，1)区间，可以看做被分为各个类的概率。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/softmax损失函数.png?raw=true" alt></p>
<p>​    其中，</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/softmax神经元输入.png?raw=true" alt></p>
<h5 id="softmax求导相关推导"><a href="#softmax求导相关推导" class="headerlink" title="softmax求导相关推导"></a>softmax求导相关推导</h5><p><img src="https://github.com/AnchoretY/images/blob/master/blog/神经网络传导图.png?raw=true" alt></p>
<p>​    对于使用作为激活函数的神经网络，最终只输出只有最大的softmax最大的项为1其余项均为0，假设yj=1，带入交叉熵公式中得</p>
<p>​                <script type="math/tex">Loss=-y_{i}loga_i</script></p>
<p>​    去掉了累加和，因为只有一项y为1，其余都为0，而将yj=1带入得</p>
<p>​       <script type="math/tex">Loss=-loga_i</script>            </p>
<p>​    下面我们准备将损失函数对参数求导，参数的形式在该例子中，总共分w41,w42,w43,w51,w52,w53,w61,w62,w63.这些，那么比如我要求出w41,w42,w43的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可，举个例子此时求w41的偏导为:</p>
<p>​                <script type="math/tex">\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}</script></p>
<p>​    其中右边第一项q求导为：</p>
<p>​             <script type="math/tex">\frac{\partial Loss}{\partial a_j} = -\frac{1}{a_j}</script></p>
<p>​    右边第三项求导为：</p>
<p>​              <script type="math/tex">\frac{\partial z_j}{\partial w_ij} = x_{i}</script></p>
<p>​    核心是求右侧第二项：$\frac{\partial a_j}{\partial z_j}$，这里我们分两种情况进行讨论</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/softmax求导.png?raw=true" alt></p>
<p>​    将前两项的结果进行连乘得：</p>
<p>​        <img src="https://github.com/AnchoretY/images/blob/master/blog/softmax求导2.png?raw=true" alt></p>
<p>​    而对于分类问题，只会有一个$y_i$为1，其余均为0，因此，对于分类问题：</p>
<p>​        <img src="https://github.com/AnchoretY/images/blob/master/blog/softmax%E6%B1%82%E5%AF%BC3.png?raw=true" alt></p>
<p>​    最终：</p>
<p>​        <script type="math/tex">\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}==(a_{i}-y{i})x{i}</script></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/30/深度学习——优化器optimzer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/30/深度学习——优化器optimzer/" class="post-title-link" itemprop="url">深度学习——优化器optimzer</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-30 20:19:24" itemprop="dateCreated datePublished" datetime="2018-10-30T20:19:24+08:00">2018-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/面试/" itemprop="url" rel="index"><span itemprop="name">面试</span></a>
                </span>
            </span>

          
            <span id="/2018/10/30/深度学习——优化器optimzer/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习——优化器optimzer" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/30/深度学习——优化器optimzer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/30/深度学习——优化器optimzer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    在机器学习和深度学习中，选择合适的优化器不仅可<strong>以加快学习速度</strong>，而且可以<strong>避免在训练过程中困到的鞍点</strong>。</p>
<h4 id="1-Gradient-Descent-（GD）"><a href="#1-Gradient-Descent-（GD）" class="headerlink" title="1.Gradient Descent （GD）"></a>1.Gradient Descent （GD）</h4><p>​    BGD是一种使用全部训练集数据来计算损失函数的梯度来进行参数更新更新的方式，梯度更新计算公式如下：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/BGD%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F.png?raw=true" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">  params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<blockquote>
<p>缺点：</p>
<p>1.<strong>由于在每一次更新中都会对整个数据及计算梯度，因此计算起来非常慢</strong>，在大数据的情况下很难坐到实时更新。</p>
<p>​    <strong>2.Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。</strong></p>
</blockquote>
<h4 id="2-Stochastic-Gradient-Descent-SGD"><a href="#2-Stochastic-Gradient-Descent-SGD" class="headerlink" title="2.Stochastic Gradient Descent(SGD)"></a>2.Stochastic Gradient Descent(SGD)</h4><p>​    SGD是一种最常见的优化方法，这种方式<strong>每次只计算当前的样本的梯度，然后使用该梯度来对参数进行更新</strong>，其计算方法为：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/SGD%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99.png?raw=true" alt="SGD计算公式"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<p>​    随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。    </p>
<blockquote>
<p><strong>缺点：1.存在比较严重的震荡</strong></p>
<p>​    <strong>2.容易收敛到局部最优点,但有时也可能因为震荡的原因跳过局部最小值</strong></p>
</blockquote>
<h4 id="3-Batch-Gradient-Descent-（BGD）"><a href="#3-Batch-Gradient-Descent-（BGD）" class="headerlink" title="3.Batch Gradient Descent （BGD）"></a>3.Batch Gradient Descent （BGD）</h4><p>​    BGD <strong>每一次利用一小批样本，即 n 个样本进行计算</strong>，这样它可以<strong>降低参数更新时的方差，收敛更稳定</strong>，<strong>另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算</strong>。</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/MBGD%E5%8F%82%E6%95%B0%E5%85%AC%E5%BC%8F.png?raw=true" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<p>​    <strong>参数值设定：batch_szie一般在设置在50~256之间</strong></p>
<blockquote>
<p><strong>缺点：1.不能保证很好的收敛性。</strong></p>
<p>​    <strong>2.对所有参数进行更新时使用的是完全相同的learnnning rate</strong></p>
</blockquote>
<p>​    这两个缺点也是前面这几种优化方式存在的共有缺陷，下面的优化方式主要就是为了晚上前面这些问题</p>
<h4 id="4-Momentum"><a href="#4-Momentum" class="headerlink" title="4.Momentum"></a>4.Momentum</h4><blockquote>
<p><strong>核心思想：用动量来进行加速</strong></p>
<p><strong>适用情况：善于处理稀疏数据</strong></p>
</blockquote>
<p>​    为了克服 SGD 振荡比较严重的问题，Momentum 将物理中的动量概念引入到SGD 当中，通过积累之前的动量来替代梯度。即:</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Momentum%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt="SGD计算公式"></p>
<p>​    其中，γ 表示动量大小，μ表示学习速率大小。</p>
<p>​    相较于 SGD，Momentum 就相当于在从山坡上不停的向下走，当没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。也就是说，<strong>在训练的时候，在梯度方向不变的维度上，训练速度变快，梯度方向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。</strong>    </p>
<p>​    <strong>超参数设定：一般 γ 取值 0.9 左右。</strong></p>
<blockquote>
<p>缺点：<strong>这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。</strong></p>
</blockquote>
<h4 id="5-Adaptive-gradient-algorithm（Adagrad）"><a href="#5-Adaptive-gradient-algorithm（Adagrad）" class="headerlink" title="5.Adaptive gradient algorithm（Adagrad）"></a>5.Adaptive gradient algorithm（Adagrad）</h4><blockquote>
<p><strong>核心思想：对学习速率添加约束，前期加速训练，后期提前结束训练以避免震荡，减少了学习速率的手动调节</strong></p>
<p><strong>适用情况：这个算法可以对低频参数进行较大的更新，高频参数进行更小的更新，对稀疏数据表现良好，提高了SGD的鲁棒性，善于处理非平稳目标</strong></p>
</blockquote>
<p>​    相较于 SGD，Adagrad 相当于对学习率多加了一个约束，即：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adagrad参数更新公式.png?raw=true" alt="SGD计算公式"></p>
<p>​    对于经典的SGD：</p>
<p>​        <img src="https://github.com/AnchoretY/images/blob/master/blog/SGD与Adagrad对比.png?raw=true" alt></p>
<p>​    而对于Adagrad：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/Adagrad和SGD对比.png?raw=true" alt></p>
<p>其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10-7    </p>
<p>​    <strong>超参数设定：一般η选取0.01，ε一般设置为10-7</strong></p>
<p>​                </p>
<blockquote>
<p>缺点：分母会不断积累，这样学习速率就会变得非常小</p>
</blockquote>
<h4 id="6-Adadelta"><a href="#6-Adadelta" class="headerlink" title="6.Adadelta"></a>6.Adadelta</h4><p>​    超参数设置：p 0.9</p>
<p>​    Adadelta算法是基于Adagrad算法的改进算法，主要改进主要包括下面两点：</p>
<blockquote>
<p>1.将分母从G换成了<strong>过去梯度平方的衰减的平均值</strong></p>
<p>2.将初始学习速率换成了<strong>RMS[Δθ]</strong>(梯度的均方根)</p>
</blockquote>
<h5 id="part-one"><a href="#part-one" class="headerlink" title="part one"></a>part one</h5><p>​    (1) 将累计梯度信息从<strong>全部的历史信息</strong>变为<strong>当前时间窗口向前一个时间窗口内的累积</strong>：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adadelta改进1.png?raw=true" alt></p>
<p>​    (2)将上述公式进行开方，作为每次迭代更新后的学习率衰减系数</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/adadelta改进2.png?raw=true" alt></p>
<p>记</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adadelta改进3.png?raw=true" alt></p>
<p>其中 是为了防止分母为0加上的一个极小值。</p>
<p>​    这里解决了梯度一直会下降到很小的值得问题。</p>
<h5 id="part-two"><a href="#part-two" class="headerlink" title="part two"></a>part two</h5><p>​    将原始的学习速率换为参数值在前一时刻的RMS</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adadelta最终更新公式.png?raw=true" alt></p>
<p>​    因为原始的学习速率已经换成了前一时刻的RMS值，因此，<strong>对于adadelta已经不需要选择初始的学习速率</strong></p>
<h4 id="7-RMSprop"><a href="#7-RMSprop" class="headerlink" title="7.RMSprop"></a>7.RMSprop</h4><p>​    RMSprop 与 Adadelta 的第一种形式相同：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RMSprop参数更新公式.png?raw=true" alt></p>
<p>​    <strong>使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率η</strong></p>
<p>​    <strong>超参数设置：建议设定 γ 为 0.9, 学习率 η 为 0.001</strong></p>
<h4 id="7-Adam"><a href="#7-Adam" class="headerlink" title="7.Adam"></a>7.Adam</h4><blockquote>
<p><strong>核心思想：结合了Momentum动量加速和Adagrad对学习速率的约束</strong></p>
<p><strong>适用情况：各种数据，前面两种优化器适合的数据Adam都更效果更好</strong>，</p>
</blockquote>
<p>​    Adam 是一个结合了 Momentum 与 Adagrad 的产物，它既考虑到了利用动量项来加速训练过程，又考虑到对于学习率的约束。利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。其公式为:    </p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/Adam%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F1.png?raw=true" alt="SGD计算公式"></p>
<p>​    其中：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adam%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F2.png?raw=true" alt="SGD计算公式"></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Adam%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F3.png?raw=true" alt="SGD计算公式"></p>
<h4 id="总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用"><a href="#总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用" class="headerlink" title="总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用"></a>总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用</h4><h3 id="如何选择最优化算法"><a href="#如何选择最优化算法" class="headerlink" title="如何选择最优化算法"></a>如何选择最优化算法</h3><p>​    1.如果数据是稀疏的，就是自适应系列的方法 Adam、Adagrad、Adadelta</p>
<p>​    2.Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum</p>
<p>​    3.随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>​    整体来说Adam是最好的选择</p>
<p><strong>参考文献:深度学习在美团点评推荐系统中的应用</strong></p>
<p><a href="https://blog.csdn.net/yukinoai/article/details/84198218" target="_blank" rel="noopener">https://blog.csdn.net/yukinoai/article/details/84198218</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/27/经典机器学习算法——KMeans/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/27/经典机器学习算法——KMeans/" class="post-title-link" itemprop="url">经典机器学习算法——KMeans</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-27 22:03:14" itemprop="dateCreated datePublished" datetime="2018-10-27T22:03:14+08:00">2018-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2018/10/27/经典机器学习算法——KMeans/" class="post-meta-item leancloud_visitors" data-flag-title="经典机器学习算法——KMeans" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/27/经典机器学习算法——KMeans/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/27/经典机器学习算法——KMeans/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="KMeans算法"><a href="#KMeans算法" class="headerlink" title="KMeans算法"></a>KMeans算法</h3><hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/23/pyspark学习心得/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/23/pyspark学习心得/" class="post-title-link" itemprop="url">pyspark学习心得</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-23 10:36:53" itemprop="dateCreated datePublished" datetime="2018-10-23T10:36:53+08:00">2018-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a>
                </span>
            </span>

          
            <span id="/2018/10/23/pyspark学习心得/" class="post-meta-item leancloud_visitors" data-flag-title="pyspark学习心得" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/23/pyspark学习心得/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/23/pyspark学习心得/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    </p>
<h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><p>​    Spark中对于一个RDD执行多次算子的<strong>默认</strong>原理是这样的：<strong>每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍</strong>，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>
<p>​    因此对于这种情况，我们的建议是：<strong>对多次使用的RDD进行持久化</strong>。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。<strong>以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作</strong>。</p>
<p>​    spark中的持久化操作主要分为两种：<strong>persist和cache</strong>。<strong>cache相当于使用MEMORY_ONLY级别的persist操作，而persist更灵活可以任意指定persist的级别。</strong></p>
<blockquote>
<h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul>
<li>默认情况下，<strong>性能最高</strong>的当然是<strong>MEMORY_ONLY</strong>，但前提是你的<strong>内存必须足够足够大</strong>，可以绰绰有余地存放下整个RDD的所有数据。<strong>因为不进行序列化与反序列化操作，就避免了这部分的性能开销</strong>；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，<strong>如果RDD中数据比较多</strong>时（比如几十亿），直接用这种持久化级别，<strong>会导致JVM的OOM内存溢出异常</strong>。</li>
<li><strong>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别</strong>。该级别<strong>会将RDD数据序列化后再保存在内存中</strong>，此时<strong>每个partition仅仅是一个字节数组</strong>而已，大大减少了对象数量，并降低了内存占用。这种级别<strong>比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销</strong>。<strong>但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的</strong>。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>
<li><strong>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略</strong>，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时<strong>该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘</strong>。</li>
<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>
</ul>
</blockquote>
<h3 id="提高性能的算子"><a href="#提高性能的算子" class="headerlink" title="提高性能的算子"></a>提高性能的算子</h3><h5 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h5><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，<strong>将RDD中的数据压缩到更少的partition中去</strong>。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，<strong>只要使用更少的task即可处理完所有的partition</strong>。在某些场景下，对于性能的提升会有一定的帮助。</p>
<h4 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h4><p>​    大多数<strong>spark作业的性能主要就消耗在shuffle环节</strong>，因为shuffle中包含了大量的磁盘IO、序列化、网络数据传输等操作。<strong>但是影响一个spark作业性能的主要因素还是代码开发、资源参数、以及数据倾斜</strong>，shuffle调优在优化spark作业性能中只能起较小的作用。</p>
<blockquote>
<h4 id="shuffle操作速度慢的原因"><a href="#shuffle操作速度慢的原因" class="headerlink" title="shuffle操作速度慢的原因"></a>shuffle操作速度慢的原因</h4></blockquote>
<p>​    </p>
<p>Pyspark使用过程中的一些小Tips：</p>
<blockquote>
<p>1、RDD.repartition(n)可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，可以减少下面操作的时间。其中的n值看cpu的个数，一般大于2倍cpu，小于1000。</p>
<p>2、<strong>Action不能够太多</strong>，每一次的action都会将以上的taskset划分一个job，这样当job增多，而其中task并不释放，会占用更多的内存，使得gc拉低效率。</p>
<p>3、<strong>在shuffle前面进行一个过滤，减少shuffle数据，并且过滤掉null值，以及空值</strong>。</p>
<p>4、<strong>groupBy尽量通过reduceBy替代</strong>。reduceBy会在work节点做一次reduce，在整体进行reduce，相当于做了一次hadoop中的combine操作，而combine操作和reduceBy逻辑一致，这个groupBy不能保证。</p>
<p>5、<strong>做join的时候，尽量用小RDD去join大RDD.</strong></p>
<p>6、<strong>避免collect的使用</strong>。因为collect如果数据集超大的时候，会通过各个work进行收集，io增多，拉低性能，因此当数据集很大时要save到HDFS。</p>
<p>7、RDD如果后面使用迭代，建议cache，但是一定要估计好数据的大小，避免比cache设定的内存还要大，如果大过内存就会删除之前存储的cache，可能导致计算错误，如果想要完全的存储可以使用persist（MEMORY_AND_DISK），因为cache就是persist（MEMORY_ONLY）。</p>
<p>8、设置spark.cleaner.ttl，定时清除task，因为job的原因可能会缓存很多执行过去的task，所以定时回收可能避免集中gc操作拉低性能。</p>
</blockquote>
<p>​    </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/22/经典机器学习算法——逻辑回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/22/经典机器学习算法——逻辑回归/" class="post-title-link" itemprop="url">经典机器学习算法——逻辑回归</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-22 21:44:43" itemprop="dateCreated datePublished" datetime="2018-10-22T21:44:43+08:00">2018-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2018/10/22/经典机器学习算法——逻辑回归/" class="post-meta-item leancloud_visitors" data-flag-title="经典机器学习算法——逻辑回归" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/22/经典机器学习算法——逻辑回归/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/22/经典机器学习算法——逻辑回归/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h3><hr>
<p>​    逻辑回归算法是一种根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类的模型。逻辑回归首先赋予每个特征相同的回归参数，然后使用<strong>梯度下降算法</strong>来不断优化各个回归参数，最终根据回归参数来对新样本进行进行预测。</p>
<blockquote>
<p><strong>注意：虽然名叫逻辑回归，但是实际上是一种分类模型</strong></p>
</blockquote>
<p><strong>工作原理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每个回归系数初始化为 <span class="number">1</span></span><br><span class="line">重复 R 次:</span><br><span class="line">    计算整个数据集的梯度</span><br><span class="line">    使用 步长 x 梯度 更新回归系数的向量(梯度下降)</span><br><span class="line">返回回归系数</span><br></pre></td></tr></table></figure>
<p><strong>逻辑回归算法的特点</strong></p>
<blockquote>
<p>优点：计算代价低，可解释性强</p>
<p>缺点：容易欠拟合，分类精度可能不高</p>
<p>使用数据类型：数值型数据和标称型数据(只存在是和否两种结果的将数据)</p>
</blockquote>
<p><strong>sigmod函数</strong></p>
<p>​    sigmod是一种近似的越阶函数，可以将任意的输入值，然后将其映射为0到1之间的值，其公式和函数图像如下图：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/sigmod%E5%85%AC%E5%BC%8F.png?raw=true" alt="sigmod公式"></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/sigmod%E5%87%BD%E6%95%B0.png?raw=true" alt="sigmod函数"></p>
<p>​    在逻辑回归中先使用每个特征乘以一个回归系数，将其乘积作为sigmod函数中的z，即</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84z.png?raw=true" alt="逻辑回归中的z"></p>
<p>​    然后将其得到的值用sigmod函数映射到0到1，可以理解为被分为1类的概率。</p>
<p><strong>梯度上升算法</strong></p>
<p>​    要找到某个函数的最大值，最好的方式就是沿着梯度方向不断地去寻找，如果梯度记做▽ ，则函数 f(x, y) 的梯度由下式表示:</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/%E6%A2%AF%E5%BA%A6%E5%85%AC%E5%BC%8F.png?raw=true" alt="sigmod函数"></p>
<p>这个梯度意味着要沿 x 的方向移动 <a href="https://github.com/apachecn/AiLearning/blob/dev/img/ml/5.Logistic/LR_6.png" target="_blank" rel="noopener"><img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_6.png" alt="f(x, y)对x求偏导"></a> ，沿 y 的方向移动 <a href="https://github.com/apachecn/AiLearning/blob/dev/img/ml/5.Logistic/LR_7.png" target="_blank" rel="noopener"><img src="https://github.com/apachecn/AiLearning/raw/dev/img/ml/5.Logistic/LR_7.png" alt="f(x, y)对y求偏导"></a> 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。<img src="https://github.com/apachecn/AiLearning/blob/dev/img/ml/5.Logistic/LR_8.png?raw=true" alt="梯度上升图"></p>
<p>​    上图展示了整个梯度上升的过程，梯度上升算法在到到每个点后都会从新估计移动的方向，而这个方向就是梯度方向，移动的速度大小由参数α控制。</p>
<p><strong>训练过程</strong></p>
<p>​    训练算法：使用梯度上升寻找最佳参数</p>
<blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> 每个回归系数初始化为 1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 重复 R 次:</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">     计算整个数据集的梯度</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">     使用 步长 x 梯度 更新回归系数的向量(梯度下降)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 返回回归系数</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    其中步长为超参数alpha，而梯度的计算如下：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/%E6%A2%AF%E5%BA%A61.png?raw=true" alt="梯度1"></p>
<p>即每个点的数据和其输入数据相同。因此权重的更新可以使用：</p>
<p>​        <strong>w:=w+α error x</strong></p>
<p>其中α为常数步长，error为在当前参数值下与目标值的误差经过sigmod函数处理后的值，x为当当前样本的输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmod</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/<span class="number">1</span>+np.exp(-x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscend</span><span class="params">(dataSet,labelSet,alpha,maxCycles)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将输入的数据转为向量格式</span></span><br><span class="line">    dataMat = np.mat(dataSet)</span><br><span class="line">    labelMat = np.mat(labelSet).tramsponse()</span><br><span class="line">    <span class="comment">#获取输入数据的维度</span></span><br><span class="line">    m,n = np.shape(dataMat)</span><br><span class="line">    <span class="comment">#初始化回归系数</span></span><br><span class="line">    weights = np.ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#对回归系数进行迭代更新</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        <span class="comment">#计算使用当前回归系数LR的hx值，结果为(m,1)维向量</span></span><br><span class="line">        h = sigmod(dataMat*weights)</span><br><span class="line">        <span class="comment">#计算误差</span></span><br><span class="line">        error = labelMat-h</span><br><span class="line">        <span class="comment">#根据梯度进行回归系数更新</span></span><br><span class="line">        weights = weights + alpha*dataMat.transponse()*error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p><strong>随机梯度上升算法</strong></p>
<p>​    随机梯度上升算法起到的作用和一般的梯度上升算法是一样的，只是由于一般的梯度上升算法在每次更新回归系数时需要遍历整个数据集，因此当数据量变动很大时，一般的梯度上升算法的时间消耗将会非常大，因此提出了<strong>每次只使用一个样本来进行参数更新</strong>的方式，<strong>随机梯度上升（下降）</strong>。</p>
<blockquote>
<p>随机梯度上升算法的特点：</p>
<p>​    1.每次参数更新只使用一个样本，速度快</p>
<p>​    2.可进行在线更新，是一个<strong>在线学习算法</strong>（也是由于每次回归系数更新只使用一个样本）</p>
</blockquote>
<p>工作原理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所有回归系数初始化为 <span class="number">1</span></span><br><span class="line">对数据集中每个样本</span><br><span class="line">    计算该样本的梯度</span><br><span class="line">    使用 alpha x gradient 更新回归系数值</span><br><span class="line">返回回归系数值</span><br></pre></td></tr></table></figure>
<p>初步随机梯度下降代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocgradAscend</span><span class="params">(dataSet,labelSet)</span>:</span></span><br><span class="line">    <span class="comment">#1.这里没有转换成矩阵的过程，整个过程完全都是在Numpy数据完成的</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    m,n = np.shape(dataSet)</span><br><span class="line"></span><br><span class="line">    weights = np.ones((n,<span class="number">1</span>))</span><br><span class="line">	<span class="comment">#2.回归系数更新过程中的h、error都是单个值，而在一般梯度上升算法中使用的是矩阵操作</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        h = np.sigmod(dataSet[i]*weights)</span><br><span class="line">        error = h - labelSet[i]</span><br><span class="line">        weights = weights + alpha*error*dataSet[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>但是这种随机梯度上升算法在在实际的使用过程出现了<strong>参数最后难以收敛，最终结果周期性波动</strong>的问题，针对这种问题我们对这个问题将随机梯度下降做了下面<strong>两种优化</strong></p>
<p>​    1.改进为 alpha 的值，alpha 在每次迭代的时候都会调整。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。</p>
<p>​    </p>
<p>​    2.修改randomIndex的值，<strong>从以前顺序的选择样本</strong>更改为<strong>完全随机的方式来选择用于回归系数的样本</strong>，每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。</p>
<p>最终版随机梯度下降：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocgradAscend</span><span class="params">(dataSet,labelSet,numsIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">	</span><br><span class="line">	m,n = np.shape(dataSet)</span><br><span class="line">	weights = np.ones(n,<span class="number">1</span>)</span><br><span class="line">	alpha = <span class="number">0.01</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numsIter):</span><br><span class="line">        <span class="comment">#生成数据的索引</span></span><br><span class="line">		dataIndex = range(m)</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment">#alpha会随着i和j的增大不断减小</span></span><br><span class="line">			alpha = <span class="number">4</span>/(i+j+<span class="number">1.0</span>)+<span class="number">0.001</span> <span class="comment"># alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001</span></span><br><span class="line">            <span class="comment">#生成随机选择要进行回归系数更新的数据索引号</span></span><br><span class="line">            randomIndex = np.random.uniform(<span class="number">0</span>,len(dataIndex))</span><br><span class="line">            h = sigmod(np.sum(dataSet[dataIndex[randomIndex]]*weights))</span><br><span class="line">            error = h - dataSet[dataIndex[randomIndex]]*weights</span><br><span class="line">            weights = weights + alpha*error*dataSet[dataIndex[randomIndex]]</span><br><span class="line">            <span class="comment">#在数据索引中删除</span></span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randomIndex])</span><br><span class="line">     <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p><strong>预测过程    </strong>    </p>
<p>​    LR模型的预测过程很简单，只需要根据训练过程训练出的参数，计算sigmod(w*x),如果这个值大于0.5，则分为1，反之则为0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classfyLR</span>:</span>(inX,weights)</span><br><span class="line">    prob = sigmod(np.sum(weights*inX))</span><br><span class="line">    <span class="keyword">if</span> prob&gt;=<span class="number">0.5</span></span><br><span class="line">    	<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：这里的阈值其实是可以自行设定的</p>
</blockquote>
<h4 id="一些其他相关问题"><a href="#一些其他相关问题" class="headerlink" title="一些其他相关问题"></a>一些其他相关问题</h4><hr>
<p><strong>1.LR模型和最大熵模型</strong></p>
<blockquote>
<p>​    (1).logistic回归模型和最大熵模型都属于对数线性模型</p>
<p>​    (2).当最大熵模型进行二分类时，最大熵模型就是逻辑回归模型</p>
<p>​    (3) 学习他们的模型一般采用极大似估计或正则化的极大似然估计</p>
<p>​    (4)二者可以形式化为无约束条件下的最优化问题</p>
</blockquote>
<p><strong>2.LR模型的多分类</strong></p>
<p>​    逻辑回归也可以作用于多分类问题，对于多分类问题，处理思路如下：将多分类问题看做多个二分类，然后在各个sigmod得到的分数中区最大的值对应的类作为最终预测标签。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://anchorety.github.io/2018/10/21/概率图模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/10/21/概率图模型/" class="post-title-link" itemprop="url">概率图模型</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-10-21 11:45:33" itemprop="dateCreated datePublished" datetime="2018-10-21T11:45:33+08:00">2018-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-19 12:05:32" itemprop="dateModified" datetime="2021-02-19T12:05:32+08:00">2021-02-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2018/10/21/概率图模型/" class="post-meta-item leancloud_visitors" data-flag-title="概率图模型" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/10/21/概率图模型/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/10/21/概率图模型/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>619</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    概率图模型是<strong>用图来表示变量概率依赖关系</strong>的理论，结合概率论与图论的知识，<strong>利用图来表示与模型有关的变量的联合概率分布</strong></p>
<p>​    基本概率图模型主要包括<strong>贝叶斯网络</strong>、<strong>马尔科夫网络</strong>和<strong>隐马尔科夫</strong>网络三种类型。</p>
<p>​    基本的Graphical Model 可以大致分为两个类别：贝叶斯网络(Bayesian Network)和马尔可夫随机场(Markov Random Field)。它们的主要区别在于采用不同类型的图来表达变量之间的关系：<strong>贝叶斯网络采用有向无环图</strong>(Directed Acyclic Graph)来表达因果关系，<strong>马尔可夫随机场则采用无向图</strong>(Undirected Graph)来表达变量间的相互作用。这种结构上的区别导致了它们在建模和推断方面的一系列微妙的差异。<strong>一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积</strong>。而<strong>对于马尔可夫场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布</strong>——这一点往往在实际应用中给参数估计造成非常大的困难。</p>
<h3 id="概率图模型的表示理论"><a href="#概率图模型的表示理论" class="headerlink" title="概率图模型的表示理论"></a>概率图模型的表示理论</h3><p>​    概率图模型的表示由<strong>参数和结构</strong>两部分组成。根据边有无方向性，可分为下面三种：</p>
<p>​    <strong>a、有向图模型—贝叶斯网络</strong></p>
<p>​    <strong>b、无向图模型—马尔科夫网络</strong></p>
<p>​    <strong>c、局部有向模型—条件随机场和链图</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AnchoretY"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">AnchoretY</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">172</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/anchorety" title="GitHub → https://github.com/anchorety" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/yhk7520831104@gmail.com" title="E-Mail → yhk7520831104@gmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AnchoretY</span>
</div>



        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='1' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>










<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'S7MlHMmpqsSeCmfOcq43iVAD-gzGzoHsz',
      appKey     : 'zItfNM4ps7umY5pL3gKAJiYX',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
