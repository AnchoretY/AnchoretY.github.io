<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习——BERT]]></title>
    <url>%2F2019%2F05%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94BERT%2F</url>
    <content type="text"><![CDATA[什么是BERT？​ BERT(Bidirectional Encoder Representations from Transformer)源自论文Google2018年的论文”Pre-training of Deep bidirectional Transformers for Language Understanding“，其前身是Google在2017年推出的transormfer模型。 ​ 核心点为： 1.预训练 2.双向的编码表征 3.深度的Transformer 4.以语言模型为训练目标 BERT的两个任务​ 1.语言模型，根据词的上下文预测这个词是什么 ​ 2.下一句话预测（NSP）模型接收成对的句子作为输入，并学习预测该对中的第二个句子是否是原始文档中的后续句子 双向attention​ 在之前常见的attention结构都是单向的attention，顺序的从左到右，而借鉴Bi_LSTM和LSTM的关系，如果能将attention改为双向不是更好吗？ ​ 将attention改为双向遇到的最大问题就是深度的增加导致信息泄露问题，如下图： 解决该问题主要的解决方案有两种： 1.多层单向RNN，独立建模(ELMo)。前项后项信息不公用，分别为两个网络 2.Mask ML(BERT采用) ​ 解决的问题：多层的self-attention信息泄漏问题 ​ 随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。 ​ 在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。最终的损失函数只计算被mask掉那个token。 ​ Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。] BERT整体结构Input representation​ 输入表征主要由下面三部分加和而成： ​ 1.词的向量化编码 就是常用的词向量化，例如Word2vec等或者直接embedding ​ 2.段编码 使用[CLS]、[SEP]做标记区分段，每个段用于其各自的向量Ei，属于A段的每个词都要加EA，属于B段的每个词都要加EB… 主要是为了下句话预测任务 ​ 3.位置编码 和transormer不同的是，这里的position embedding是可训练的，不再是适用固定的公式计算 Transformer Encoder​ 这里还会沿用Transformer的Encoder网络，首先是一个Multi-head self-attention，然后接一个Position-wise前馈网络，并且每个结构上都有残差连接. Losses​ Losses就是两部分，一部分是语言模型的任务的损失，一部分是上下文是否连续的损失。 ​ 语言模型的任务的损失 ​ 对于Mask ML随机选择进行mask的15%的词，是否正确做损失函数(一般为交叉熵损失函数) ​ 上下文是否连续损失 ​ 二分类的损失函数，连续/不连续 常见问题1.Bert的mask ml相对Cbow有什么相同和不同？​ 相同点：两种方式都采用了使用一个词周围词去预测其自身的模式。 ​ 不同点：1.mask ml是应用在多层的bert中，用来防止 transformer 的全局双向 self-attention所造成的信息泄露的问题；而Cbow时使用在单层的word2vec中，虽然也是双向，但并不存在该问题 ​ 2.cbow会将语料库中的每个词都预测一遍，而mask ml只会预测其中的15%的被mask掉的词 2.Bert针对以往的模型存在哪些改进？​ 1.创造性的提出了mask-ml来解决多层双向 self-attention所出现的信息泄露问题 ​ 2.position embedding采用了可训练的网络取到了余弦函数公式 3.Bert的双向体现在那里？​ Bert的双向并不是说他和transformer相比，模型结构进行了什么更改，而是transformer原始的Encoder部分在使用到语言模型时就是一种双向的结构，而本身transformer之所以不是双向的是因为他并不是每个单词的语言建模，而是一种整体的表征，因此不存在单向双向一说 4.对输入的单词序列，随机地掩盖15%的单词，然后对掩盖的单词做预测任务，预训练阶段随机用符号[MASK]替换掩盖的单词，而下游任务微调阶段并没有Mask操作，会造成预训练跟微调阶段的不匹配，如何金额绝？​ 15%随机掩盖的单词并不是都用符号[MASK]替换，掩盖单词操作进行了以下改进： ​ 80%用符号[MASK]替换：my dog is hairy -&gt; my dog is [MASK] ​ 10%用其他单词替换：my dog is hairy -&gt; my dog is apple ​ 10%不做替换操作：my dog is hairy -&gt; my dog is hairy 5.手写muti-attention 6、 elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert） （1）特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。 （2）单/双向语言模型： GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。 GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试——RNN和LSTM]]></title>
    <url>%2F2019%2F05%2F24%2F%E9%9D%A2%E8%AF%95%E2%80%94%E2%80%94RNN%E5%92%8CLSTM%2F</url>
    <content type="text"><![CDATA[为什么RNN会造成梯度消失和梯度爆炸，而LSTM可以防止梯度消失？对于RNN： 而对于LSTM：]]></content>
      <tags>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——旋转数组]]></title>
    <url>%2F2019%2F05%2F24%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[旋转数组的最小数字​ 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0 123456789101112131415161718192021222324class Solution: def minNumberInRotateArray(self, rotateArray): # write code here if len(rotateArray) == 0: return 0 left = 0 right = len(rotateArray) - 1 def find_rotate_index(arr, left, right): if right-left &lt;= 1: return right mid = (left + right) &gt;&gt; 1 # 当左半边有序 if arr[left] &lt;= arr[mid]: return find_rotate_index(arr,mid,right) else: return find_rotate_index(arr,left,mid) min_index = find_rotate_index(rotateArray, left, right) return rotateArray[min_index]]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索二叉树删除节点]]></title>
    <url>%2F2019%2F05%2F20%2F%E6%90%9C%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%91%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[首先要找到目标数值，然后看该节点的左右子树情况， ​ 1.没有左子树，返回其右子树 ​ 2.没有右子树，返回其左子树 ​ 3.左右子树都有，查找到其右子树的最小值的节点，替换掉被删除的节点，并删除找到的最小节点 1234567891011121314151617181920class Solution(object): def deleteNode(self, root, key): """ :type root: TreeNode :type key: int :rtype: TreeNode """ if not root: return None if root.val == key: if not root.right: left = root.left return left else: right = root.right while right.left: right = right.left root.val, right.val = right.val, root.val root.left = self.deleteNode(root.left, key) root.right = self.deleteNode(root.right, key) return root]]></content>
      <tags>
        <tag>面试</tag>
        <tag>机试，算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试常见的计算机基础知识]]></title>
    <url>%2F2019%2F05%2F20%2F%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%A7%81%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.三次握手过程以及四次挥手过程 2.进程和线程的区别 3.操作系统的页式存储是怎么样的？有什么优缺点 Time_wait和close_wait是什么。拥塞控制和流量控制。 快排]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据——spark调优]]></title>
    <url>%2F2019%2F05%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94spark%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark调优核心参数设置 num-executors 该参数一定被设置， 为当前Application生产指定个数的Executors 实际生产环境分配80个左右的Executorsexecutor-memory 与JVM OOM(内存溢出)紧密相关，很多时候甚至决定了spark运行的性能 实际生产环境下建议8GB左右 若运行在yarn上，内存占用量不超过yarn的内存资源的50%excutor-cores 决定了在Executor中能够并行执行的Task的个数 实际生产环境建议2~3个 driver-memory 作为驱动，默认是1GB 生产环境一般设置4GB spark.default.parallelism 建议至少设置100个，官方推荐是num-executors*excutor-cores的2~3倍spark.storage.memoryFraction 用于存储的比例默认占用60%，如果计算比较依赖于历史数据，则可以适当调高该参数，如果计算严重依赖于shuffle，则需要降低该比例spark.shuffle.memoryFraction 用于shuffle的内存比例，默认占用20% 如果计算严重依赖于shuffle,则需要提高该比例 spark生态的主要组件： spark core，任务调度，内存管理，错误恢复spark sql，结构化数据处理spark streaming，流式计算spark MLlib，机器学习库GraphX,图计算 spark运行模式： local模式 standalone模式，构建master+slave集群 Spark on Yarn模式 Spark on Mesos模式 宽窄依赖 1.窄依赖是1对1或1对多，宽依赖是多对1 2.窄依赖前一步map没有完全完成也可以进行下一步，在一个线程里完成不划分stage;宽依赖下一步需要依赖前一步的结果，划分stage 4.在传输上，窄依赖之间在一个stage内只需要做pipline，每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换；宽依赖在stage做shuffle，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输 3.容错上，窄依赖只需要重新计算子分区对应的负分区的RDD即可；宽依赖，在极端情况下所有负分区的RDD都要被计算 map­reduce中数据倾斜的原因?应该如何处理?如何处理spark中的数据倾斜? 原因：在物理执行期间，RDD会被分为一系列的分区，每个分区都是整个数据集的子集。当spark调度并运行任务的时候，Spark会为每一个分区中的数据创建一个任务。大部分的任务处理的数据量差不多，但是有少部分的任务处理的数据量很大，因而Spark作业会看起来运行的十分的慢，从而产生数据倾斜 处理方式： 1.使用需要进行shuffle人工指定参数并行度 2.进行数据的清洗,把发生倾斜的刨除,用单独的程序去算倾斜的key 3.join的时候使用小数据join大数据时，换用map join 尽量减少shuffle的次数 Spark分区数设置 1、分区数越多越好吗？ 不是的，分区数太多意味着任务数太多（一个partion对应一个任务），每次调度任务也是很耗时的，所以分区数太多会导致总体耗时增多。 2、分区数太少会有什么影响？ 分区数太少的话，会导致一些结点没有分配到任务；另一方面，分区数少则每个分区要处理的数据量就会增大，从而对每个结点的内存要求就会提高；还有分区数不合理，会导致数据倾斜问题。 3、合理的分区数是多少？如何设置？ 总核数=executor-cores * num-executor 一般合理的分区数设置为总核数的2~3倍 Worker、Master、Executor、Driver 4大组件 1.master和worker节点 搭建spark集群的时候我们就已经设置好了master节点和worker节点，一个集群有一个master节点和多个worker节点。 master节点常驻master守护进程，负责管理worker节点，我们从master节点提交应用。 worker节点常驻worker守护进程，与master节点通信，并且管理executor进程。 2.driver和executor进程 driver进程就是应用的main()函数并且构建sparkContext对象，当我们提交了应用之后，便会启动一个对应的driver进程，driver本身会根据我们设置的参数占有一定的资源（主要指cpu core和memory）。下面说一说driver和executor会做哪些事。 driver可以运行在master上，也可以运行worker上（根据部署模式的不同）。driver首先会向集群管理者（standalone、yarn，mesos）申请spark应用所需的资源，也就是executor，然后集群管理者会根据spark应用所设置的参数在各个worker上分配一定数量的executor，每个executor都占用一定数量的cpu和memory。在申请到应用所需的资源以后，driver就开始调度和执行我们编写的应用代码了。driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。 executor进程宿主在worker节点上，一个worker可以有多个executor。每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回给driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的 RDD 提供内存式存储，RDD 是直接缓存在executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 driver进程会将我们编写的spark应用代码拆分成多个stage，每个stage执行一部分代码片段，并为每个stage创建一批tasks，然后将这些tasks分配到各个executor中执行。 Spark是如何进行资源管理的？ 1）资源的管理和分配 资源的管理和分配，由Master和Worker来完成。 Master给Worker分配资源， Master时刻知道Worker的资源状况。 客户端向服务器提交作业，实际是提交给Master。 2）资源的使用 资源的使用，由Driver和Executor。程序运行时候，向Master请求资源。 Spark和mapreduce点的区别 优点： 1.最大的区别在于.spark把用到的中间数据放入内存，而mapreduce需要通过HDFS从磁盘中取数据。 2.spark算子多，mapreduce只有map和reduce两种操作 缺点： ​ spark过度依赖内存计算，如果参数设置不当，内存不够时就会因频繁GC导致线程等待 什么是RDD RDD是一个只读的分布式弹性数据集，是spark的基本抽象 主要特性： ​ 1.分布式。由多个partition组成，可能分布于多台机器，可并行计算 ​ 2.高效的容错（弹性）。通过RDD之间的依赖关系重新计算丢失的分区 ​ 3.只读。不可变 RDD在spark中的运行流程？ 创建RDD对象 sparkContext负责计算RDD之间的依赖关系，构建DAG DAGScheduler负责把DAG分解成多个stage(shuffle stage和final stage)，每个stage中包含多个task，每个task会被TAskScheduler分发给WORKER上的Executor执行 ###spark任务执行流程： Driver端提交任务，向Master申请资源 Master与Worker进行RPC通信，让Work启动Executor Executor启动反向注册Driver，通过Driver–Master–Worker–Executor得到Driver在哪里 Driver产生Task，提交给Executor中启动Task去真正的做计算 spark是如何容错的？ 主要采用Lineage(血统)机制来进行容错，但在某些情况下也需要使用RDD的checkpoint 对于窄依赖，只计算父RDD相关数据即可，窄依赖开销较小 对于宽依赖，需计算所有依赖的父RDD相关数据，会产生冗余计算，宽依赖开销较大。 在两种情况下，RDD需要加checkpoint 1.DAG中的Lineage过长，如果重算，开销太大 2.在宽依赖上Cheakpoint的收益更大 一个RDD的task数量是又什么决定？一个job能并行多少个任务是由什么决定的？ task由分区决定，读取时候其实调用的是hadoop的split函数，根据HDFS的block来决定每个job的并行度由core决定 cache与checkpoint的区别 cache 和 checkpoint 之间有一个重大的区别，cache 将 RDD 以及 RDD 的血统(记录了这个RDD如何产生)缓存到内存中，当缓存的 RDD 失效的时候(如内存损坏)，它们可以通过血统重新计算来进行恢复。但是 checkpoint 将 RDD 缓存到了 HDFS 中，同时忽略了它的血统(也就是RDD之前的那些依赖)。为什么要丢掉依赖？因为可以利用 HDFS 多副本特性保证容错！ reduceByKey和groupByKey的区别? 如果能用reduceByKey,那就用reduceByKey.因为它会在map端,先进行本地combine,可以大大减少要传输到reduce端的数据量,减小网络传输的开销。 groupByKey的性能,相对来说要差很多,因为它不会在本地进行聚合,而是原封不动,把ShuffleMapTask的输出,拉取到ResultTask的内存中,所以这样的话,就会导致,所有的数据,都要进行网络传输从而导致网络传输性能开销非常大! map和mapPartition的区别？ 1.map是对rdd中的每一个元素进行操作；mapPartitions则是对rdd中的每个分区的迭代器进行操作如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。 2.如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。 3.SparkSql或DataFrame默认会对程序进行mapPartition的优化。 mapPartition缺点： 一次性读入整个分区全部内容，分区数据太大会导致内存OOM 详细说明一下GC对spark性能的影响?优化 GC会导致spark的性能降低。因为spark中的task运行时是工作线程,GC是守护线程,守护线程运行时,会让工作线程停止,所以GC运行的时候,会让Task停下来,这样会影响spark 程序的运行速度,降低性能。 默认情况下,Executor的内存空间分60%给RDD用来缓存,只分配40%给Task运行期间动态创建对象,这个内存有点小,很可能会发生full gc,因为内存小就会导致创建的对象很快把内存填满,然后就会GC了,就是JVM尝试找到不再被使用的对象进行回收,清除出内存空间。所以如果Task分配的内存空间小,就会频繁的发生GC,从而导致频繁的Task工作线程的停止,从而降低Spark应用程序的性能。 优化方式： ​ 1.增加executor内存 ​ 2.可以用通过调整executor比例,比如将RDD缓存空间占比调整为40%,分配给Task的空间变为了60%,这样的话可以降低GC发生的频率 spark.storage.memoryFraction ​ 2.使用Kryo序列化类库进行序列化 为什么要使用广播变量？ 当RDD的操作要使用driver中定义的变量时,每次都要把变量发送给worker节点一次,如果这个变量的数据很大的话,会产生很高的负载,导致执行效率低;使用广播变量可以高效的使一个很大的只读数据发送给多个worker节点,而且对每个worker节点只需要传输一次,每次操作时executor可以直接获取本地保存的数据副本,不需要多次传输]]></content>
      <tags>
        <tag>面试</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——SMOTE算法]]></title>
    <url>%2F2019%2F05%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94SMOTE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考文章]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——背包问题（动态规划）]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%EF%BC%88%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 背包问题是指给定义一些候选集，在这些候选集中找出一些特定要求的组合，组合出目标值(也就是用一些大小不同的东西装满背包)。 ​ 常见的背包问题问题有：能否组成一个值，组成一个值得最小元素个数，组成一个值可使用的最多元素个数等 ​ 背包问题根据背包选用的物品是否可以复用，分为完全背包问题和0/1背包问题。根据背包的维度可以分为一维背包问题和二维背包问题。下面我们针对这些问题中的关键点进行总结： 0/1背包问题： ​ 1.遍历时一定要从后向前遍历目标值数组（dp），不能从前向后，从前往后会产生一个物品适用多次的问题 ​ 2.要在最外层循环遍历物品，这样能保证在选择将物品是否使用在哪里使用 ​ 3.dp数组长度为目标值得大小+1 完全背包问题： ​ 1.dp数组长度为目标值得大小+1 ​ 一维背包问题1.零钱兑换(leetcode 232)给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 示例 1: 123输入: coins = [1, 2, 5], amount = 11输出: 3 解释: 11 = 5 + 5 + 1 示例 2: 12输入: coins = [2], amount = 3输出: -1 说明:你可以认为每种硬币的数量是无限的。 分析:这道题题目是标准的完全背包问题，用数目不定的元素组合成指定金额，因为各个值都可能有两种情况组成：1.直接由当前金额的硬币直接组成 2.由之前组成的金额再加上一个硬币组成，因此递推关系为: ​ dp[i] = min(dp[i],dp[i-c]) c为硬币的各个金额 123456789101112131415def coinChange(self, coins: List[int], amount: int) -&gt; int: MAX_INT = pow(2,32)-1 dp = [MAX_INT for _ in range(amount+1)] dp[0] = 0 for i in range(1,len(dp)): for c in coins: if i-c&gt;=0: dp[i] = min(dp[i],dp[i-c]+1) if dp[-1]==MAX_INT: return -1 else: return dp[-1] 2.小米大礼包小米之家是成人糖果店。里面有很多便宜，好用，好玩的产品。中秋节快到了，小米之家想给米粉们准备一些固定金额大礼包。对于给定的一个金额，需要判断能不能用不同种产品（一种产品在礼包最多出现一次）组合出来这个金额。聪明的你来帮帮米家的小伙伴吧。 输入描述:123输入 N （N 是正整数， N &lt;= 200）输入 N 个价格p（正整数, p &lt;= 10000）用单空格分割输入金额 M（M是正整数，M &lt;= 100000 ） 输出描述:12能组合出来输出 1否则输出 0 示例1 输入： 123699 199 1999 10000 39 149910238 输出： 11 分析：这是一个标准的一维0/1背包问题，最终目标看是否能完成组合。因此首先我们推断出递推公式 ​ dp[i] = max(dp[i],dp[i-c]) 注意：0/1背包问题必须要从后往前进行遍历，否则会出现已经当前c在前面使用dp[i] = max(dp[i],dp[i-c])已经更新过的结果，相当于使用了多次c 12345678910111213141516n = int(input())nums = list(map(int,input().split()))target = int(input())dp = [0 for i in range(target+1)]dp[0] = 1for c in nums: for i in range(target,c-1,-1): dp[i] = max(dp[i],dp[i-c])print(dp[-1]) 二维背包问题(leetcode 474)1.小米大礼包在计算机界中，我们总是追求用有限的资源获取最大的收益。 现在，假设你分别支配着 m 个 0 和 n 个 1。另外，还有一个仅包含 0 和 1 字符串的数组。 你的任务是使用给定的 m 个 0 和 n 个 1 ，找到能拼出存在于数组中的字符串的最大数量。每个 0 和 1 至多被使用一次。 注意: 给定 0 和 1 的数量都不会超过 100。 给定字符串数组的长度不会超过 600。 示例 1: 1234输入: Array = &#123;&quot;10&quot;, &quot;0001&quot;, &quot;111001&quot;, &quot;1&quot;, &quot;0&quot;&#125;, m = 5, n = 3输出: 4解释: 总共 4 个字符串可以通过 5 个 0 和 3 个 1 拼出，即 &quot;10&quot;,&quot;0001&quot;,&quot;1&quot;,&quot;0&quot; 。 示例 2: 1234输入: Array = &#123;&quot;10&quot;, &quot;0&quot;, &quot;1&quot;&#125;, m = 1, n = 1输出: 2解释: 你可以拼出 &quot;10&quot;，但之后就没有剩余数字了。更好的选择是拼出 &quot;0&quot; 和 &quot;1&quot; 。 分析：这一题是比较典型的0/1背包问题,将翻译后的string看做一个物品，这个物品有两个value，value1为0 的个数，value2为1的个数，初始状态下你有用m个0和n个1，求最多能获取的物品总个数。 ​ 核心依然是找到状态转移方程，因为题目具有两个变量，属于二维背包问题，因此创建一个二位数组，分别代表使用num0个0和num1个1时可以获得的最多字符串数。 ​ dp[num0][num1] =max(dp[num0][num1],dp[nums0-zeros][nums1-ones]+1) 注意：这里的二重循环必须从m,n开始递减，而不能从0开始递增，因为在0/1背包问题中，每个物品只能被使用一次，如果从0开始向后，dp[num0][num1]可以获得的是这次循环中更新过的dp[num0][num1] =max(dp[num0][num1],dp[nums0-zeros][nums1-ones]+1)，相当于一个物品可以重复购买，变成了完全背包问题。 12345678910111213141516171819202122232425def findMaxForm(self, strs: List[str], m: int, n: int) -&gt; int: tmp = [] for i in strs: zeros = 0 ones = 0 for j in i: if j=='0': zeros+=1 else: ones+=1 tmp.append([zeros,ones]) #m 行，代表0 n列代表1 dp = [[0 for _ in range(n+1)]for _ in range(m+1)] for s in tmp: zeros = s[0] ones = s[1] for nums1 in range(m,zeros-1,-1): for nums2 in range(n,ones-1,-1): dp[nums1][nums2] = max(dp[nums1][nums2],dp[nums1-zeros][nums2-ones]+1) return dp[-1][-1] 2.大礼包在LeetCode商店中， 有许多在售的物品。 然而，也有一些大礼包，每个大礼包以优惠的价格捆绑销售一组物品。 现给定每个物品的价格，每个大礼包包含物品的清单，以及待购物品清单。请输出确切完成待购清单的最低花费。 每个大礼包的由一个数组中的一组数据描述，最后一个数字代表大礼包的价格，其他数字分别表示内含的其他种类物品的数量。 任意大礼包可无限次购买。 示例 1: 1234567输入: [2,5], [[3,0,5],[1,2,10]], [3,2]输出: 14解释: 有A和B两种物品，价格分别为¥2和¥5。大礼包1，你可以以¥5的价格购买3A和0B。大礼包2， 你可以以¥10的价格购买1A和2B。你需要购买3个A和2个B， 所以你付了¥10购买了1A和2B（大礼包2），以及¥4购买2A。 最小车票花费]]></content>
      <tags>
        <tag>算法</tag>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——贪心算法]]></title>
    <url>%2F2019%2F05%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[挑选代表我们有很多区域，每个区域都是从a到b的闭区间，现在我们要从每个区间中挑选至少2个数，那么最少挑选多少个？ 输入描述:12第一行是N（N&lt;10000）,表示有N个区间，之间可以重复然后每一行是ai,bi，持续N行，表示现在区间。均小于100000 输出描述:1输出一个数，代表最少选取数量。 输入例子1:1234544 72 40 23 6 输出例子1:14 思路分析：​ 本题是一个贪心问题，即挑选最少的点，也就是在每一步种都选择可能和下一步公用的点。可以先把区间按照结尾去接进行排序，然后从第一个区间开始记录最后两个元素的值， ​ 如果下个区间中包含了这两个元素，那么挑选点数+0，x、y不变， ​ 如果下个区间中只包含了一个元素，那么挑选点数+1,y继承x的值，x变为当前区间的最后一个元素 ​ 如果下个区间中不包含任何x、y一个元素，那么挑选点数+2，x、y更新为区间最大、次大值 1这里之所以按照末尾元素进行排序，主要是因为后续要判断结尾两个元素和下一个区间是否具有关系 1234567891011121314151617181920212223242526n = int(input())nums = []for _ in range(n): tmp = list(map(int,input().split())) nums.append(tmp)nums = sorted(nums,key=lambda x:x[1])ans = 2x= nums[0][-1] #最大的元素y = nums[0][-1]-1 #次大的元素for l in nums[1:]: if l[0]&lt;=x&lt;=l[-1] and l[0]&lt;=y&lt;=l[-1]: ans += 0 elif l[0]&lt;=x&lt;=l[-1] or l[0]&lt;=y&lt;=l[-1]: y = x x = l[-1] ans+=1 else: ans+=2 x = l[-1] y = l[-1] - 1print(ans)]]></content>
  </entry>
  <entry>
    <title><![CDATA[美团技术笔试——最长全1串]]></title>
    <url>%2F2019%2F05%2F14%2F%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AF%95%E2%80%94%E2%80%94%E6%9C%80%E9%95%BF%E5%85%A81%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[题目描述给你一个01字符串，定义答案=该串中最长的连续1的长度，现在你有至多K次机会，每次机会可以将串中的某个0改成1，现在问最大的可能答案 输入描述:123456输入第一行两个整数N,K，表示字符串长度和机会次数第二行输入N个整数，表示该字符串的元素( 1 &lt;= N &lt;= 300000, 0 &lt;= K &lt;= N ) 输出描述:1输出一行表示答案 输入例子1:1210 2 1 0 0 1 0 1 0 1 0 1 输出例子1:15 解题思路​ 首先应该分几种情况进行分类讨论： 1.当K&gt;N时，输出应该直接为K 2.当K&lt;N，如果K等于0，结果直接为最长连续1子串长度 ​ 如果K不等于0，那么需要进行动态滑动窗口实验 滑动窗口实验思路为： ​ 1.首先计算不进行替换时，最长连续1子串长度，即为max ​ 2.设置初始值 ​ 滑动窗口大小初始值 slide = max+K ​ 滑动窗口最大和初始值 slide_sum = max ​ 3.使用当前滑动窗口大小进行扫描数据，看是都存在一个滑动窗口内的和超过当前滑动窗口最大和 ​ 如果有，那么说明存在更大的连续子串，因此将silde和silde_sum都加1(在初始值时已经设置了相当于K个空位，如果值大于silde_sum,说明空格还没用完，如果等于说明空格用完了，但是还可能存在更大的连续1，因此只有当值小于silde_sum才能保证是最大的连续1串） ​ 如果没有，那么说明silde-1为最大窗口，也就是最长全1串 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 10 2# 1 0 0 1 0 1 0 1 0 1n,k = map(int,input().split())nums = list(map(int,input().split()))#判断新的滑动窗口是不是存在和大于等于原来的最大和def get_maxsum(silde,nums,max_sum): end = silde start = 0 while end&lt;len(nums): new_max_sum = sum(nums[start:end]) if new_max_sum&gt;=max_sum: return True start += 1 end+=1 return Falseif k&gt;n: print(n)else: max_len = 0 i =0 tmp_len = 0 while i&lt;n: if nums[i]==1: tmp_len +=1 else: tmp_len = 0 max_len = max(max_len,tmp_len) i+=1 if max_len+k&gt;=n: print(n) else: flag = True silde = max_len + k max_sum = max_len while flag: if get_maxsum(silde,nums,max_sum): silde += 1 max_sum+=1 else: flag = False print(silde-1)]]></content>
  </entry>
  <entry>
    <title><![CDATA[降维技术]]></title>
    <url>%2F2019%2F05%2F13%2F%E9%99%8D%E7%BB%B4%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[​ 常见的降维技术只要分为PCA、LDA和t-sne三种，下面我们将具体介绍这三种降维技术以及适用范围 1.PCA2.LDA3.t-sne​ t-sne是在NLP词切入可视化降维的最佳选择，因为它具有保存向量之间相对距离的特性，能有效地保存线性子结构和关系，可以很好的表达不同的单词的在当前训练的词向量上是否相似。 ​ 核心特性: 会保存向量之间的相对距离]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcloud词云工具]]></title>
    <url>%2F2019%2F05%2F10%2Fwordcloud%E8%AF%8D%E4%BA%91%E5%9B%BE%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[wordcloud是一种NLP中常用的可视化工具，主要用途是可视化展示文本中各个词出现的频率多少，将出现频率多的使用更大的字体进行展示。 基本用法1234567import wordcloudwith open("./type1.txt","r") as f: type1 = f.read() w = wordcloud.WordCloud()w.generate(type1)w.to_file("type1.png") wordcloud内部处理流程： ​ 1 、分隔：以空格分隔单词 ​ 2、统计 ：单词出现的次数并过滤 ​ 3、字体：根据统计搭配相应的字号 ​ 4 、布局 常用参数​]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP建模常见处理流程]]></title>
    <url>%2F2019%2F05%2F09%2FNLP%E5%BB%BA%E6%A8%A1%E5%B8%B8%E8%A7%81%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.清洗​ 主要包括清除掉无关内容和区分出各个部分。 段落的首尾单独区分：这里比较常见的一种却分时将段落的首尾单独区分出来，因为首尾句一般都是更加具有代表性的句子 2.标准化​ 主要包含了字母小写化和标点符号替换两个步骤 123456#字母小写化str.lower()#标点符号替换为空格import retext = re.sub(r"a-zA-Z0-9"," ") 3.标记化(分词)​ 标记化是指将目标切分成无法再分符号，一般主要指分词，一般的处理中都会将句子按照” “进行分词。 12345678#自写原始分词，进行前面的标准化以后进行words = text.split()#使用nltk进行分词,分词会比上面的更加准确，根据标点符号的不同位置进行不同种处理，例如Dr. 中的.不会被处理掉from nltk.tokenize import word_tokenizesentence = word_tokenize(text)words = word_tokenize(sentence)#nltk提供了多种token方式，包括正则表达式等，按需选择 4.删除停用词​ 删除停用词是指删除掉哪些去掉哪些和当前任务判断关系不大的词，对于设计到的语料没有具体领域时，可以使用英文常用停用词，其中包括800多个英文的常见停用词。 ​ 英文常见停用词标准表 在特定领域时，最好使用专门针对于该领域的停用词表，因为在一个问题中的停用词可能会在另一个问题中肯能就是关键词 1234567891011121314#去除停用词def get_stopword(path): """ 获取停用词表 return list """ with open(path) as f: stopword = f.read() stopword_list = stopword.splitlines() return stopword_liststopwords = get_stopword(path)words = [word for word in words if word not in stopwords] 5.词性标注​ 用于标注句子中各个单词分别属于什么词性，更加有助于理解句子的含义，另一方面，词性标注更加有利于后续处理。 常见的一种利用词性标注的后续处理步骤就是直接去掉非名词的部分，因为在一个句子中，名词在很大程度就可以表现两个句子的相似度。 1234#使用nltk进行词性标注from nltk import pos_tagsentence = word_tokenize("this is a dog") #分词pos = pos_tag(sentence) #标注 6.命名实体识别​ 命名实体识别指的是识别 ​ 条件：命名实体识别首先要完成词性标注 ​ 应用：对新闻文章进行简历索引和搜索 实践性能并不是一直都很好，但对大型语料库进行实验确实有效 1234from nltk import pos_tag,ne_chunkfrom nltk.tokenize import word_tokenizene_chunk(pos_tag(word_tokenize("I live in Beijing University"))) 7.词干化和词型还原​ 词干提取是指将词还原成词干或词根的过程 ​ 方式：利用简单的搜索和替换样式规则，例如去除结尾的s、ing，将结尾的ies变为y等规则 ​ 作用：有助于降低复杂度，同时保留次所含的意义本质 还原的词干不一定非常准确，但是只要这个词的所有形式全部都转化成同一个词干就可以了，因为他们都有共同的含义 12from nltk.stem.porter import PorterStemmerstemmed = [PoeterStemmer().stem(w) for w in words] ​ 词型还原是将词还原成标准化形式的另一种技术，利用字典的方式将一个词的不同形式映射到其词根 ​ 方式：字典 ​ 优点:可以将较大的词型变化很大的正确还原到词根 123from nltk.stem.wordnet import WordNetLemmaterlemmed = [WordNetLemmater.lemmative(w) for w in words] ​ 这里我们发现只有ones被还原成了one，其他词并没有找到词的原型，这是因为词型转化是针对词型进行的，只会转化指定词型的词，默认只转换名词，因此上面只有ones被转换了，下面我们来指定转换动词： 123from nltk.stem.wordnet import WordNetLemmaterlemmed = [WordNetLemmater.lemmative(w) for w in words] 8.向量化​ 向量化是将提取好的token转化成向量表示，准备输入到模型中。常见的方式包括词袋模型、tf-idf、Word2vec、doc2vec等 9. 分类模型或聚类模型​ 根据实际情况选用合适的分类模型，聚类模型。 注意:上面的处理流程并不是全部都一定要进行,可以根据实际情况进行选择,例如在下一篇文章情感分类中,只是使用了标准化、去停用词、词干提取、向量化、分类等步骤]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2019%2F05%2F08%2FGBDT%2F</url>
    <content type="text"><![CDATA[简介​ GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT算的上TOP3的算法。想要理解GBDT的真正意义，那就必须理解GBDT中的Gradient Boosting 和Decision Tree分别是什么？ 分类树和回归树####1.分类树 ​ 分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。 ​ 以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 2.回归树​ 回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。 ​ 回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 Decision Tree：CART回归树​ GBDT使用的决策树都是CART数回归树，无论是处理回归问题还是二分类以及多分类。 为什么不用CART分类树呢？ ​ 因为GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树。 ​ CART回归树的评价指标：平方误差 为什么CART回归时的评价指标不再使用Gini、熵等不纯度指标？ ​ 对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。 Graident Boosting:梯度提升树​ 梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树 提升树 Boosting Tree​ 提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。 ​ 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。 ​ 当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。 Graident Boosting:梯度提升树​ 核心：利用损失函数的负梯度作为提升树算法中的残差的近似值。 下面我们来看一下负梯度具体的样子，第t轮的第i个样本的损失函数的负梯度为： 那么对于分类问题呢？二分类和多分类的损失函数都是logloss，下面以回归问题为例对GBDT算法进行讲解。 GBDT 常见问题： 1.GBDT和Xgboost的区别？ 1.损失函数上 在GBDT的损失函数上XGboost加入了正则化项 2.优化方法上 GBDT在优化上只使用一阶导数的信息，而XGBoost则对代价函数进行了二阶的展开。 3.基分类器的支持上 GBDT只支持CART数作为基分类器，XGBoost在其基础上加入了线性分类器 4.Xgboost加入了shrinkage策略。在完成一次迭代后会将叶子节点的权值乘以该系数削弱了每棵树的影响，使后面的数拥有更大的学习空间 5.列抽样 借鉴了随机森林的做法，支持列抽样，不仅能防止过拟合还能减少计算 6.缺失值自动处理 对于有缺失值的样本，XGBoost可以自动学习出分裂方向 7.计算特征增益时并行 预先对特征值进行排序，保存成block结构，后面的迭代重复使用这个结构 2.lightgbm和Xgboost的区别在哪里？ ​ lightgbm基本原理和Xgboost一样，在框架上做了一些优化 1.xgboost采用的level-wise的分裂策略，而lightgbm采用的是leaf-wise的策略，区别是xgboost对每一层节点做无差别的分裂，可能有些节点的信息增益非常小，对结果影响不大，但是依然进行分裂；leaf-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂。明显leaf-wise更容易过拟合，陷入高度较高的深度中，因此lightgbm更应该注意对深度进行限制 2.lightgbm使用histgram的决策树算法，而xgboost使用exact算法，hostgram算法在内存和计算代价上都有不小的优势 3.lightgbm采用直方图加速计算 4.并行化。 ​ a.特征并行化 ​ 一般的特征并行化都并行化都采用将数据进行垂直切分，然后分割后的数据分散到各个worker，各个worker计算器拥有的数据上计算 best split point，然后汇总得到最优切点。这种方式在数据量很大的时候效率提升有限 ​ lightgbm采用直接将全量数据分散到每个worker，然因此最优的特征分裂结果不需要传输到其他worker中，只需要将最优特征以及分裂点告诉其他worker，worker随后本地自己进行处理。 ​ b.数据并行化 ​ 传统的数据并行算法，首先水平切分数据集，每个worker基于数据集构建局部特征直方图（Histogram），归并所有局部的特征直方图，得到全局直方图，找到最优分裂信息，进行数据分裂。 ​ LightGBM算法使用Reduce Scatter并行算子归并来自不同worker的不同特征子集的直方图，然后在局部归并的直方图中找到最优局部分裂信息，最终同步找到最优的分裂信息。 ​ 除此之外，LightGBM使用直方图减法加快训练速度。我们只需要对其中一个子节点进行数据传输，另一个子节点可以通过histogram subtraction得到。 参考文献：https://blog.csdn.net/zpalyq110/article/details/79527653]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[度小满编程记——火车站台问题]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%BA%A6%E5%B0%8F%E6%BB%A1%E7%BC%96%E7%A8%8B%E8%AE%B0%E2%80%94%E2%80%94%E7%81%AB%E8%BD%A6%E7%AB%99%E5%8F%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[​ ​ 思路：这道题讲道理如果起点终点没有那么大就很简单了，直接使用字典进行存储，然后选择value最大的那个值即可。而这道题目中明显直接使用上面的思路是行不通了，因此这题使用了一种比较巧的方式， ​ 先将各个列车的起点终点分别编码为(站点,编号)，起点编号为-1，终点编号为0，然后从小到大对元组进行排序，然后便利元组列表，如果是终点，那么将维护数+1，如果是起点，那么代表到这里已经有一辆车不再需要维护了，同时记录最大的维护值，便利完全部列表最大的维护值即为结果 1234567891011121314151617181920n = int(input())train = []t = 0ans = 0for i in range(n): l = list(map(int,input().split())) train.append((l[0],1)) train.append((l[1],-1))train.sort() #这里是关键点，sort函数将对元组进行排序for i in train: if i[1]==0: t+=1 else: t-=1 ans = max(ans,t)pritn(ans) 上式中的sort函数对元素为元祖的列表来进行排序，默认规则是先使用元组的第一个元素进行排序，当第一个元素值相同时再使用第二个元素进行排序，下面是一个例子： 12345l = [[1,1],[2,1],[2,-1],[1,-1]]l.sort()output: [[1, -1], [1, 1], [2, -1], [2, 1]] 在这个问题中使用sort进行排序后，表示由于同一个车站的负值被排在前面，每次先减去1，也就是前一个车一这里为起点，不需要再使用维护。 之前还遇到过好多类似的问题，其实都可以采用这种类似的思路来减少内存占用，比如之前360笔试中遇到过的找]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__new__和__init__]]></title>
    <url>%2F2019%2F04%2F26%2Fnew-%E5%92%8C-init%2F</url>
    <content type="text"><![CDATA[执行顺序：类中同时出现new()和init()时，先调用new()，再调用init() python中__new__和__init__的区别 1.用法不同： ​ __new__()用于创建实例，所以该方法是在实例创建之前被调用，它是类级别的方法，是个静态方法； ​ __init__() 用于初始化实例，所以该方法是在实例对象创建后被调用，它是实例级别的方法，用于设置对象属性的一些初始值 ​ 注：由此可知，__new__()在__init__() 之前被调用。如果__new__() 创建的是当前类的实例，会自动调用__init__()函数，通过return调用的__new__()的参数cls来保证是当前类实例，如果是其他类的类名，那么创建返回的是其他类实例，就不会调用当前类的__init__()函数 2.传入参数不同： ​ __new__()至少有一个参数cls，代表当前类，此参数在实例化时由Python解释器自动识别； ​ __init__()至少有一个参数self，就是这个__new__()返回的实例，__init__()在__new__()的基础上完成一些初始化的操作。 3.返回值不同： ​ __new__()必须有返回值，返回实例对象； __init__()不需要返回值。 __new__的两种常见用法1.继承不可变的类​ __new__()方法主要用于继承一些不可变的class，比如int, str, tuple， 提供一个自定义这些类的实例化过程的途径，一般通过重载__new__()方法来实现 12345678class PostiveInterger(int): def __new__(cls,value): return super(PostiveInterger,cls).__new__(cls,abs(value))a = PostiveInterger(-10)print(a)output: 10 2.实现单例模式​ 可以用来实现单例模式，也就是使每次实例化时只返回同一个实例对象。 1234567891011121314151617class Singleobject(object): def __new__(cls): if not cls.instance: cls.instance = super(Singleobject,cls).new(cls) return cls.instanceobject1 = Singleobject()object2 = Singleobject()object1.attr = 'value1'print(object1.attr,object2.attr)print(object1 is object2)output: value1,value1 True]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学算法——排列组合]]></title>
    <url>%2F2019%2F04%2F20%2F%E6%95%B0%E5%AD%A6%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%2F</url>
    <content type="text"><![CDATA[​ 最近在做各大场笔试题的过程中，发现排列组合在笔试中是一个经常出现的内容，由于读研后就在没有接触过，忘得已经差不多了，大的都不是很好，因此决定写一篇博客来重新复习一下相关知识，下面开始进行总结。 排列组合1.排列​ n个元素中取m个元素按照一定的书序排成一排，用$A_n^m$表示。 计算公式： ​ $A_n^m = n(n-1)(n-2)…(n-m+1)​$ 2.组合​ n个元素中取m个不同的元素(不关心顺序) 计算公式： ​ $C_n^m = A_n^m/A_m^m = \frac{n(n-1)(n-2)…(n-m+1)}{m(m-1)(m-2)…1}$ 常用技巧1.捆绑法​ 要求几个元素相邻时，可以将它们作为一个整体在进行排列 2.差空法​ 要求元素不相邻时，如ABCDEF排成一排，要求AB不相邻，则可以把CDEF先排好，把AB插进CDEF产生的5个空中就好 3.插板法​ 要求n个元素分成m个组，每个组必须要有一个元素时，可以在n个元素中产生的n-1个空中插m-1个板子 4.留一法​ 排列问题中，有元素的顺序已定，如alibaba全排列产生多少个字符串，7个元素中a重复3次，b重复两次，则结果为元素全排除以重复元素的全排 常见问题环形排列问题：]]></content>
      <tags>
        <tag>数学算法</tag>
        <tag>排列组合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——堆相关的问题]]></title>
    <url>%2F2019%2F04%2F17%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%A0%86%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[堆相关知识​ 堆是一种特殊的完全二叉树，其父节点的值都比子节点的大(大根堆)， 注意：堆的孩子节点左右无大小关系 相关知识：完全二叉树​ 性质：1.完全二叉树的深度为logn ​ 2.最后一个非叶子节点为n//2 ​ 3.一个编号为x的节点父节点的编号为x//2 ​ 4.一个编号为x的左孩子节点为2*x ​ 完全二叉树一般都存储在数组中，但是由于二叉树节点的序号是从1开始的，数组索引是从0开始的，所以需要将恰其全部向后移动一位，将索引为0的位空出来，从1开始计数，但是在python中数组因为没有appendleft方法，因此一般采用colllections中的deque链表类来进行存储(因为其有appendleft方法，直接在首位添加空位) 123from collections import dequeL = deque([50, 16, 30, 10, 60, 90, 2, 80, 70])L.appendleft(0) ​ 堆操作​ 性质：1.插入新元素的时间复杂度为logn，比较次数就是完全二叉树的深度 插入元素​ 直接将新元素插入到末尾，再根据情况判断新元素是否需要上移，直到满足堆的特性为止。如果堆的大小为N（即有N个元素），那么插入一个新元素所需要的时间也是O(logN)。 ​ 下面以在小根堆中插入新节点为例： 12345678910111213heap.append(i)def insert_heapq(i): flag = 0 #标志是否还需要进行向上调整 if i==1: return while i!=1 and flag==0: if heap[i]&lt;heap[i//2]: heap[i],heap[i//2] = heap[i//2],heap[i] else: flag = 1 i = i//2 建立堆​ 建立堆最自然的思路就是从空的堆开始不断向堆中添加元素，直到所有数据都被插入堆中，此时由于插入每个元素的时间复杂度为O(logi)，所以插入全部数据的时间复杂度为O(nlogn) ​ 而真正的堆建立往往采取另外一种更加高效的时间复杂度为O(n)的方法来进行，即直接先将全部数放入完全二叉树,然后在这个棵完全二叉树中，我们从最后一个结点开始依次判断以这个结点为根的子树是否符合最小堆的特性。如果所有的子树都符合最小堆的特性，那么整棵树就是最小堆了。 ​ 具体做法如下： ​ 首先我们从叶结点开始。因为叶结点没有儿子，所以所有以叶结点为根结点的子树（其实这个子树只有一个结点）都符合最小堆的特性（即父结点的值比子结点的值小）。这些叶结点压根就没有子节点，当然符合这个特性。因此所有叶结点都不需要处理，直接跳过。从第n/2个结点开始（n为完全二叉树的结点总数，这里即7号结点）处理这棵完全二叉树。（这里用到了完全二叉树的性质：最后一个非叶结点是第n/2个结点)。 12345678910111213141516171819202122232425#调整编号为n的节点符合堆结构(这里是最小堆)def head_adjust(i,end): tmp = L[i] j = i*2 #j是i的左子节点索引 while j&lt;=end: if j&lt;end and heap[j]&gt;heap[j+1]: j = j+1 #这里是比较两个孩子，将比较小的索引付给j if heap[j]&lt;heap[i]: #比较该节点和孩子中比较小的，如该节点比孩子中比较小的大，那么交换两个节点 heap[i],heap[j] = heap[j],heap[i] i = j j *= i else: #如果比孩子中较小的还小，说明一符合堆特性，不必继续向下遍历 break #由于是自下向上的，如果该节点移到的位置已经比两个子节点都小，那么他们也一定比孩子的孩子小#从一个列表创建一个堆def create_heap(L): from collections import deque heap =deque(L) heap.appendleft(0) length = len(heap)-1 last_no_leaf_index = length//2 for i in range(last_no_leaf_index): heap_adjust(last_no_leaf_index-i,length) 堆排序​ 平均时间复杂度：O(nlogn) ​ 最坏时间复杂度：O(nlogn) 时间复杂度主要是由于建立好堆后输出排序时，每输出一个结果要将一个数据从头向下比较，时间为O(logn)，有n次比较，因此总的时间复杂度为O(nlogn) ​ 堆排序的核心思想如下： 首先将待排序的数组构造出一个小根堆 取出这个小根堆的堆顶节点(最小值)，与堆的最下最右的元素进行交换，然后把剩下的元素再构造出一个小根堆 重复第二步，直到这个小根堆的长度为1，此时完成排序。 ​ 这里第一步就是小根堆的建立过程，上面已经有了，不在赘述，下面是第二、三不断交换完成啊排序的过程： 123456for i in range(length-1): heap[i],heap[length-i] = heap[length-i],heap[i] heap_adjust(i,length-i) #每次都会有一个元素相当于已经输出，从后向前依次 result = [L[i] for i in range(1,length+1)] return result ​ 因此整个堆排序过程为: 123456789101112131415161718192021222324252627282930313233#调整编号为n的节点符合堆结构(这里是最小堆)def head_adjust(i,end): tmp = L[i] j = i*2 #j是i的左子节点索引 while j&lt;=end: if j&lt;end and heap[j]&gt;heap[j+1]: j = j+1 #这里是比较两个孩子，将比较小的索引付给j if heap[j]&lt;heap[i]: #比较该节点和孩子中比较小的，如该节点比孩子中比较小的大，那么交换两个节点 heap[i],heap[j] = heap[j],heap[i] i = j j *= i else: #如果比孩子中较小的还小，说明一符合堆特性，不必继续向下遍历 break #由于是自下向上的，如果该节点移到的位置已经比两个子节点都小，那么他们也一定比孩子的孩子小#从一个列表创建一个堆def heap_sort(L): #创建堆 from collections import deque heap =deque(L) heap.appendleft(0) length = len(heap)-1 last_no_leaf_index = length//2 for i in range(last_no_leaf_index): heap_adjust(last_no_leaf_index-i,length) #输出堆的各个元素 for i in range(length-1): heap[i],heap[length-i] = heap[length-i],heap[i] heap_adjust(i,length-i) #每次都会有一个元素相当于已经输出，从后向前依次 result = [L[i] for i in range(1,length+1)] return result python中内置的堆​ python中只内置了小根堆，要使用大根堆的功能，可以将数转化成对应的负值进行堆操作，出堆时再取负值即为原来的最大值 python中的堆引用： 1import heapq 常用方法： 1.heapq.heapify(list) 将一个列表、元组穿换成小根堆对象，后续可以直接用堆操作 2.heapq.heappop(heap) 将堆顶元素出堆 堆常见题目1.前K个高频的单词给一非空的单词列表，返回前 k 个出现次数最多的单词。 返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。 示例 1： 1234输入: [&quot;i&quot;, &quot;love&quot;, &quot;leetcode&quot;, &quot;i&quot;, &quot;love&quot;, &quot;coding&quot;], k = 2输出: [&quot;i&quot;, &quot;love&quot;]解析: &quot;i&quot; 和 &quot;love&quot; 为出现次数最多的两个单词，均为2次。 注意，按字母顺序 &quot;i&quot; 在 &quot;love&quot; 之前。 示例 2： 1234输入: [&quot;the&quot;, &quot;day&quot;, &quot;is&quot;, &quot;sunny&quot;, &quot;the&quot;, &quot;the&quot;, &quot;the&quot;, &quot;sunny&quot;, &quot;is&quot;, &quot;is&quot;], k = 4输出: [&quot;the&quot;, &quot;is&quot;, &quot;sunny&quot;, &quot;day&quot;]解析: &quot;the&quot;, &quot;is&quot;, &quot;sunny&quot; 和 &quot;day&quot; 是出现次数最多的四个单词， 出现次数依次为 4, 3, 2 和 1 次。 分析：本题的主要难点在出现频率相同的但此处理上 解法一：利用Counter进行排序 关键点：使用Couner进行词频统计后如何进行排序，这里的排序只能使用频率的负值和首字母进行升序排序。为什么仔细进行思考，例:[“i”, “love”, “leetcode”, “i”, “love”, “coding”] 123456789def topKFrequent(self, words: List[str], k: int) -&gt; List[str]: from collections import Counter result = [] word_list = list(Counter(words).most_common()) word_list = sorted(word_list,key=lambda x:[-x[1],x[0]]) #这里的排序使用只能使用频率的负值进行排序和首字母进行升序排序 for i in range(k): result.append(word_list[i][0]) return result 解法二：使用headp进行堆排序 12345678def topKFrequent(self, words: List[str], k: int) -&gt; List[str]: import collections count = collections.Counter(nums) heap = [(-freq, word) for word, freq in count.items()] import heapq heapq.heapify(heap) return [heapq.heappop(heap)[1] for _ in range(k)]]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python——Counter对象]]></title>
    <url>%2F2019%2F04%2F14%2Fpython%E2%80%94%E2%80%94Counter%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[​ Counter对象就是python内部的一个计数器，常用来统计列表、字符串中各个字符串出现的频次，以及找到出现频次最该以及最低的元素 ​ 使用前必须先引入引用： 1from collections import Counter ​ 下面介绍在日常使用过程中常见的用法： 1.统计列表和字符串中各个元素出现的频数12345678s = "acfacs"l = [1,1,2,4,2,7]print(Counter(s))print(Counter(l))output: Counter(&#123;'c': 2, 'a': 2, 's': 1, 'f': 1&#125;) Counter(&#123;1: 2, 2: 2, 4: 1, 7: 1&#125;) 2.获取最高频的N个元素及频数​ Counter对象的most_common方法可以获取列表和字符串的前N高频的元素及频次。 most_common: ​ param n:前几个高频对象，从1开始，默认为全部，也就相当于按照频数排序 ​ return list:按照出现的频数高低已经排好序的前N个列表，列表的元素是两元组，第一项代表元素，第二项代表频率 12345s = "acfacs"print(Counter(s).most_common(1))output: [('c', 2)]]]></content>
      <tags>
        <tag>python基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——动态规划和回溯法]]></title>
    <url>%2F2019%2F04%2F13%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%92%8C%E5%9B%9E%E6%BA%AF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[动态规划DP​ 基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。 ​ 核心：找到递推公式 二维递归#####1.背包问题 2.分割等和子数组(也会背包问题)给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 12345输入: [1, 5, 11, 5]输出: true解释: 数组可以分割成 [1, 5, 5] 和 [11]. ​ 本题是一个经典的动态规划问题的题型——0/1背包问题,背包的大小为sum(nums)/2。该问题首先要我们初始化一个数组w，w[i]代表能否将背包填充到i，而能将背包填充到i有两种方式，一种是直接使用i大小的块，第二是使用多个小块，因此我们可以总结出递推公式： ​ w[i] = w[i]||w[i-num] ​ 这个递推公式用程序表示就是： 123for num in nums: for i in range(c, num - 1, -1): w[i] = w[i] or w[i - num] ​ 举例来说： ​ 对于输入[1,5,11,5]来说，​ 当num=1时，通过递推式只能得到w[1]=true​ 当num=5时，通过递推式能够得到w[5]=true,w[6]=true，因为可以通过1+5组合​ 当num=5时，通过递推式能够得到新的w[11]=true（5+6=11）​ 当num=11时，没有新改动w​ 所以此时可以发现w[11]=true，所以可以等分 123456789101112131415def canPartition(self, nums) -&gt; bool: # 计算总价值 c = sum(nums) # 奇数直接排除 if c % 2 != 0: return False c = c // 2 w = [False] * (c + 1) # 第0个位置设置为true，表示当元素出现的时候让w[i-num]为True,也就是w[i]为True w[0] = True for num in nums: for i in range(c, num - 1, -1): w[i] = w[i] or w[i - num] return w[c] ​ 当然本题也就可以使用BST，但是时间复杂度太高，leetcode没过 ##### 回溯法-深度优先搜索BST​ 在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根结点出发深度探索解空间树。当探索到某一结点时，要先判断该结点是否包含问题的解，如果包含，就从该结点出发继续探索下去，如果该结点不包含问题的解，则逐层向其祖先结点回溯。 ​ 核心：暴力遍历 1.求解一个集合的全部子集给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 说明：解集不能包含重复的子集。 示例: 123456789101112输入: nums = [1,2,3]输出:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] ​ 找子集相关问题的BST基本上采用的核心思想：每个位置都可能出现采用或者不采用两种情况，而如果可能出现重复的元素，那么就要事先将原数组进行排序，存进result之前判断是否已有 1234567891011121314151617181920def subsets(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ def core(nums,i,tmp): if i==length: result.append(tmp) return #每次向后遍历时有两种情况，一种是将当前节点值加入到tmp中，一种是不加入 core(nums,i+1,tmp+[nums[i]]) core(nums,i+1,tmp) nums.sort() length = len(nums) result = [] core(nums,0,[]) return result 拓展：含重复的子集 1234567891011121314151617181920def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ def core(nums,i,tmp): if i==length: if tmp not in result: result.append(tmp) return core(nums,i+1,tmp) core(nums,i+1,tmp+[nums[i]]) length = len(nums) result = [] nums.sort() #这里必须要先排序 core(nums,0,[]) return result 2.全排列给定一个没有重复数字的序列，返回其所有可能的全排列。 示例: 12345678910输入: [1,2,3]输出:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 12345678910111213141516class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: def core(nums,tmp): if nums==[]: result.append(tmp) return for num in nums: s = nums[::] s.remove(num) core(s,tmp+[num]) result = [] core(nums,[]) return result 拓展：含重复数组的全排列 12345678910111213141516171819def permuteUnique(self, nums: List[int]) -&gt; List[List[int]]: def core(nums,tmp): if nums==[]: if tmp not in result: result.append(tmp) return for num in nums: s = nums[::] s.remove(num) core(s,tmp+[num]) result = [] nums.sort() core(nums,[]) return result 3.划分为k个相等的子集给定一个整数数组 nums 和一个正整数 k，找出是否有可能把这个数组分成 k 个非空子集，其总和都相等。 示例 1： 123输入： nums = [4, 3, 2, 3, 5, 2, 1], k = 4输出： True说明： 有可能将其分成 4 个子集（5），（1,4），（2,3），（2,3）等于总和。 12345678910111213141516171819202122232425262728293031def canPartitionKSubsets(self, nums: List[int], k: int) -&gt; bool: if k == 1: return True #如果不能被k整除，那么直接无解 sum_num = sum(nums) if sum_num % k != 0: return False avg = sum_num // k nums.sort(reverse=True) n = len(nums) if n &lt; k :return False visited = set() #标志位，标志哪个位置已经被使用过了 def dfs(k,tmp_sum,loc): #当选用的几个数之和等于目标值，那么k减一，再找下一个子集 if tmp_sum == avg: return dfs(k-1,0,0) #如果k==1，由于上面已经验证过可以被k整除，因此一定成立 if k == 1: return True for i in range(loc,n): if i not in visited and nums[i] + tmp_sum &lt;= avg: visited.add(i) if dfs(k,tmp_sum+nums[i],i+1): return True visited.remove(i) return False return dfs(k,0,0)]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——词向量表示之word2vec]]></title>
    <url>%2F2019%2F04%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E4%B9%8Bword2vec%2F</url>
    <content type="text"><![CDATA[原始的神经网络语言模型：里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层），里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值 Word2Vec对原始语言模型的改进： 1.对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。 比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。 2.word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射（Hierarchical Softmax）。这样隐藏层到输出层的softmax不是一步完成的，而是沿着哈弗曼树一步一步完成的。 Hierarchical Softmax​ 和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。 使用Hierarchical Softmax的好处 1.由于是二叉树，之前计算量为V,现在变成了log2V 2.由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。 算法过程STEP 1：扫描语料库，统计每个词出现的频数，保存在一个hash表中 STEP2：根据个词的词频建立哈弗曼树 最终每个词汇都是哈弗曼树的叶子节点，词频就是相应的权值 根节点对应的词向量就是我们投影后的词向量 而所有叶子节点就类似神经网络softmax输出层的神经元，叶子节点个数就是词汇表大小 非叶子节点代表某一类词 哈弗曼树建立好后每个词都会有一个二进制的哈弗曼编码 STEP3：初始化词向量和哈弗曼树非叶子节点的向量 ​ 向量维度是我们给定的参数K。 STEP4：训练，也就是通过梯度下降算法不断优化词向量 ​ 在初始化后的词向量，回到语料库，逐句读取一系列的词，然后用梯度下降算法算法算出梯度，更新词向量的值、非叶子检点的值。(哈弗曼树就相当于一个优化后的神经网络) 参数更新过程基于Negative Sampling的Word2vecHierarchical Softmax的的缺点： ​ 对于生僻词需要在哈弗曼树中向下走很久。 Negative Sampling算法​ Negative Sampling不再使用(复杂的Huffman树），而是利用相对简单的随机负采样，能大幅度提升性能，因此，将其作为Hierarchical softmax的替代方案 ​ 核心思想：通过负采样将问题转化为求解一个正例和neg个负例进行二元回归问题。每次只是通过采样neg个不同的中心词做负例，就可以训练模型 ​ 方法：我们有一个训练样本，中心词是w,它周围上下文共有2c个词，记为context(w)。由于这个中心词w,的确和context(w)相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和w不同的中心词wi,i=1,2,..neg，这样context(w)和wi就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词wi对应的模型参数θi，和每个词的词向量。 ​ 本质上是对训练集进行了采样，从而减小了训练集的大小。 Negative Sampling负采样方法 3、 word2vec负采样有什么作用？ 1.加速了模型计算，模型每次只需要更新采样的词的权重，不用更新所有的权重 2.保证了模型训练的效果，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系 常见问题1.skip gram和cbow各自的优缺点 ​ (1) cbow的速度更快，时间复杂度为O(V)，skip-gram速度慢,时间复杂度为O(nV) ​ 在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V); ​ 而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围每个词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。 ​ (2)当数据较少或生僻词较多时，skip-gram会更加准确； ​ 在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。 2.Negative Sampling和Hierarchical softmax各自的优缺点 Hierarchical softmax 优点： ​ 1.由于是二叉树，之前计算量为V,现在变成了log2V，效率更高 ​ 2.由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到。 缺点: ​ 对于生僻词在hierarchical softmax中依旧需要向下走很久 Negative Sampling 优点： ​ 1.对于低频词的计算效率依然很高 ​ 3.word2vec的缺点 1.使用的只是局部的上下文信息，对上下文的利用有限 2.和glove相比比较难并行化 ​ 4、word2vec和fastText对比有什么区别？（word2vec vs fastText） 1）都可以无监督学习词向量， fastText训练词向量时会考虑subword； 2）fastText还可以进行有监督学习进行文本分类，其主要特点： 结构与CBOW类似，但学习目标是人工标注的分类结果； 采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径； 引入N-gram，考虑词序特征； 引入subword来处理长词，处理未登陆词问题； 参考文献：基于Negative Sampling的模型 基于Hierarchical Softmax的模型]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——高斯混合模型GMM]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8BGMM%2F</url>
    <content type="text"><![CDATA[GMM 是学习出一些概率密度函数 k-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment。 假设数据服从 Mixture Gaussian Distribution ，换句话说，数据可以看作是从数个 Gaussian Distribution 中生成出来的]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习——EM算法]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94EM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[准备知识####1.参数估计的方法 概率模型的参数估计分为两大类： 1.不含隐变量的参数估计—极大似然估计/贝叶斯估计法 2.含隐变量的参数估计—EM算法 2.jensen不等式X是一个随机变量，f(X)是一个凸函数（二阶导数大或等于0），那么有： 当且仅当X是常数的时候等号成立 如果f（X）是凹函数，不等号反向 3.先验概率、后验概率、条件概率​ 先验概率：P(Y) 先验概率是只根据事情之前发生各个结果出现情况估计的概率(无关特征) ​ 后验概率：P(Y|X) 后验概率是在各个X的分布下各个Y出现的概率(特征符合这个X时Y为这个的概率) ​ 条件概率：P(X|Y) 条件概率是在结果某一种情况时X出现这种分布的概率 4.自信息、互信息​ 自信息：I(x) = -logp(x) ​ 概率是衡量确定性的度量，那么信息是衡量不确定性的度量.越不确定信息量越高。 ​ 互信息：I(x;y) = log(p(x|y)/p(x)) ​ 已知y，x的不确定性减少量(其值可正可负) 5.熵​ 对随机变量平均不确定性的度量，一个系统越有序，信息熵越低。 ​ 熵的另一种解读也就是自信息的期望 ​ H(X) = E[I(X)] = ∑P(x)I(x) = -∑p(x)logp(x) 6.条件熵​ 在给定y条件下，x的条件自信息量为I(x|y)，X的集合的条件熵为 ​ 进一步在给定Y（各个y）的条件下，X集合的条件熵： ​ ​ 也就是在联合符号集合上的条件自信息量两个概率的加权平均 EM算法​ EM算法主要用于求解概率模型的极大似然估计或极大后验概率。EM算法是通过迭代求解观测数据对数似然函数L(θ) = logP(Y|θ)的极大化，实现参数估计的。 每次迭代主要分为E、M两步： ​ E步：求期望。即求log(P，Z|θ)关于P(Z|Y，θi)的期望 (各个隐变量可能的概率下乘以出现这种结果的总和) ​ ​ M步：极大化Q函数得到新的参数θ ​ 在构建具体的EM算法时，最重要的时定义Q函数，每次迭代中，Em算法通过极大似然化Q函数来增大对数似然函数L(θ) 算法推导注意：1.EM算法在每次迭代后均能提高观测数据的似然函数值 ​ 2.EM算法不能保证全局最优，只能保证局部最优，因此算法受初值的影响 ​ 3.EM算法可以用于无监督学习]]></content>
      <tags>
        <tag>面试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——XGBoost]]></title>
    <url>%2F2019%2F03%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94XGBoost%2F</url>
    <content type="text"><![CDATA[XGB的优势​ 1. XGBoost加入了正则化项，正则化项中包含了叶子节点个数，使学到的模型更加简单。原始的GBDT没有，可以有效防止过拟合 ​ 2. XGBoost实现了局部并行计算，比原始的GBDT速度快的多 ​ 3. XGBoost中内置了缺失值的处理，尝试对缺失值进行分类，然后学习这种分类 ​ 4. 可在线学习，这个sklearn中的GBDT也有 ​ 5. XGboost允许在交叉验证的过程中实现boosting，通过一次run就能得到boosting迭代的优化量；而GBDT只能人工的使用grid-search ​ 6.支持列抽样。不仅能有效防止过拟合，还能减少计算量 XGBoost的并行计算是如何实现的？ ​ 注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完成才能进行下一次迭代的（第t次迭代的代价函数里面包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行排序，然后保存block结构，后面的迭代中重复的使用这个结构，大大减小计算量。这个block结构也使得并行称为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 XGBoost的参数​ XGBoost的参数主要分为三大类： 1.调控整个方程的参数 2.调控每步树的参数 3.调控优化表现的变量 1.调控整个方程的参数 booster [defalut=gbtree] 基模型 gbtree：树模型 gblinear：线性模型 nthread [default to maximum number of threads available if not set] 使用的线程数 用于并行计算，默认使用全部内核 2.调节基分类器的参数​ 这里只讨论树模型作为基模型的情况，因为树模型作为基分类器效果总是优于线性模型。 eta/learning rate [default=0.3] 学习的初始速率 通过减小每一步的权重能够使建立的模型更加具有鲁棒性 通常最终的数值范围在[0.01-0.2]之间 Shrinkage（缩减），相当于学习速率。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了消弱每棵树的影响，让后面有更大的学习空间。在实际应用中，一般把学习率设置的小一点，然后迭代次数设置的大一点（补充：传统GBDT的实现也有学习速率） gamma [default=0] 一个节点分裂的条件是其分裂能够起到降低loss function的作用，gamma 定义loss function降低多少才分裂 它的值取决于 loss function需要被调节 lambda/reg_lambda [default=1] L2正则化的权重，用于防止过拟合 alpha/reg_alpha [default=0] L1正则化的权重，可以用于特征选择 一般用于特征特别多的时候，可以大大提升算法的运算效率 subsample [default=1] 每棵树使用的样本比例 [0.5~1] 低值使得模型更保守且能防止过拟合，但太低的值会导致欠拟合 colsample_bytree [default=1] 每棵树随机选取的特征的比例 [0.5-1] 3.调控优化表现的参数 objective [default=reg:linear] eval_metric seed 调参调参开始时一般使用较大的学习速率 0.1 1.初始参数设置 max_depth = 5 min_child_weight = 1 #如果是不平衡数据，初始值设置最好小于1 2.首先调节的参数 max_depth和min_child_weight​ 在整个GBDT中，对整个模型效果影响最大的参数就是max_depth和min_child_weight。 max_depth 一般在3~10先用step为2进行网格搜索找到范围，找到范围再用step为1的网格搜索确定具体值 min_child_weight 一般现在1~6先使用step为2的网格搜索找到最佳参数值范围，然后再用step为1的网格索索确定具体参数值 3. 调整gamma gamma参数主要用于控制节点是否继续分裂，一般使用网格搜索在0~0.5之间进行步长为0.1的搜索 4.调整subsample和colsample_bytree 这两个参数主要是用来防止拟合的，参数值越小越能防止过拟合 一般0.6~1之间网格搜索 5.尝试降低学习速率增加更多的树 学习速率降为0.1或0.01 结论：1.仅仅通过调参来提升模型效果是很难的 ​ 2.要想提升模型效果最主要是通过特征工程、模型融合等方式]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-BN]]></title>
    <url>%2F2019%2F03%2F28%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-BN%2F</url>
    <content type="text"><![CDATA[为什么要进行归一化？ ​ 原因在于神经网络的本身就在于学习数据的分布，一旦训练数据和测试数据分布不同，那么网络的泛化能力也将大大降低；另外一方面，再使用BSGD时一旦每批训练数据的分布不相同，那么网络在每次进行迭代时都要去适应不同的数据分布，这将大大降低网络的学习速度。 为什么要使用BN？ ​ 这主要是因为对于一般的归一化，只是在输入网络之前对数进行了归一化，而在神经网络的训练过程中并没有对数据做任何处理，而在神经网络的的训练过程中只要网络的前面几层的数据分布发生微小的变化，那么后面的网络就会不断积累放大这个分布的变化，因此一旦有任意一层的数据发生改变，这层以及后面的网络都会需要去从新适应学习这个新的数据分布，而如果训练过程中，每一层的数据都在不断发生变化，那么更将大大影响网络的训练速度，因此需要在网络的每一层输入之前都将数据进行一次归一化，保证数据分布的相同，加快网络训练速度。 ​ 在另一方面，由于将网络的每一步都进行了标准化，数据分布一致，因此模型的泛化能力将更强。 BN的本质是什么？ 一个可学习、有参数（γ、β）的使每层数据之前进行归一化的网络层 BN使用位置 线性层后全连接层之前 BN过程 对于一般的归一化没使用下面的公式进行归一化计算： ​ 但是如果仅仅使用上面的公式来对某层的输出做下一层的输入做归一化，那么是会影响到前面一层学习到的特征的。例如：网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，强制把它归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被搞坏了。因此，BN引入了可学习的参数γ、β： ​ ​ 上面的公式表明，通过学习到的重构参数γ、β，是可以恢复出原始的某一层所学到的特征的。 BN中为什么要在后面γ、β？不加可以吗？ ​ 不可以，因为这是BN中的最关键步骤。不使用γ、β会造成归一化的同时破坏前一层提取到的特征，而BN通过记录每个神经元上的γ、β，使前一层的特征可以通过γ、β得以还原。 BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？ ​ 对于CNN上采用了类似权值共享的策略，将一个特征图看做一个神经元，因此参数个数并不会很多。 例如：如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,w,h)，m为min-batch sizes，f为特征图个数，w、h分别为特征图的宽高。在CNN中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m.w.h，于是对于每个特征图都只有一对可学习参数：γ、β，总参数个数也就是2m个。 BN的作用 1.防止过拟合。有了BN，dropout和正则化的需求下降了 2.加速训练]]></content>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——二叉树遍历]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[​ 二叉树最常用的遍历算法主要分为下面几种： ​ 1.先序遍历 ​ 2.中序遍历 ​ 3.后序遍历 ​ 4.层次遍历 ​ 下面我们将针对这些遍历算法的递归与非递归实现分别给出代码实现以及特点。 这里有一点我们需要注意: ​ 无论是前序、中序、后续，都是指根节点访问的顺序，而左右节点的相对访问顺序永远是相同的，即先访问做节点，后访问右节点。 先序遍历​ 先序遍历指在二叉树遍历过程中首先输出根节点，然后再分别输出左右节点的遍历方式。 #####递归实现 123456789101112131415def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ def core(result,root): if root==None: return result.append(root.val) core(result,root.left) core(result,root.right) result = [] core(result,root) return result 非递归实现123456789101112131415161718192021def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] res = [] stack = [root] while stack: node = stack.pop() res.append(node.val) #注意这里的顺序一定是先右后左，和一般的相反 if node.right!=None: stack.append(node.right) if node.left!=None: stack.append(node.left) return res 中序遍历​ 二叉树的中序遍历是指现先遍历左节点，中间遍历根节点，最后在遍历右节点的便利方式。 递归实现1234567891011121314def Core(root): if root==None: return [] Core(root.left) result.append(root.val) Core(root.right) return result result = [] Core(root) return result 非递归实现12345678910111213141516171819202122def inorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] stack = [] result = [] pos = root while stack or pos: if pos: stack.append(pos) pos = pos.left else: pos = stack.pop() result.append(pos.val) pos = pos.right return result 后序遍历层次遍历非递归实现​ 利用队列先进先出的特点，依次将结点的左、右孩子入队，然后依次出队访问，以此为循环。当有些题目中要求按照层输出时，需要根据每层的节点个数做一个计数。 1234567891011121314151617181920212223242526def levelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ if not root: return [] queue = [root] result = [] while queue: tmp = [] number_flag = len(queue) #层节点个数计数器 i = 0 while i&lt;number_flag: node = queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) i += 1 result.append(tmp) return result 根据两个序列复原二叉树​ 这种题目其实只有两个，核心是找出先根据一个序列找出根节点，然后在根据另一个序列找出其左右子树的元素，然后不断的递归这个过程即可。 已知前序遍历中序遍历​ 在已知前序遍历的题目中，就以前序遍历为基础，去不断地区分剩下的数据应该在左子树还是右子树即可 12345678910111213141516171819202122232425262728def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: """ 先将前序遍历的第一个节点作为根节点，然后在后序遍历中找到其对应的位置，左右分别做相同的操作 """ len_pre = len(preorder) len_in = len(inorder) if len_pre==0 or len_in==0: return None tree_root = TreeNode(preorder[0]) preorder = preorder[1:] left_len = 0 for i in inorder: if i==tree_root.val: break else: left_len+=1 inorder.remove(tree_root.val) if left_len&gt;=1: tree_root.left = self.buildTree(preorder[:left_len],inorder[:left_len]) if len(preorder)-left_len&gt;=1: tree_root.right = self.buildTree(preorder[left_len:],inorder[left_len:]) return tree_root 已知前序遍历和后序遍历123456789101112131415161718192021222324252627282930def constructFromPrePost(self, pre, post): """ :type pre: List[int] :type post: List[int] :rtype: TreeNode """ """ 前序遍历的第一个节点必定是根节点，随后的节点就是其左子树的根节点，然后再在 后序遍历中找到这个节点的位置就可以确定左子树中有哪些节点，右子树中有哪些节点 """ tree_root = TreeNode(pre[0]) pre = pre[1:] post = post[:-1] left_len = 0 for i in post: if i==pre[0]: left_len+=1 break else: left_len+=1 if left_len&gt;=1: tree_root.left = self.constructFromPrePost(pre[:left_len],post[:left_len]) if len(post)-left_len&gt;=1: tree_root.right = self.constructFromPrePost(pre[left_len:],post[left_len:]) return tree_root 已知中序后序遍历构造二叉树没有前序遍历时，使用后序遍历定根节点 1234567891011121314151617181920212223def buildTree(self, inorder: List[int], postorder: List[int]) -&gt; TreeNode: len_in = len(inorder) len_post = len(postorder) if len_in==0 or len_in!=len_post: return None tree_root = TreeNode(postorder[-1]) postorder = postorder[:-1] left_len = 0 for i in inorder: if i==tree_root.val: break else: left_len += 1 inorder.remove(tree_root.val) if left_len&gt;=1: tree_root.left = self.buildTree(inorder[:left_len],postorder[:left_len]) if len(postorder)-left_len&gt;=1: tree_root.right = self.buildTree(inorder[left_len:],postorder[left_len:]) return tree_root 二叉搜索树​ 二叉搜索树的性质: ​ 1.中序遍历的结果有序 ​ 2.左子树上的节点都比根节点小，右子树都比根节点大 修剪二叉搜索树​ 给定一个二叉搜索树，同时给定最小边界L 和最大边界 R。通过修剪二叉搜索树，使得所有节点的值在[L, R]中 (R&gt;=L) 。你可能需要改变树的根节点，所以结果应当返回修剪好的二叉搜索树的新的根节点。 1234567891011121314151617181920def trimBST(self, root, L, R): """ :type root: TreeNode :type L: int :type R: int :rtype: TreeNode """ if root==None: return None if root.val&lt;L: return self.trimBST(root.right,L,R) elif root.val&gt;R: return self.trimBST(root.left,L,R) else: root.left = self.trimBST(root.left,L,R) root.right = self.trimBST(root.right,L,R) return root 把二叉搜索树转化为累加树给定一个二叉搜索树（Binary Search Tree），把它转换成为累加树（Greater Tree)，使得每个节点的值是原来的节点值加上所有大于它的节点值之和。 例如： 123456789输入: 二叉搜索树: 5 / \ 2 13输出: 转换为累加树: 18 / \ 20 13 1234567891011121314151617def convertBST(self, root): """ :type root: TreeNode :rtype: TreeNode """ root_ref = root stack = [] prev = 0 while stack or root: while root: stack.append(root) root = root.right root = stack.pop() root.val += prev prev = root.val root = root.left return root_ref 验证搜索二叉树给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 123456789101112131415161718192021222324252627方法一：用搜索二叉树的性质1，中序遍历一定有序，那么我们只需要在中序遍历中保证后添加的数比前面添加的最后一个数的即可，出现不符合这一规律的直接返回False 注：这里需要特别注意，二叉搜索数中不能出现两个一样的值，因此不能直接输出中序序列和排序号好的序列对比def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ stack = [] pos = root result = [] while stack or pos: while pos: stack.append(pos) pos = pos.left pos = stack.pop() if result!=[]: if result[-1]&lt;pos.val: result.append(pos.val) else: return False else: result.append(pos.val) pos = pos.right return True]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-回文子串相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[回文子串 例：给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被计为是不同的子串。 1234567891011121314def countSubstrings(self, s): """ :type s: str :rtype: int """ length = len(s) result = 0 for i in range(length): for j in range(i+1,length+1): #这里注意循环的范围为range(i+1,length+1) if s[i:j]==s[i:j][::-1]: result += 1 return result 最长回文子串​ 最长回文子串也是回文串中常见的一中题目，下面是例题 例：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 思路一：Manacher算法 ​ 首先先将字符串首尾以及字符和字符之间采用”#“进行补齐，补齐后的字符串总长度2n+1(n为原始字符串长度)。然后从第一个非#字符 12345678910111213141516171819202122232425262728293031323334def get_length(string, index): # 循环求出index为中心的最长回文字串 length = 0 seq = "" if string[index]!="#": seq = string[index] length = 1 string_len = len(string) for i in range(1,index+1): if index+i&lt;string_len and string[index-i]==string[index+i]: # print(string[index-i],seq+string[index+i]) if string[index-i]!="#": length +=2 seq = string[index-i]+seq+string[index+i] else: break return length,seq s_list = [i for i in s] string = "#"+"#".join(s)+"#" length = len(string) max_length = 0 max_seq = "" for index in range(0,length): # print("====") tmp_len,tmp_seq = get_length(string,index) # print(tmp_len,tmp_seq) if tmp_len&gt;max_length: max_length = tmp_len max_seq = tmp_seq return max_seq 思路二：动态规划 ​ 这里的动态规划的核心思路就是从头开始向后进行遍历，每次想看头尾同时加入比最大之前最大回文子串的长多+1字符串是不是回文子串(注意但是首部索引不能超过0)，如果是则记录起始节点start，max_len的值+2；否则判断只在尾部进行字符串加1的字符串时不是回文子串（这里之说以不必尝试在头部加1，因为再从头开始遍历的过程中已经尝试了头部加1），如果是记录start节点，max_len的值+2 ​ f(x+1) 12345678910111213141516171819def longestPalindrome(self, s): """ :type s: str :rtype: str """ length = len(s) max_len = 0 start = 0 for i in range(length): if i-max_len&gt;=1 and s[i-max_len-1:i+1]==s[i-max_len-1:i+1][::-1]: start = i-max_len-1 max_len += 2 elif i-max_len&gt;=0 and s[i-max_len:i+1]==s[i-max_len:i+1][::-1]: start = i-max_len max_len += 1 return s[start:start+max_len] 最长回文子序列516​ z]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-含环链表相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E5%90%AB%E7%8E%AF%E9%93%BE%E8%A1%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[​ 在含环的问题中，存在一些关键性的结论，在解决问题时非常有帮助，下面是一些相关的总结。 1.判断链表是否有环​ 结论：一个速度为1的low指针和一个速度为2的fast指针同时从头向前走，如果其中fast指针为None，那么则为无环，如果两个只能指向的元素相等，那么链表有环。 2.判断链表的环入口节点​ 结论：函数一样的双指针进行遍历，如果fast指针为None,那么则为无环。如果两个指针指向的的元素相同，那么这个节点到链表入口点的长度和链表头到链表入口点的长度相等。 推导过程： ​ 设链表头到入口节点的长度为a ​ 链表入口节点到相遇节点的长度为b ​ 相遇节点到链表入口节点的长度为c ​ 那么因为fast的速度为2，low的速度为1，因此可以认为low入环时走在前面，每次fast和low之间的距离缩小1，因此，必定会在第一圈完成之前相遇。所以有 ​ low 在环内位置: (a+b)-a mod (b+c) -&gt; b mod (b+c) ​ fast 在环内位置：2(a+b)-a mod (b+c) -&gt; a+2b mod (b+c) 二者应该相等，因此得出 a+b mod (b+c) = 0 即a = c ​ 利用这个结论，我们可以先判断判断链表是否有环，如果有环，那么先找到相间的节点，然后再用一个新指针从头开始以速度为1和low指针从相交节点同时开始遍历，当两个点相交的节点即为环入口节点。 例题：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null. 12345678910111213141516def detectCycle(head): """ :type head: ListNode :rtype: ListNode """ low,fast = head,head while fast and fast.next and fast.next: low, fast = low.next, fast.next.next if fast==low: p = head while p!=low: p = p.next low = low.next return p return None 3.变形型题目​ 有一类题目不会明显的说让解决环的问题，但是使用环来解决，往往会起到意想不到的效果。 例题：编写一个程序，找到两个单链表相交的起始节点。 123456789101112131415161718192021222324252627282930313233343536373839def getIntersectionNode(headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ if headA==None or headB==None: return None #相判断两个是否相交 pA = headA pB = headB while pA.next: pA = pA.next while pB.next: pB = pB.next if pA!=pB: return None #将PA首尾相接 tail = pA pA.next = headA fast = headB low = headB while True: fast = fast.next.next low = low.next if fast==low: s = headB while s!=low: low = low.next s = s.next tail.next = None return s ​ 这道题利用了和上一道题目完全一样的规律解决]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——transformer模型]]></title>
    <url>%2F2019%2F02%2F28%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94transformer%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ transformer模型来自于Google的经典论文Attention is all you need，在这篇论文中作者采用Attention来取代了全部的RNN、CNN，实现效果效率的双丰收。 ​ 现在transformer在NLP领域已经可以达到全方位吊打CNN、RNN系列的网络，网络处理时间效率高，结果稳定性可靠性都比传统的CNN、RNN以及二者的联合网络更好，因此现在已经呈现出了transformer逐步取代二者的大趋势。 ​ 下面是三者在下面四个方面的对比试验结果 ​ 1.远距离特征提取能力 ​ 2.语义特征提取能力 ​ 3.综合特征提取能力 ​ 4.特征提取效率 下面是从一系列的论文中获取到的RNN、CNN、Transformer三者的对比结论： ​ 1.从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。 ​ 2.速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。(主流经验上transformer和CNN速度差别不大，RNN比前两者慢3倍到几十倍) Transformer模型具体细节​ transformer模型整体结构上主要Encoder和Decoder两部分组成，Encoder主要用来将数据进行特征提取，而Decoder主要用来实现隐向量解码出新的向量表示(原文中就是新的语言表示)，由于原文是机器翻译问题，而我们要解决的问题是类文本分类问题，因此我们直接减Transformer模型中的Encoder部分来进行特征的提取。其中主要包括下面几个核心技术模块： ​ 1.残差连接 ​ 2.Position-wise前馈网络 ​ 3.多头self-attention ​ 4.位置编码 ​ 1.采用全连接层进行Embedding （Batch_size,src_vocab_size,model_dim） ​ 2.在进行位置编码，位置编码和Embedding的结果进行累加 ​ 3.进入Encoder_layer进行编码处理(相当于特征提取) ​ (1) ​ 1.位置编码（PositionalEncoding）​ 大部分编码器一般都采用RNN系列模型来提取语义相关信息，但是采用RNN系列的模型来进行语序信息进行提取具有不可并行、提取效率慢等显著缺点，本文采用了一种 Positional Embedding方案来对于语序信息进行编码，主要通过正余弦函数， ​ 在偶数位置，使用正弦编码;在奇数位置使用余弦进行编码。 为什么要使用三角函数来进行为之编码？ ​ 首先在上面的公式中可以看出，给定词语的pos可以很简单其表示为dmodel维的向量，也就是说位置编码的每一个位置每一个维度对应了一个波长从2π到10000*2π的等比数列的正弦曲线，也就是说可以表示各个各个位置的绝对位置。 ​ 在另一方面，词语间的相对位置也是非常重要的，这也是选用正余弦函数做位置编码的最主要原因。因为 ​ sin(α+β) = sinαcosβ+cosαsinβ ​ cos(α+β) = cosαcosβ+sinαsinβ ​ 因此对于词汇间位置偏移k，PE(pos+k)可以表示为PE(pos)和PE(k)组合的形式，也就是具有相对位置表达能力 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class PositionalEncoding(nn.Module): """ 位置编码层 """ def __init__(self, d_model, max_seq_len): """ 初始化 Args: d_model: 一个标量。模型的维度，论文默认是512 max_seq_len: 一个标量。文本序列的最大长度 """ super(PositionalEncoding, self).__init__() # 根据论文给的公式，构造出PE矩阵 position_encoding = np.array([ [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)] for pos in range(max_seq_len)]) # 偶数列使用sin，奇数列使用cos position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2]) position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2]) position_encoding = torch.Tensor(position_encoding) # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似 # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐， # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码 pad_row = torch.zeros([1, d_model]) position_encoding = torch.cat((pad_row, position_encoding)) # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码， # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似 self.position_encoding = nn.Embedding(max_seq_len + 1, d_model) self.position_encoding.weight = nn.Parameter(position_encoding, requires_grad=False) def forward(self, input_len,max_len): """ 神经网络的前向传播。 Args: input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。 param max_len:数值，表示当前的词的长度 Returns: 返回这一批序列的位置编码，进行了对齐。 """ # 找出这一批序列的最大长度 tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor # 对每一个序列的位置进行对齐，在原序列位置的后面补上0 # 这里range从1开始也是因为要避开PAD(0)的位置 input_pos = tensor( [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len.tolist()]) return self.position_encoding(input_pos) 2.scaled Dot-Product Attention​ scaled代表着在原来的dot-product Attention的基础上加入了缩放因子1/sqrt(dk)，dk表示Key的维度，默认用64. 为什么要加入缩放因子？ ​ 在dk(key的维度)很大时，点积得到的结果维度很大，使的结果处于softmax函数梯度很小的区域，这是后乘以一个缩放因子，可以缓解这种情况的发生。 ​ Dot-Produc代表乘性attention，attention计算主要分为加性attention和乘性attention两种。加性 Attention 对于输入的隐状态 ht 和输出的隐状态 st直接做 concat 操作，得到 [ht:st] ，乘性 Attention 则是对输入和输出做 dot 操作。 ​ Attention又是什么呢？通俗的解释Attention机制就是通过query和key的相似度确定value的权重。论文中具体的Attention计算公式为： ​ 在这里采用的scaled Dot-Product Attention是self-attention的一种，self-attention是指Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。就是下面来具体说一下K、Q、V具体含义： 在一般的Attention模型中，Query代表要进行和其他各个位置的词做点乘运算来计算相关度的节点，Key代表Query亚进行查询的各个节点，每个Query都要遍历全部的K节点，计算点乘计算相关度，然后经过缩放和softmax进行归一化的到当前Query和各个Key的attention score，然后再使用这些attention score与Value相乘得到attention加权向量 在self-attention模型中，Key、Query、Value均来自相同的输入 在transformer的encoder中的Key、Query、Value都来自encoder上一层的输入，对于第一层encoder layer，他们就是word embedding的输出和positial encoder的加和。 query、key、value来源： ​ 他们三个是由原始的词向量X乘以三个权值不同的嵌入向量Wq、Wk、Wv得到的，三个矩阵尺寸相同 Attention计算步骤： 如上文，将输入单词转化成嵌入向量； 根据嵌入向量得到 q、k、v三个向量； 为每个向量计算一个score： score = q*k 为了梯度的稳定，Transformer使用了score归一化，即除以 sqrt(dk)； 对score施以softmax激活函数； softmax点乘Value值 v ，得到加权的每个输入向量的评分 v； 相加之后得到最终的输出结果Sum(z) ： 。 1234567891011121314151617181920212223242526272829303132333435363738class ScaledDotProductAttention(nn.Module): """ 标准的scaled点乘attention层 """ def __init__(self, attention_dropout=0.0): super(ScaledDotProductAttention, self).__init__() self.dropout = nn.Dropout(attention_dropout) self.softmax = nn.Softmax(dim=2) def forward(self, q, k, v, scale=None, attn_mask=None): """ 前向传播. Args: q: Queries张量，形状为[B, L_q, D_q] k: Keys张量，形状为[B, L_k, D_k] v: Values张量，形状为[B, L_v, D_v]，一般来说就是k scale: 缩放因子，一个浮点标量 attn_mask: Masking张量，形状为[B, L_q, L_k] Returns: 上下文张量和attention张量 """ attention = torch.bmm(q, k.transpose(1, 2)) if scale: attention = attention * scale if attn_mask is not None: # 给需要 mask 的地方设置一个负无穷 attention = attention.masked_fill(attn_mask,-1e9) # 计算softmax attention = self.softmax(attention) # 添加dropout attention = self.dropout(attention) # 和V做点积 context = torch.bmm(attention, v) return context, attention 3.多头Attention​ 论文作者发现将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention 效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。 ​ 这里采用了四个全连接层+有个dot_product_attention层(也可以说是8个)+layer_norm实现。 为什么要使用多头Attention？ ​ 1.”多头机制“能让模型考虑到不同位置的Attention ​ 2.”多头“Attention可以在不同的足空间表达不一样的关联 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MultiHeadAttention(nn.Module): """ 多头Attention层 """ def __init__(self, model_dim=512, num_heads=8, dropout=0.0): super(MultiHeadAttention, self).__init__() self.dim_per_head = model_dim // num_heads self.num_heads = num_heads self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads) self.dot_product_attention = ScaledDotProductAttention(dropout) self.linear_final = nn.Linear(model_dim, model_dim) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, key, value, query, attn_mask=None): # 残差连接 residual = query dim_per_head = self.dim_per_head num_heads = self.num_heads batch_size = key.size(0) # 线性层 (batch_size,word_nums,model_dim) key = self.linear_k(key) value = self.linear_v(value) query = self.linear_q(query) # 将一个切分成多个(batch_size*num_headers,word_nums,word//num_headers) """ 这里用到了一个trick就是将key、value、query值要进行切分不需要进行真正的切分，直接将其维度整合到batch_size上，效果等同于真正的切分。过完scaled dot-product attention 再将其维度恢复即可 """ key = key.view(batch_size * num_heads, -1, dim_per_head) value = value.view(batch_size * num_heads, -1, dim_per_head) query = query.view(batch_size * num_heads, -1, dim_per_head) #将mask也复制多份和key、value、query相匹配 （batch_size*num_headers,word_nums_k,word_nums_q） if attn_mask is not None: attn_mask = attn_mask.repeat(num_heads, 1, 1) # 使用scaled-dot attention来进行向量表达 #context:(batch_size*num_headers,word_nums,word//num_headers) #attention:(batch_size*num_headers,word_nums_k,word_nums_q) scale = (key.size(-1)) ** -0.5 context, attention = self.dot_product_attention( query, key, value, scale, attn_mask) # concat heads context = context.view(batch_size, -1, dim_per_head * num_heads) # final linear projection output = self.linear_final(context) # dropout output = self.dropout(output) # 这里使用了残差连接和LN output = self.layer_norm(residual + output) return output, attention 4.残差连接​ 在上面的多头的Attnetion中，还采用了残差连接机制来保证网络深度过深从而导致的精度下降问题。这里的思想主要来源于深度残差网络(ResNet)，残差连接指在模型通过一层将结果输入到下一层时也同时直接将不通过该层的结果一同输入到下一层，从而达到解决网络深度过深时出现精确率不升反降的情况。 为什么残差连接可以在网络很深的时候防止出现加深深度而精确率下降的情况？ ​ 神经网络随着深度的加深会会出现训练集loss逐渐下贱，趋于饱和，然后你再加深网络深度，训练集loss不降反升的情况。这是因为随着网络深度的增加，在深层的有效信息可能变得更加模糊，不利于最终的决策网络做出正确的决策，因此残差网络提出，建立残差连接的方式来将低层的信息也能传到高层，因此这样实现的深层网络至少不会比浅层网络差。 5.Layer normalizationNormalization​ Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 #####Batch Normalization(BN) ​ 应用最广泛的Normalization就是Batch Normalization，其主要思想是:在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 Layer normalization(LN)​ LN 是在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差. Layer normalization在pytorch 0.4版本以后可以直接使用nn.LayerNorm进行调用 6.Mask​ mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。 ​ 在我们使用的Encoder部分，只是用了padding mask因此本文重点介绍padding mask。 padding mask​ 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。 123456789101112131415def padding_mask(seq_k, seq_q): """ param seq_q:(batch_size,word_nums_q) param seq_k:(batch_size,word_nums_k) return padding_mask:(batch_size,word_nums_q,word_nums_k) """ # seq_k和seq_q 的形状都是 (batch_size,word_nums_k) len_q = seq_q.size(1) # 找到被pad填充为0的位置(batch_size,word_nums_k) pad_mask = seq_k.eq(0) #(batch_size,word_nums_q,word_nums_k) pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1) # shape [B, L_q, L_k] return pad_mask 3.Position-wise 前馈网络​ 这是一个全连接网络，包含两个线性变换和一个非线性函数(实际上就是 ReLU) ​ 这里实现上用到了两个一维卷积。 1234567891011121314151617181920class PositionalWiseFeedForward(nn.Module): &quot;&quot;&quot; 前向编码，使用两层一维卷积层实现 &quot;&quot;&quot; def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0): super(PositionalWiseFeedForward, self).__init__() self.w1 = nn.Conv1d(model_dim, ffn_dim, 1) self.w2 = nn.Conv1d(ffn_dim, model_dim, 1) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, x): output = x.transpose(1, 2) output = self.w2(F.relu(self.w1(output))) output = self.dropout(output.transpose(1, 2)) # add residual and norm layer output = self.layer_norm(x + output) return output]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试总结-Python和C语言中的一些不同]]></title>
    <url>%2F2019%2F02%2F27%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93-Python%E5%92%8CC%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%8D%E5%90%8C%2F</url>
    <content type="text"><![CDATA[###1.python在除法和C语言中的一点区别 ​ 在Python3中，除法有 “/” 以及 “//” 两种，这两个有着明显的区别，具体区别看代码： 12print(12//10)print(12/10) 这两行代码的输出如下： 1211.2 当被除数是负数的时候，又是另一种情况： 12345678print(-12/10) #不补整print(int(-12/10)) #向正方向进行补整print(-13//10) #向负方向进行补整output: -1.2 -1 -2 ​ 因此，综合前面的正负两种情况，我们可以看出当我们想要达到和C++同样的向上取整，只能使用int(a/b)方式。 2.python在求余时和C的一点区别​ 对于正数求余运算，python和C++完全相同，但是对于负数求余运算，python和C++存在着较大的差别，下面我们通过例子来说明二者的差别。 123456789#C++count&gt;&gt;-123%10;output: -3#pythonprint(-123%10)output: -7 #这里是向下取10的余数 ​ 为了实现和C++相同效果的取余运算，我们只能采用如下方式进行取余运算 1234if a&gt;=0 print(a%10)else: print(a%-10)]]></content>
      <tags>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——排序算法总结]]></title>
    <url>%2F2019%2F02%2F10%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[​ 机试中，排序算法是主要面临的一类算法，很久都没有接触机试的题了，解决的时候感觉有点思路不是很清楚了，因此写了这一片博客，来整理下常见的排序算法以及各种常见算法的效率稳定性等特点。 在机试中常用的排序算法主要包含下面几种： ​ 1.插入排序 ​ 2.选择排序 ​ 3.快速排序(最常用的排序) ​ 4.冒泡排序 ​ 5.归并排序 ​ 6.桶排序 下面我将具体介绍各种排序算法的一些特点： 排序算法 平均时间复杂度 最坏时间复杂度 空间复杂度 是否稳定 冒泡排序 O（n2） O（n2） O（1） 稳定 选择排序 O（n2） O（n2） O（1） 不稳定 直接插入排序 O（n2） O（n2） O（1） 稳定 希尔排序 O（n2） O(O3/2) 归并排序 O(nlogn) O(nlogn) O（n） 稳定 快速排序 O(nlogn) O(n2) O（logn） 不稳定 堆排序 O(nlogn) O(nlogn) O(1) 不稳定 时间复杂度辅助记忆： 冒泡、选择、直接 排序需要两个for循环，每次只关注一个元素，平均时间复杂度为O（n2）（一遍找元素O(n)，一遍找位置O(n)） 快速、归并、希尔、堆基于二分思想，log以2为底，平均时间复杂度为O(nlogn)（一遍找元素O(n)，一遍找位置O(logn)） 1.插入排序​ 每次从头到尾选择一个元素，并且将这个元素和整个数组中的所有已经排序的元素进行比较，然后插入到合适的位置。 ​ 注意：插入排序的核心点就是两两比较时从后向前进行比较，如果比插入值大，那么将其向后移动，直到找到比插入值小的。 12345678910def insertion_sort(arr): length = len(arr) for i in range(1,length): #从第一个元素开始依次进行排序 tmp = arr[i] j = i while arr[j-1]&gt;tmp and j&gt;0: #从当前元素从后向前向前开始遍历，寻找第一个比当前元素更小的元素 arr[j] = arr[j-1] #再找比当前小的元素位置的同时，只要扫描到的位置比当前元素大，那么将该元素后移一维 j -= 1 arr[j] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 为什么插入排序是稳定的排序算法？ ​ 当前从头到尾选择元素进行排序时，当选择到第i个元素时，前i-1个元素已经排好了续，取出第i个元素，从i-1开始向前开始比较，如果小于，则将该位置元素向后移动，继续先前的比较，如果不小于，那么将第i个元素放在当前比较的元素之后。 2.选择排序​ 选择排序主要采用了从头到尾依次确定各个位置的方式来进行排序，首先遍历一次整个数组，如果遇到比第一个元素小的元素那么交换位置，一次遍历完成那么第一个位置就已经是整个数组中最小的元素了，经过n次遍历，确定全部位置的元素。 123456789def selection_sort(arr): length = len(arr) for i in range(length): for j in range(i,length): if arr[i]&gt;arr[j]: tmp = arr[i] arr[i] = arr[j] arr[j] = tmp return arr 稳定性：不稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 3.冒泡排序​ 冒泡排序额是实现是不停地进行两两比较，将较大的元素换到右侧，然后继续进行两两比较，直到比较完全全部元素，每进行完一轮两两比较，确定一个元素的位置。例如：第一轮两两比较确定最大的值，第二轮比较确定次大元素。 1234567891011def bubble_sort(arr): length = len(arr) for i in range(0,length): for j in range(1,length-i): if arr[j]&lt;arr[j-1]: tmp = arr[j] arr[j] = arr[j-1] arr[j-1] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 冒泡排序在原始冒泡排序算法的基础上还能做哪些优化？ ​ 1.设置是否已经排好序的flag。如果在某一轮的便利中没有出现任何的交换发生，这说明已经都排好序,那么直接将flag置True，每轮结束时检测flag，如果为True则直接返回 ​ 2.某一轮的结束为止为j，但这一轮最后一次交换发生在lastSwap位置，那么说明lastSwap到j之间已经排好序，下次遍历的结束点就不需要再到j—而是直接到lastSwap即可。 4.希尔排序​ 希尔排序是一种插入排序的改良算法，简单的插入排序不管元素怎么样，都从头到尾一步一步的进行元素比较，如果遇到逆序序列如：[5,4,3,2,1,0]数组末端的0要回到原始位置需要n-1次的比较和移动。而希尔排序使用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后在各个组内进行插入排序，随后逐步缩小增量，继续按照组进行排序，直至增量为1。 ​ 希尔排序通过这种策略使的整个数组在初始阶段宏观上基本有序，小的基本在前，大的基本在后，然后缩小增量相当于进行微调，不会过多的设计元素移动。 基本思想：把记录按照下标的一定增量进行分组，对每组使用直接插入排序算法进行排序；随着增量逐渐减少，魅族包含的元素个数越来越多，当增量减至1时，整个文件被分成一组，算法终止。 稳定性：不稳定 平均时间复杂度：O($$n^2$$) 最坏时间复杂度 : O($$n^\frac{3}{2}$$) 空间复杂度:O( $$n^2$$ ) 5.快速排序​ 快速排序的的主要思想是先找到一个任意一个元素作为基准元素pivot（一般都采用第一个元素作为基准），然后从右向左搜索，如果发现比pivot小，那么和pivot交换,然后从右向左进行搜索，如果发现比pviot大，那么进行交换，遍历一轮后pivot左边的元素都比它小，右边的元素都比他大，此时pivot的位置就是排好序后他也应该在的位置。然后继续用递归算法分别处理pivot左边的元素和右边的元素。 对于大的乱序数据快速排序被认为是最快速的排序方式123456789101112131415161718192021222324252627282930313233#方式一：递归def quick_sort(arr,l,r): if(l&lt;r): q = mpartition(arr,l,r) quick_sort(arr,l,q-1) #前面经过一次mpartion后q位置已经排好序，因此递归时两部分跳过q位置 quick_sort(arr,q+1,r) return arr def mpartition(arr,l,r): """ 递归子函数，povit放到指定位置 return l:最终标志元素被放置的位置，本轮确定了的元素位置 """ poviot = arr[l] while l&lt;r: while l&lt;r and arr[r]&gt;=poviot: r -= 1 if l&lt;r: arr[l] = arr[r] l += 1 while l&lt;r and arr[l]&lt;poviot: l += 1 if l&lt;r: arr[r] = arr[l] r -= 1 arr[l] = poviot return l 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#方式二：非递归，利用栈def partition(nums,low,high): #确定nums数组中指定部分low元素的位置，左边都比它小，右边都比他大 pivot = nums[low] high_flag = True #这里之所以设置这两个flag是为了确保交叉进行，否则可能会出现最大索引值处没有值或者最大索引值处一直付给各个low low_flag = False while low&lt;high and low&lt;len(nums) and high&lt;len(nums): if high_flag: if nums[high]&lt;pivot: nums[low]=nums[high] high_flag = False low_flag = True else: high -= 1 if low_flag: if nums[low]&gt;pivot: nums[high] = nums[low] low_flag = False high_flag = True else: low += 1 nums[low] = pivot return low def quick_sort(nums): low = 0 high = len(nums)-1 stack = [] #存储每次遍历起始索引和结束索引 if low&lt;high: #先手动将找到第一个节点的最终位置，将原数组分为左右两个数组，分别左右索引入栈 mid = partition(nums,low,high) if low&lt;mid-1: stack.append(low) stack.append(mid-1) if high&gt;mid+1: stack.append(mid+1) stack.append(high) #取出之前入栈的一个数组，来进行确定最终位置，分为左右两个子数组，分别左右索引入栈的操作，重复直到所有元素都已经排好序 while stack: #这里写的是属于右半部都排好后左半部 r = stack.pop() l = stack.pop() mid = partition(nums,l,r) if l&lt;mid-1: stack.append(l) stack.append(mid-1) if r&gt;mid+1: stack.append(mid+1) stack.append(r) return nums 稳定性：不稳定（排序过程中不停地交换元素位置造成了排序算法不稳定） 时间复杂度： ​ 平均时间O(nlogn) ​ 最坏情况：O(n^2) 空间复杂度：O(nlogn) 6.归并排序​ 该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 ​ 每次合并操作的平均时间复杂度为O(n)，而完全二叉树的深度为|log2n|。总的平均时间复杂度为O(nlogn)。而且，归并排序的最好，最坏，平均时间复杂度均为O(nlogn)。 1234567891011121314151617181920212223def MergeSort(lists): if len(lists) &lt;= 1: return lists num = int(len(lists) / 2) left = MergeSort(lists[:num]) right = MergeSort(lists[num:]) return Merge(left, right)def Merge(left, right): r, l = 0, 0 result = [] while l &lt; len(left) and r &lt; len(right): if left[l] &lt;= right[r]: result.append(left[l]) l += 1 else: result.append(right[r]) r += 1 result += list(left[l:]) result += list(right[r:]) return result 7.堆排序​ 见堆排序 7.桶排序]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——Attention相关]]></title>
    <url>%2F2019%2F01%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Attention%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[1.为什么要使用Attention机制？​ Attention机制最初起源于seq2seq中，经典的encoder-decoder做机器翻译时，通常是是使用两个RNN网络，一个用来将待翻译语句进行编码输出一个vector，另一个RNN对上一个RNN网络的输出进行解码，也就是翻译的过程。但是经典的encoder-decoder模式最大的缺点在于：不管输入多么长的语句，最后输出的也只是最后一个vector，这个向量能否有效的表达该语句非常值得怀疑，而Attention机制正是利用了RNN整个过程中的各个输出来综合进行编码 原始序列模型的不足： ​ 1.从编码器到解码器的语境矩阵式大小是固定的，这是个瓶颈问题 ​ 2.难以对长的序列编码，并且难以回忆长期依赖 2.Attention原理1.首先在RNN的过程中保存每个RNN单元的隐藏状态(h1….hn) 2.对于decoder的每一个时刻t，因为此时有decoder的输入和上一时刻的输出，所以我们可以的当前步的隐藏状态St 3.在每个t时刻用St和hi进行点积得到attention score 4.利用softmax函数将attention score转化为概率分布 ​ 利用下面的公式进行概率分布的计算： 5.利用刚才的计算额Attention值对encoder的hi进行加权求和，得到decoder t时刻的注意力向量（也叫上下文向量） ​ 6.最后将注意力向量和decoder t时刻的隐藏状态st并联起来做后续步骤（例如全连接进行分类） 3.Attention计算方式​ 前面一节中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。在各个attention中，attention的计算方式主要有加法attention和乘法attention两种。 3.1 加法attention​ 在加法attention中我们不在使用st和hi的点乘，而是使用如下计算: ​ 其中,va和Wa都是可以训练的参数。使用这种方式产生的数在送往softmax来进行概率分布计算 3.2 乘法attention​ 在乘法attention中使用h和s做点乘运算: ​ 显然乘法attention的参数更少，计算效率更高。 4.self-attention​ 思想：在没有任何额外信息情况下，句子使用self-attention机制来处理自己，提取关键信息 在attention机制中经常出现的一种叫法： ​ query：在一个时刻不停地要被查询的那个向量（前面的decodert时刻的隐藏状态st）。 ​ key: 要去查询query计算个query相似关度的向量（前面的encoder在各个时刻的隐藏状态hi） ​ value: 和softmax得到的概率分布相乘得到最终attention上下文向量的向量(前面的encoder在各个时刻的隐藏状态hi) 这里我们可以明显知道，任意attention中key和value是相同的 ​ attention就是key、value、和query都来自同一输入的(也是相同的)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python进阶-面向对象编程]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.__slots__ ​ 用于指定class 实例能够指定的属性 注意：__slots__只对当前类起作用，对其子类无效 12345678910import tracebackclass Myclass(object): __slots__ = ["name","set_name] s = MyClass()s.name = "john" #这里可以进行正常的赋值，因为包含在__slots__中try: s.age = 2 #这里不能进行正常赋值except AttributeError: traceback.print_exc() Output: 2.@property属性 ​ @property 可以实现比较方便的属性set、get设置 1.使用@property相当于讲将一个函数变为get某个属性值2.@属性名称.setter可以实现设置一个属性的set条件 ​ 使用上面的两种修饰符，可以实现 ​ 1.对写入属性的限制，只有符合规范的才允许写入 ​ 2.设置只读属性，只能够读取，不能写入，只能从其他属性处计算出 下面的就是对score属性的写操作进行了一些限制，将double_score属性设置为只读属性 123456789101112131415161718192021222324252627282930313233343536373839404142class MyClass(object): @property def score(self): return self._score @score.setter def score(self,value): #不是int类型时引发异常 if not isinstance(value,int): raise ValueError("not int") #raise的作用是显示的引发异常 #超出范围时引发异常 elif (value&lt;0) or (value&gt;100): raise ValueError("score must in 0 to 100") self._score = value @property def double_score(self): return self._score*2 s = MyClass()s.score = 3print(s.score)try: s.score = 2300except ValueError: traceback.print_exc() try: s.score = "dfsd"except ValueError: traceback.print_exc() print(s.double_score)try: s.double_score = 2except Exception: traceback.print_exc() 描述器，主要是用来读写删除类的行为 函数可以直接使用__name__属性来获取函数名称 1234567def now(): print("2012")print(now.__name__)output: "now" ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[python进阶-生成器和迭代器]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E8%BF%AD%E4%BB%A3%E5%99%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pythonic程序规范化]]></title>
    <url>%2F2019%2F01%2F07%2Fpythonic%E7%A8%8B%E5%BA%8F%E8%A7%84%E8%8C%83%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.常量名称全部大写 2.代码长度一行不能超过80字符 3.类名使用驼峰式命名，一般不超过3 4.函数名称]]></content>
  </entry>
  <entry>
    <title><![CDATA[python2和python3的不同点]]></title>
    <url>%2F2019%2F01%2F07%2Fpython2%E5%92%8Cpython3%E7%9A%84%E4%B8%8D%E5%90%8C%E7%82%B9%2F</url>
    <content type="text"><![CDATA[​ 因为系统移植过程中一直出现python3程序向python2转化的问题，因此这里记录下我在程序移植过程中遇到过的坑。 1.python2和python3的url编码解码函数接口 2.python2和python3向文件中写入中文时指定编码方式 ​ 对于python3来说，要在写入文件时指定编码方式是十分简单的，只需要下面的方式即可： 12with open(filename,'a',encoding='utf-8') as f: f.write("中文") ​ 但对于python2，要在写入文件时,手动添加utf-8文件的前缀 123456import sysreload(sys)sys.setdefaultencoding('utf-8')with open(r'd:\sss.txt','w') as f: f.write(unicode("\xEF\xBB\xBF", "utf-8"))#函数将\xEF\xBB\xBF写到文件开头，指示文件为UTF-8编码。 f.write(u'中文') 3.python2和python3外置函数区别 在python3中外置函数文件可以直接进行调用，如在下面的文件结构 12|--main.py tools—— 在python2中外置函数文件目录下必须要有init.py 空文件，否则无法进行加载 12 4.python2和python3文件中中文问题 ​ 在python3中，输出和备注等一切位置都可以直接使用中文，不需要任何额外的代码，在python2中，必须要在包含中文的python文件中加入 1#coding=utf-8 ​ 才能出现中文，否则报错。]]></content>
      <categories>
        <category>python 进阶操作</category>
      </categories>
      <tags>
        <tag>python进阶操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch建模基本操作]]></title>
    <url>%2F2019%2F01%2F02%2Fpytorch%E5%BB%BA%E6%A8%A1%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.查看模型结构 12]]></content>
  </entry>
  <entry>
    <title><![CDATA[python不显示警告信息设置]]></title>
    <url>%2F2019%2F01%2F02%2Fpython%E4%B8%8D%E6%98%BE%E7%A4%BA%E8%AD%A6%E5%91%8A%E4%BF%A1%E6%81%AF%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[再使用python处理多线程或者循环次数较多时，常常会因为系统爆出一些警告信息而影响结果的查看，比如下面的警告： 十分影响美观，造成结果混乱，很难找到有效的信息，下面我们使用python自带的warning设置，设置过滤warn级别的告警 12import warningswarnnings.filterwarnings("ignore") 结果变为：]]></content>
      <tags>
        <tag>python进阶操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2018%2F12%2F30%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ python多进程之前一直在在写一些小程序，这次正好需要写一个比较正式的多进行处理，正好系统的总结下python多进行的一些核心知识。 ​ 首先python中常用的提高效率的方式主要主要包括多线程、多进程两种，但是在python中的多线程并不是正正的多线程，只有在网络请求密集型的程序中才能有效的提升效率，在计算密集型、IO密集型都不是很work甚至会造成效率下降，因此提升效率的方式就主要以多进程为主。 ​ python中多进程需要使用python自带包multiprocessing，multiprocessing支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件 1from multiprocessing import Lock,Value,Queue,Pool 创建进程类1.单个进程类的创建​ 1.创建单进程 ​ 12 ####使用进程池创建多进程 ​ Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 ​ 常用方法： ​ 1.apply ​ 用于传递不定参数，同python中的apply函数一致，主进程会被阻塞直到函数执行结束（不建议使用，并且3.x以后不在出现）。 ​ 2.apply_async ​ 和apply类似，非阻塞版本，支持结果返回后进行回调 ​ 3.map ​ 函数原型:map(func, iterable[, chunksize=None]) ​ Pool中的map和python内置的map用法基本上一致，会阻塞直到返回结果 ​ 4.map_async ​ 函数原型：map_async(func, iterable[, chunksize[, callback]]) ​ 和map用法一致，但是它是非阻塞的 ​ 5.close ​ 关闭进程池，使其不再接受新的任务 ​ 6.terminal ​ 结束工作进程，不再处理未处理的任务 ​ 7.join ​ 主进程阻塞等待子进程退出，join方法要在close或terminal方法后面使用 ​ 1234from mutiprocess import Poolprocess_nums = 20pool = Pool(process_nums) 使用Lock来避免冲突​ lock主要用于多个进程之间共享资源时，避免资源访问冲突，主要包括下面两个操作： ​ 1.look.acquire() 获得锁 ​ 2.lock.release() 释放锁 ​ 下面是不加锁时的程序： 1234567891011121314151617import multiprocessingimport timedef add(number,value,lock): print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 12345678910111213main endinit add1 number = 0init add3 number = 0add1 number = 1add3 number = 3add1 number = 2add3 number = 6add1 number = 3add3 number = 9add1 number = 4add3 number = 12add1 number = 5add3 number = 15 ​ 两个进程交替的来对number进行加操作，下面是加锁后的程序： 1234567891011121314151617181920212223import multiprocessingimport timedef add(number,value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) except Exception as e: raise e finally: lock.release()if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 1234567891011121314main endinit add1 number = 0 #add1优先抢到锁，优先执行add1 number = 1add1 number = 2add1 number = 3add1 number = 4add1 number = 5init add3 number = 0 #add3被阻塞，等待add1执行完成，释放锁后执行add3add3 number = 3add3 number = 6add3 number = 9add3 number = 12add3 number = 15#注意观察上面add3部分，虽然在add1部分已经将number加到了5，但是由于number变量只是普通变量，不能在各个进程之间进行共享，因此add3开始还要从0开始加 ####使用Value和Array来进行内存之中的共享通信 ​ 一般的变量在进程之间是没法进行通讯的，multiprocessing 给我们提供了 Value 和 Array 模块，他们可以在不通的进程中共同使用。 ​ 1.Value多进程共享变量 ​ 将前面加锁的程序中的变量使用multiprocessing提供的共享变量来进行 123456789101112131415161718192021222324import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release() if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 输出结果为： 123456789101112131415161718192021222324#add3开始时是在add1的基础上来进行加的main endinit add1 number = 0***************add1 has added***********add1 number = 1***************add1 has added***********add1 number = 2***************add1 has added***********add1 number = 3***************add1 has added***********add1 number = 4***************add1 has added***********add1 number = 5init add3 number = 5***************add3 has added***********add3 number = 8***************add3 has added***********add3 number = 11***************add3 has added***********add3 number = 14***************add3 has added***********add3 number = 17***************add3 has added***********add3 number = 20 ​ 2.Array实现多进程共享内存变量 12345678910111213141516171819202122232425262728293031323334import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release()def change(arr): for i in range(len(arr)): arr[i] = -arr[i] if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) arr = multiprocessing.Array('i', range(10)) print (arr[:]) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p3 = multiprocessing.Process(target=change,args=(arr,)) p1.start() p2.start() p3.start() p3.join() print (arr[:]) print ("main end") ​ 输出结果为： 12345678910111213141516171819202122232425[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]init add3 number = 0***************add3 has added***********[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]main endadd3 number = 3***************add3 has added***********add3 number = 6***************add3 has added***********add3 number = 9***************add3 has added***********add3 number = 12***************add3 has added***********add3 number = 15init add1 number = 15***************add1 has added***********add1 number = 16***************add1 has added***********add1 number = 17***************add1 has added***********add1 number = 18***************add1 has added***********add1 number = 19***************add1 has added***********add1 number = 20 ####使用Queue来实现多进程之间的数据传递 ​ Queue是多进程安全队列，可以使用Queue来实现进程之间的数据传递，使用的方式： 1.put 将数据插入到队列中 ​ 包括两个可选参数：blocked和timeout ​ (1)如果blocked为True（默认为True）,并且timeout为正值，该方法会阻塞队列指定时间，直到队列有剩余，如果超时，会抛出Queue.Full ​ (2)如果blocked为False，且队列已满，那么立刻抛出Queue.Full异常 2.get 从队列中读取并删除一个元素 ​ 包括两个可选参数:block和timeout ​ （1）blocked为True，并且timeout为正值，那么在等待时间结束后还没有取到元素，那么会抛出Queue.Empty异常 ​ （2）blocked为False，那么对列为空时直接抛出Queue.Empty异常]]></content>
      <categories>
        <category>python进阶操作</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop使用]]></title>
    <url>%2F2018%2F12%2F25%2Fhadoop%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.查看hadoop某个目录下的文件 1sudo hadoop fs -ls path 2.从hdfs上下拉文件到本地 1sudo hdfs fs -get file 3.获取部署在docker中的hadoop的挂载信息等元数据 1sudo docker inspect hdp-server]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题之Cpp基础知识回顾]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%88%B7%E9%A2%98%E4%B9%8BCpp%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[由于C++已经多 1.动态数组的声明]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题心得]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%88%B7%E9%A2%98%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[1.在只需要考虑是不是存在元素的个数，只可考虑是否存在的情况下，先将list装换成set可以非常有效的提升计算效率 2.对Int类型数值的范围要保持敏感​ Int类型数值范围为 ​ Max 0x7fffffff 2^31-1 2147483647 ​ Min 0x80000000 2^31 -2147483648 ​ 注意：负数的范围会比正数的范围大一，这按需要特别注意 3.常见数学问题要考虑的情况​ 1.是否有负数 ​ 2.是否有小数 ​ 3.是否考虑错误输入？如何进行处理 ​ 4.数据范围极端值 ​ 5.0或空如何进行处理 ​ .]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题记录]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[#####1.除自身以外的数组乘积 给定长度为 n 的整数数组 nums，其中 n &gt; 1，返回输出数组 output ，其中 output[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。 示例: 12输入: [1,2,3,4]输出: [24,12,8,6] 说明：不能使用除法，在O(n)时间复杂度内解决此问题 解决思路： ​ 可以使用先从左到右进行遍历，记录每个位置左面的元素相乘获得的值，存储在output的对应位置，再从右到左进行遍历，记录每个位置右侧的元素乘积，再和output中已经存储的该位置左侧的元素乘积相乘，就可以得到最终结果，时间复杂度为O(n) 12345678910111213141516171819202122232425class Solution: def productExceptSelf(self, nums): """ 完美解 :type nums: List[int] :rtype: List[int] """ left = 1 right = 1 len_nums = len(nums) output = [0]*len_nums #从左到右进行一次遍历，在output中对应位置记录该值左面的元素乘积 for i in range(0,len_nums): output[i] = left left = left*nums[i] #从右到左进行一次遍历，记录每个值右面元素的乘积，和output中已经进行存储的左面乘积相乘，得到各个位置最终的结果 for j in range(len_nums-1,-1,-1): output[j] *= right right *= nums[j] return output 2.缺失数字 给定一个包含 0, 1, 2, ..., n 中 n 个数的序列，找出 0 .. n 中没有出现在序列中的那个数。 示例 1: 12输入: [3,0,1]输出: 2 示例 2: 12输入: [9,6,4,2,3,5,7,0,1]输出: 8 说明:你的算法应具有线性时间复杂度。你能否仅使用额外常数空间来实现? 思路一：最常见的思路应该是先排序，然后顺序遍历，对不上则为缺失位置 1234567891011class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ for key,value in emumerate(nums): if key!=value: return key else: return key+1 思路二：另一种比较巧妙地思路就是直接利用数学的方法来解决这个问题，仔细研究题目，我们可以发现题目中所给的nums数组内所有元素的加和可以看做等差数列的加和减去缺失数，因此我们可以直接计算等差数列的加和(n*(n-1))/2，然后减去nums数组的加和，二者相减即为缺失的数. 1234567class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ return (int(len(nums)*(len(nums)-1)/2)- sum(nums)) 爬楼梯问题（动态规划）假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 12345输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶 示例 2： 123456输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶 解题思路：首先经过题目分析我们最自然的可以想到，要想问到第n层楼梯的走法，那么一定为到第n-1和第n-2层楼梯走法之和，因此我们可以清楚地可以看出这是一道递归问题。即n(i) = n(i-1)+n(i-2) 1234567891011121314class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; def f(n): if n==0|n==1: return 1 else: return f(n-1)+f(n-2) if n&gt;=2: return f(n) return 1 转换成非递归问题（其实本质就是讲递归问题由系统储存的信息改为程序储存，从而改编程序的运行方式，提高程序的运行效率） 12345678910class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; way = [0,1,2] for i in range(3,n+1): way.append(way[i-1]+way[i-2]) return way[n]]]></content>
      <categories>
        <category>机试</category>
      </categories>
      <tags>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化部署pyspark程序记录]]></title>
    <url>%2F2018%2F12%2F07%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2pyspark%E7%A8%8B%E5%BA%8F%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[​ 项目要求需要在pyspark的集群中将一部分程序做集群的自动化部署，本文记录了程序部署过程中，使用到的一些技术以及遇到的一些问题。 ####1.SparkSession创建时设置不生效 ​ 首先，要进行程序的自动化部署首先要将程序封装成python文件，在这个过程中可能会出现sparkSession谁知不能生效的问题，不论对SparkSession进行什么设置，都不会生生效 这种问题是由于SparkSession的创建过程不能写在主程序中，必须要写在所有函数的外层，并且进行的在文件的初始部分穿创建 2.python 文件传入获取参数​ python文件也可以和shell脚本一样进行运行时传入参数，这里主要使用的的是python自带的sys和getopt包 1234567891011要接受参数的python文件：import sysimport getoptopts,args = getopt.getopt(sys.argv[1:],"d:",["d:"])for opt,arg in opts: if opt in ("-d","--d"): input_file = arg#后续可以直接使用input——file获取的变量名进行操作 ####3.将python文件执行封装到shell脚本中 ​ 这里之所以将python文件进行封装主要是为了方便移植，其实也可以直接设置将python脚本文件执行设置成定时任务，这里是一波瞎操作。主要为了练习和方便移植 123456789101112#首先在这个shell重要实现获取当前日期或前n天的日期date = `date -d "1 days ago"+%Y-%m-%d`#然后在将date作为参数后台执行这个程序并且生成日志python ***.py -d date &gt; /path/$&#123;date&#125;.log 2&gt;&amp;1 &amp;#=====================注意==============================#上面直接使用python执行时可能会出现系统中存在多个python导致部署时使用的python和之前测试使用的python不是一个python环境导致的，那么如何确定测试时使用的python环境呢？#要解决上述问题可以先从新进入到测试用的python环境，然后进行下面操作import sysprint(sys.execyutable)#然后将python目录改为上面的python目录]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习训练基本经验]]></title>
    <url>%2F2018%2F11%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E5%9F%BA%E6%9C%AC%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[1.在各个隐藏层和激活函数之间加入Batch Normalization层可以大大缩短训练时间，而且还存在隐藏效果，比如出现还可以改善效果。 调用： ​ Normalization(num_features) 参数设置： ​ CNN后接Batch Normalization: nums_feeatures为CNN感受野个数(即输出深度) ​ 全连接层后接Batch Normalization：num_features为输出的特征个数 2.Batch Normalization和Dropout层不要一起使用，因为BN以及具备了dropout的效果，一起使用不但起不到效果，而且会产生副作用 常见副作用: ​ 1.只能使用特别小的速率进行训练，使用较大的速率进行训练时，出现梯度消失，无法进行下降 ​ 2.最终与训练集拟合程度不高，例如与训练集的拟合程度只能达到90% 若一定要将dropout和BN一起使用，那么可以采用下面方式： ​ 1.将dropout放在BN后面进行使用 ​ 2.修改Dropout公式(如高斯Dropout)，使其对对方差不那么敏感 总体思路:降低方差偏移 3.深度学习不收敛问题 ​ 1.最常见的原因可能是由于学习速率设置的过大，这种情况一般先准确率不断上升，然后就开始震荡 ​ 2.当训练样本较小，而向量空间较大时，也可能会产生不收敛问题，这种情况一般从一开始就开始震荡，机会没有准确率上升的过程 ​ 3.训练网络问题。当面对的问题比较复杂，而使用的网络较浅时，可能会产生无法收敛问题 ​ 4.数据没有进行归一化。数据输入模型之前如果没有进行归一化，很有可能会产生收敛慢或者无法进行收敛的问题 注意：收敛与否主要是看损失函数是否还在下降，而不是准确率是否还在上升，存在很多情况损失函数在迭代过程中还是在不断地下降，但是准确率基本上处于停滞状态，这种情况也是一种未完全拟合的表现，经过一段时间损失函数的下降后准确率还可能会迎来较大的提升]]></content>
  </entry>
  <entry>
    <title><![CDATA[pyspark-spark.ml.linalg包]]></title>
    <url>%2F2018%2F11%2F21%2Fpyspark-spark-ml-linalg%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pyspark 的pyspark.ml.linalg包主要提供了向量相关(矩阵部分不是很常用因此本文不提)的定义以及计算操作 主体包括： ​ 1.Vector ​ 2.DenseVector ​ 3.SparseVector 1.Vector​ 是下面所有向量类型的父类。我们使用numpy数组进行存储，计算将交给底层的numpy数组。 主要方法： ​ toArray() 将向量转化为numpy的array ###2.DenseVector ​ 创建时，可以使用list、numpy array、等多种方式进行创建 常用方法： ​ dot() 计算点乘,支持密集向量和numpy array、list、SparseVector、SciPy Sparse相乘 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 12345678910111213141516171819202122232425262728293031323334#密集矩阵的创建v = Vectors.dense([1.0, 2.0])u = DenseVector([3.0, 4.0])#密集矩阵计算v + uoutput: DenseVector([4.0, 6.0]) #点乘v.dot(v) #密集向量和密集向量之间进行点乘output: 5.0v.dot(numpy.array([1,2])) #使用密集向量直接和numpy array进行计算output: 5.0 #计算非零元素个数DenseVector([1,2,0]).numNonzeros()#计算两个元素之间的平方距离a = DenseVector([0,0])b = DenseVector([3,4])a.squared_distance(b)output: 25.0 #密集矩阵转numpy arrayv = v.toArray()voutput: array([1., 2.]) 3.SparseVector​ 简单的系数向量类，用于将数据输送给ml模型。 ​ Sparkvector和一般的scipy稀疏向量不太一样，其表示方式为，（数据总维数，该数据第n维存在值列表，各个位置对应的值列表） 常用方法： ​ dot() SparseVector的点乘不仅可以在SparseVector之间还可以与numpy array相乘 ​ indices 有值的条目对应的索引列表，返回值为numpy array ​ size 向量维度 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 注：加粗部分为SparseVector特有的 1234567891011121314151617181920#创建稀疏向量a = SparseVector(4, [1, 3], [3.0, 4.0])a.toArray()output: array([0., 3., 0., 4.])#计算点乘a.dot(array([1., 2., 3., 4.]))output: 22.0 #获得存值得对应的索引列表a.indicesoutput: array([1, 3], dtype=int32)#获取向量维度a.sizeoutput: 4]]></content>
      <tags>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark-向量化技术]]></title>
    <url>%2F2018%2F11%2F20%2Fpyspark-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[​ 在pyspark中文本的向量化技术主要在包pyspark.ml.feature中，主要包括以下几种： 1.Ngram 2.tf-idf 3.Word2Vec 1.Ngram​ 2.tf-idf​ 在pyspark中tf和idf是分开的两个步骤 ​ （1）tf ​ 整个tf的过程就是一个将将各个文本进行计算频数统计的过程，使用前要先使用也定的算法来对语句进行分词，然后指定统计特征的数量再进行tf统计 ​ 常用参数： ​ 1.numsFeatures 统计的特征数量，这个值一般通过ParamGridBuilder尝试得出最合适的值 ​ 2.inputCol 输入列，输入类列为ArrayType的数据 ​ 3.outputCol 输出列 ,输出列为Vector类型的数据 123456789101112df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])hashingTF = HashingTF(numFeatures=10, inputCol="words", outputCol="tf")hashingTF.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+|words |tf |+-----------------------------------+--------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|+-----------------------------------+--------------------------------+ ​ 其中，10代表了特征数，[1,3]代表了this和is对应的哈希值，[2.0,1.0]代表了this和is出现的频数. ​ (2)idf 常用参数： ​ 1.minDocFreq 最少要出现的频数，如果超过minDocFreq个样本中出现了这个关键词，这个频数将不tf-idf特征，直接为0 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 123456789101112idf = IDF(inputCol="tf",outputCol="tf-idf")idf_model = idf.fit(df)idf_model.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+--------------------------------------------------------------+|words |tf |tf-idf |+-----------------------------------+--------------------------------+--------------------------------------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) |(10,[1,3],[0.0,0.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|(10,[1,2,3,7],[0.0,0.4054651081081644,0.0,0.4054651081081644])|+-----------------------------------+--------------------------------+--------------------------------------------------------------+ 3.CountVec​ CountVec是一种直接进行文本向量，直接词频统计的向量化方式，可以 常用参数包括： ​ minDF：要保证出现词的代表性。当minDF值大于1时，表示词汇表中出现的词最少要在minDf个文档中出现过，否则去除掉不进入词汇表；当minDF小于1，表示词汇表中出现的词最少要在包分之minDF*100个文档中出现才进入词汇表 ​ minTF：过滤文档中出现的过于罕见的词，因为这类词机乎不在什么文本中出现因此作为特征可区分的样本数量比较少。当minTF大于1时，表示这个词出现的频率必须高于这个才会进入词汇表；小于1时，表示这个大于一个分数时才进入词汇表 ​ binary: 是否只计算0/1,即是否出现该词。默认值为False。 ​ inputCol:输入列名，默认为None ​ outputCol:输出列名，默认为None 1234567891011121314151617181920212223242526df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])#使用Word2Vec进行词向量化countvec = CountVectorizer(inputCol='words',outputCol='countvec')countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[2.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+#使用CountVec的binary模式进行向量化，countvec = CountVectorizer(inputCol='words',outputCol='countvec',binary=True)countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[1.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+ ###4.Word2Vec ​ Word2Vec 是一种常见的文本向量化方式,使用神经网络讲一个词语和他前后的词语来进行表示这个这个词语，主要分为CBOW和Skip- ​ 特点：Word2Vec主要是结合了前后词生成各个词向量，具有一定的语义信息 在pyspark.ml.feature中存在Word2Vec和Word2VecModel两个对象，这两个对象之间存在什么区别和联系呢？ ​ Word2Vec是Word2Vec基本参数设置部分，Word2VecModel是训练好以后的Word2Vec，有些函数只有Word2VecModel训练好以后才能使用 常见参数： ​ 1.vectorSize 生成的词向量大小 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 ​ 4.windowSize 输出的词向量和该词前后多少个词与有关 ​ 5.maxSentenceLength 输入句子的最大长度，超过改长度直接进行进行截断 ​ 6.numPartitions 分区数，影响训练速度 常用函数： ​ 这里的常见函数要对Word2VecModel才能使用 ​ getVectors() 获得词和词向量的对应关系,返回值为dataframe ​ transform() 传入一个dataframe，将一个词列转换为词向量 ​ save() 保存模型 使用要先使用训练集对其进行训练： 123456789101112输入数据： 已经使用一定的分词方式已经进行分词后的ArrayType数组输出： 当前句子各个词进行word2vec编码后的均值，维度为vectorSize word2vec = Word2Vec(vectorSize=100,inputCol="word",outputCol="word_vector",windowSize=3,numPartitions=300)word2vec_model = word2vec.fit(data)#features将会在data的基础上多出一列word_vector，为vectorSize维数组features = word2vec.trandform(data)word2vec_model.save("./model/name.word2vec") Word2Vec如何查看是否已经训练的很好： ​ 1.选择两个在日常生活中已知词义相近的两个词A、B，再选一个与A词义不那么相近但也有一定相似度的词C ​ 2.计算A和B以及A和C的余弦距离 ​ 3.比较其大小，当满足AB距离小于AC时，重新选择三个词重复上过程多次都满足，那么认为模型已经训练完毕；若不满足上述过程，那么继续加入样本进行训练 当word2vec中为了表达两个比较相近的词的相似性可以怎么做？比如在当前word2vec下tea、cooffe之间的相似性非常高，接近于1 ​ 增加word2vec的向量维度。可能是在当前维度中向量维度过小，导致这两个词无法表达充分，因此我们可以增加向量维度，以期待在更高维的向量空间中，可以区分这个名词 过程中可能用的： 12345#获得某个词对应的词向量word2vec_model.getVectors().filter("word=='0eva'").collect()[0]['vector']#计算两个词向量之间距离平方a1.squared_distance(a2)]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux——恢复误删除文件]]></title>
    <url>%2F2018%2F11%2F16%2Flinux%E2%80%94%E2%80%94%E6%81%A2%E5%A4%8D%E8%AF%AF%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[​ ububtu默认情况下会存在一个默认回收站，当使用的文件被误删除需要找回时，可以进入到回收站找到该文件，将其恢复出来即可 回收站位置： ​ ~/.local/share/Trash 找回方式： ​ 进入该目录，直接将该目录中的文件考出即可]]></content>
  </entry>
  <entry>
    <title><![CDATA[pandas常用设置]]></title>
    <url>%2F2018%2F11%2F12%2Fpandas%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.]]></content>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——HMM]]></title>
    <url>%2F2018%2F11%2F07%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94HMM%2F</url>
    <content type="text"><![CDATA[马尔科夫模型假设： ​ 1.马尔科夫模型认为每一时刻的表现x有一个状态z与其对应 ​ 2.观测独立性假设：每个时刻的输出都只与当前的状态有关 ​ 3.贝叶斯公式P(o|λ) = P(λ|o)P(o) / P(λ) 马尔科夫模型的推导过程​ 对于马尔科夫模型，求解的最终目的是 1maxP(o1o2...on|λ1λ2...λn) ​ 由于p(o|λ)是个关于2n各变量的条件概率，并且n不固定，因此没办法进行精确计算。因此马尔科夫链模型采用了一种更加巧妙地方式来进行建模。 ​ 首先，根据贝叶斯公式可以得： 1P(o|λ) = P(λ|o)P(o) / P(λ) ​ λ为给定的输入，因此P(λ)为常数，因此可以忽略.因此 1P(o|λ) = P(λ|o)P(o) ​ 而根据观测独立性假设，可以得到 12P(o1o2...on|λ1λ2...λn) = P(o1|λ1)P(o2|λ2)...P(on|λn) P(o) = P(o1)P(o2|o1)P(o3|o1,o2)...P(on|on-1,on-2,...,o1) ​ 而由于其次马尔科夫假设，每个输出仅与上一个一个输出有关，那么 1P(o) = p(o1)P(o2|o1)....P(on|on-1) ​ 因此最终可得： 1P(o|λ) ~ p(o1)P(o1|o2)P(λ|o2)P(o2|o3)P(λ3|o3)...P(on|on-1)P(λ|on) ​ 其中，P(λk|ok)为发射概率，P(ok|ok-1)为转移概率。 维特比算法​ 维特比算法是隐马尔科夫模型最终求解当前表现链最可能对应的状态链使用的一种动态规划算法。主要思想是： HMM模型训练后的输出为：初始状态概率矩阵、状态转移矩阵、发射概率矩阵三个结果，用来后续进行预测 概率图模型大部分都是这样，输出为几个概率矩阵 而LR模型的输出就为系数矩阵了 我们可以看出来，LR在使用训练好的模型进行与测试效率较高，而HMM使用训练好的模型进行训练时效率较低]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习和深度学习在实践中的一些经验]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[####使用GBDT算法构造特征 ​ Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题。[1]GBDT思想对于发现多种有区分性的特征和组合特征具有天然优势，可以用来构造新的组合特征。 ​ 在这篇论文中提出可以使用GBDT各棵数输出节点的索引号来作为新的特征，对各个树渠道的索引号做one-hot编码，然后与原始的特征一起新的特征输入到模型中往往会起到不错的效果。 实践情况 ​ 1.本人使用这种方式在ctr预估中已经进行过实验，准确率提升2% ​ 2.美团在外卖预计送达时间预测中进行了实验，各个时段平均偏差减少了3% (1) 超参数选择 a. 首先为了节点分裂时质量和随机性，分裂时所使用的最大特征数目为√n。b. GBDT迭代次数（树的数量）。 树的数量决定了后续构造特征的规模，与学习速率相互对应。通常学习速率设置较小，但如果过小，会导致迭代次数大幅增加，使得新构造的特征规模过大。 通过GridSearch+CrossValidation可以寻找到最合适的迭代次数+学习速率的超参组合。c. GBDT树深度需要足够合理，通常在4~6较为合适。 虽然增加树的数量和深度都可以增加新构造的特征规模。但树深度过大，会造成模型过拟合以及导致新构造特征过于稀疏。 （2）训练方案 ​ 将训练数据随机抽样50%，一分为二。前50%用于训练GBDT模型，后50%的数据在通过GBDT输出样本在每棵树中输出的叶子节点索引位置，并记录存储，用于后续的新特征的构造和编码，以及后续模型的训练。如样本x通过GBDT输出后得到的形式如下：x → [25,20,22,….,30,28] ，列表中表示样本在GBDT每个树中输出的叶子节点索引位置。 ​ 由于样本经过GBDT输出后得到的x → [25,20,22,….,30,28] 是一组新特征，但由于这组新特征是叶子节点的ID，其值不能直接表达任何信息，故不能直接用于ETA场景的预估。为了解决上述的问题，避免训练过程中无用信息对模型产生的负面影响，需要通过独热码（OneHotEncoder）的编码方式对新特征进行处理，将新特征转化为可用的0-1的特征。 ​ 以图5中的第一棵树和第二棵树为例，第一棵树共有三个叶子节点，样本会在三个叶子节点的其中之一输出。所以样本在该棵树有会有可能输出三个不同分类的值，需要由3个bit值来表达样本在该树中输出的含义。图中样本在第一棵树的第一个叶子节点输出，独热码表示为{100}；而第二棵树有四个叶子节点，且样本在第三个叶子节点输出，则表示为{0010}。将样本在每棵树的独热码拼接起来，表示为{1000010}，即通过两棵CART树构造了7个特征，构造特征的规模与GBDT中CART树的叶子节点规模直接相关。 Wide&amp;Deep在推荐中应用 【参考文献】 He X, Pan J, Jin O, et al. Practical Lessons from Predicting Clicks on Ads at Facebook[C]. Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2014: 1-9.]]></content>
      <categories>
        <category>机器学习，深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web安全——CSRF和SSRF]]></title>
    <url>%2F2018%2F11%2F02%2Fweb%E5%AE%89%E5%85%A8%E2%80%94%E2%80%94CSRF%E5%92%8CSSRF%2F</url>
    <content type="text"><![CDATA[CSRF​ CSRF(Cross-site request Forgery，跨站请求伪造)通过伪装成受信任用户请求受信任的网站。 1注意:scrf漏洞并不需要获取用户的cookie等信息 目标：已经登陆了网站的用户 目的：以合法用户的身份来进行一些非法操作 需要条件： ​ 1.用户已经登陆了目标网站 ​ 2.目标用户访问了攻击者构造的url 攻击过程: ​ 1.找到存于登陆状态的存在csrf网站的合法用户，向其发送可以构造的恶意链接，诱使其点击 ​ 2.用户点击该链接，由该合法用户向服务器发出包含恶意链接里隐藏操作（如删除数据、转账等）的请求 ​ 3.服务器收到已经登录用户的请求，认为是合法用户的主动的操作行为，执行该操作 典型的csrf实例 ​ 当你使用网上银行进行转账时，首先需要登录网上银行，点击转账按钮后，会发出http://www.xxbank.com/pay.php?user=xx&amp;money=100请求，当存在攻击者想要对你进行csrf攻击时，他会向你发送一个邮件或者短信，其中包含可以构造的恶意链接 http://www.bank.com/pay,php?user=hack&amp;money=100,并且采用一定的伪装手段诱使你进行点击，当你点击后即向该hack转账100元。 流量中检测csrf的可行性 ​ 1.对于比较低级的csrf而言，可以直接通过检测请求的referer字段来进行确定是否为scrf。因为在正常scrf页面中应该是在主页等页面跳转得到，而csrf请求一般的referer是空白或者是其他网站，但是该方法可以被绕过。 ​ 2.完全的检测很难 csrf漏洞修复建议 ​ 1.验证请求的referer ​ 2.在请求中加入随机的token等攻击者不能伪造的信息 ####SSRF ​ SSRF(Server-Side Request Forgery，服务端请求伪造)是一种有由攻击者构造请求，服务器端发起请求的安全漏洞。 目标：外网无法访问的服务器系统 目的：获取内网主机或者服务器的信息、读取敏感文件等 形成原因：服务器端提供了从其他服务器获取数据的功能，但没有对目标地址做限制和过滤 攻击过程： ​ 1.用户发现存在ssrf漏洞的服务器a的页面访问的url，以及可使用SSRF攻击的参数 ​ 2.修改要请求参数要请求的文件，将其改成内网服务器b和文件，直接访问 ​ 3.服务器a接收到要访问的参数所包含的服务器b和文件名，去服务器b下载资源 ​ 3.对于服务器b，由于是服务器a发起的请求，直接将文件返回给服务器a ​ 4.服务器a将该文件或页面内容直接返回给用户 两种典型的ssrf攻击实例: ​ 本地存在ssrf漏洞的页面为：http://127.0.0.1/ssrf.php?url=http://127.0.0.1/2.php 原始页面的功能为通过GET方式获取url参数的值，然后显示在网页页面上。如果将url参数的值改为http://www.baidu.com ，这个页面则会出现百度页面内容。 ​ 因此利用这个漏洞，我们可以将url参数的值设置为内网址，这样可以做到获取内网信息的效果。 ​ 探测内网某个服务器是否开启 ​ 将url参数设置为url=”192.168.0.2:3306”时，可以获取大到该内网主机上是否存在mysql服务。 ​ 读取内网服务器文件 ​ 访问ssrf.php?url=file:///C:/Windows/win.ini 即可读取本地文件 流量中检测SSRF可行性分析： ​ 对于只能抓到外网向内网访问的流量的网口来说，从流量中检测SSRF只能从请求参数异常或返回包是否异常、是否包含敏感信息来进行检测。 SSRF漏洞修复建议: ​ 1.限制请求的端口只能是web端口，只允许访问http和https的请求 ​ 2.限制不能访问内网IP，以防止对内网进行攻击 ​ 3.屏蔽返回的信息详情]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>web安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP—关键词提取算法]]></title>
    <url>%2F2018%2F11%2F01%2FNLP%E2%80%94%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关键词提取算法 tf-idf(词频-逆文档频率)​ ​ 其中count(w)为关键词出现的次数，|Di|为文档中所有词的数量。 ​ ​ 其中，N为所有文档的总数，I(w,Di)表示文档Di是否包含该关键词，，包含则为1，不包含则为0，若词在所有文档中均未出现，则IDF公式中分母则为0，因此在分母上加1做平滑(smooth) ​ 最终关键词在文档中的tf-idf值： ​ tf-idf特点： ​ 1.一个词在一个文档中的频率越高，在其他文档中出现的次数越少，tf-idf值越大 ​ 2.tf-idf同时兼顾了词频和新鲜度，可以有效地过滤掉常见词 ​ TextRank​ TextRank算法借鉴于Google的PageRank算法，主要在考虑词的关键度主要考虑链接数量和链接质量（链接到的词的重要度）两个因素。 ​ TextRank算法应用到关键词抽取时连个关键点：1.词与词之间的关联没有权重（即不考虑词与词是否相似） 2.每个词并不是与文档中每个次都有链接关系而是只与一个特定窗口大小内词与才有关联关系。 TextRank特点： ​ 1.不需要使用语料库进行训练，由一篇文章就可以提取出关键词 ​ 2.由于TextRank算法涉及到构建词图以及迭代计算，因此计算速度较慢 ​ 3.虽然考虑了上下文关系，但是仍然将频繁次作为关键词 ​ 4.TextRank算法具有将一定的将关键词进行合并提取成关键短语的能力 ​]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——损失函数]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在机器机器学习和深度学习中有许多常见的损失函数，主要包括： ​ 1.平方差函数MSE（Mean Squared Error） ​ 2.交叉熵函数（Cross Entory） 损失函数选择的方法：1.线性模型中使用平方误差函数，深度学习使用交叉熵函数 ​ 2.平方误差损失函数更适合输出为连续,并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失函数更适合二分类或多分类的场景。 线性模型​ 效果较好的损失函数：平方误差损失函数 ​ 计算公式： ​ ​ 其中，y是我们期望的输出，a是神经元的实际输出a=σ(Wx+b) ​ 损失函数求导： ​ ​ 这也就是每次进行参数更新量的基数，需要再乘以学习速率 为什么深度学习中很少使用MSE作为损失函数？ ​ 当使用MSE作为损失函数时，有上面求导后的公式可以明显的看出，每次的参数更新量取决于σ′(z) ，由Sigmod函数的性质可知，σ′(z) 在 z 取大部分值时会取到一个非常小的值，因此参数更新会异常的缓慢 ​ 深度学习​ 效果最好的损失函数：交叉熵函数 ​ 计算公式： ​ 如果有多个样本，则整个样本集的平均交叉熵为: ​ 对于二分类而言，交叉损失函数为： ​ 损失函数求导： ​ ​ 对于b的求导同理。 ​ 我们可以看出，交叉熵作为损失函数，梯度中的σ′(z) 被消掉了，另外σ(z)-y就是输出值和真实值之间的误差，误差越大，梯度更新越大，参数更新越快。 Softmax损失函数softmax函数​ softmax用于多分类过程中，将多个神经元的输出映射到(0，1)区间，可以看做被分为各个类的概率。 ​ 其中， softmax求导相关推导 ​ 对于使用作为激活函数的神经网络，最终只输出只有最大的softmax最大的项为1其余项均为0，假设yj=1，带入交叉熵公式中得 ​ $$ Loss=-y_{i}loga_i $$ ​ 去掉了累加和，因为只有一项y为1，其余都为0，而将yj=1带入得 ​ $$ Loss=-loga_i $$ ​ 下面我们准备将损失函数对参数求导，参数的形式在该例子中，总共分w41,w42,w43,w51,w52,w53,w61,w62,w63.这些，那么比如我要求出w41,w42,w43的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可，举个例子此时求w41的偏导为: ​ $$\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}$$ ​ 其中右边第一项q求导为： ​ $$\frac{\partial Loss}{\partial a_j} = -\frac{1}{a_j}$$ ​ 右边第三项求导为： ​ $$\frac{\partial z_j}{\partial w_ij} = x_{i}$$ ​ 核心是求右侧第二项：$\frac{\partial a_j}{\partial z_j}$，这里我们分两种情况进行讨论 ​ 将前两项的结果进行连乘得： ​ ​ 而对于分类问题，只会有一个$y_i$为1，其余均为0，因此，对于分类问题： ​ ​ 最终： ​ $$\frac{\partial Loss}{\partial w_{ij}} = \frac{\partial Loss}{\partial a_j}\frac{\partial a_j}{\partial z_i}\frac{\partial z_i}{\partial w_{ij}}==(a_{i}-y{i})x{i}$$]]></content>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——优化器optimzer]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E5%99%A8optimzer%2F</url>
    <content type="text"><![CDATA[​ 在机器学习和深度学习中，选择合适的优化器不仅可以加快学习速度，而且可以避免在训练过程中困到的鞍点。 1.Gradient Descent （GD）​ BGD是一种使用全部训练集数据来计算损失函数的梯度来进行参数更新更新的方式，梯度更新计算公式如下： ​ 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 缺点： 1.由于在每一次更新中都会对整个数据及计算梯度，因此计算起来非常慢，在大数据的情况下很难坐到实时更新。 ​ 2.Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。 2.Stochastic Gradient Descent(SGD)​ SGD是一种最常见的优化方法，这种方式每次只计算当前的样本的梯度，然后使用该梯度来对参数进行更新，其计算方法为： ​ 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad ​ 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况，那么可能只用其中部分的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。 缺点：1.存在比较严重的震荡 ​ 2.容易收敛到局部最优点,但有时也可能因为震荡的原因跳过局部最小值 3.Batch Gradient Descent （BGD）​ BGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。 ​ 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad ​ 参数值设定：batch_szie一般在设置在50~256之间 缺点：1.不能保证很好的收敛性。 ​ 2.对所有参数进行更新时使用的是完全相同的learnnning rate ​ 这两个缺点也是前面这几种优化方式存在的共有缺陷，下面的优化方式主要就是为了晚上前面这些问题 4.Momentum 核心思想：用动量来进行加速 适用情况：善于处理稀疏数据 ​ 为了克服 SGD 振荡比较严重的问题，Momentum 将物理中的动量概念引入到SGD 当中，通过积累之前的动量来替代梯度。即: ​ 其中，γ 表示动量大小，μ表示学习速率大小。 ​ 相较于 SGD，Momentum 就相当于在从山坡上不停的向下走，当没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。也就是说，在训练的时候，在梯度方向不变的维度上，训练速度变快，梯度方向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。 ​ 超参数设定：一般 γ 取值 0.9 左右。 缺点：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。 5.Adaptive gradient algorithm（Adagrad） 核心思想：对学习速率添加约束，前期加速训练，后期提前结束训练以避免震荡，减少了学习速率的手动调节 适用情况：这个算法可以对低频参数进行较大的更新，高频参数进行更小的更新，对稀疏数据表现良好，提高了SGD的鲁棒性，善于处理非平稳目标 ​ 相较于 SGD，Adagrad 相当于对学习率多加了一个约束，即： ​ 对于经典的SGD： ​ ​ 而对于Adagrad： ​ 其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10-7 ​ 超参数设定：一般η选取0.01，ε一般设置为10-7 ​ 缺点：分母会不断积累，这样学习速率就会变得非常小 6.Adadelta​ 超参数设置：p 0.9 ​ Adadelta算法是基于Adagrad算法的改进算法，主要改进主要包括下面两点： 1.将分母从G换成了过去梯度平方的衰减的平均值 2.将初始学习速率换成了RMS[Δθ](梯度的均方根) part one​ (1) 将累计梯度信息从全部的历史信息变为当前时间窗口向前一个时间窗口内的累积： ​ (2)将上述公式进行开方，作为每次迭代更新后的学习率衰减系数 记 其中 是为了防止分母为0加上的一个极小值。 ​ 这里解决了梯度一直会下降到很小的值得问题。 part two​ 将原始的学习速率换为参数值在前一时刻的RMS ​ 因为原始的学习速率已经换成了前一时刻的RMS值，因此，对于adadelta已经不需要选择初始的学习速率 7.RMSprop​ RMSprop 与 Adadelta 的第一种形式相同： ​ 使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动。允许使用一个更大的学习率η ​ 超参数设置：建议设定 γ 为 0.9, 学习率 η 为 0.001 7.Adam 核心思想：结合了Momentum动量加速和Adagrad对学习速率的约束 适用情况：各种数据，前面两种优化器适合的数据Adam都更效果更好， ​ Adam 是一个结合了 Momentum 与 Adagrad 的产物，它既考虑到了利用动量项来加速训练过程，又考虑到对于学习率的约束。利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。其公式为: ​ ​ 其中： 总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用如何选择最优化算法​ 1.如果数据是稀疏的，就是自适应系列的方法 Adam、Adagrad、Adadelta ​ 2.Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum ​ 3.随着梯度变的稀疏，Adam 比 RMSprop 效果会好。 ​ 整体来说Adam是最好的选择 参考文献:深度学习在美团点评推荐系统中的应用 https://blog.csdn.net/yukinoai/article/details/84198218]]></content>
      <categories>
        <category>深度学习</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——KMeans]]></title>
    <url>%2F2018%2F10%2F27%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94KMeans%2F</url>
    <content type="text"><![CDATA[KMeans算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark学习心得]]></title>
    <url>%2F2018%2F10%2F23%2Fpyspark%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[​ ###持久化 ​ Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 ​ 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 ​ spark中的持久化操作主要分为两种：persist和cache。cache相当于使用MEMORY_ONLY级别的persist操作，而persist更灵活可以任意指定persist的级别。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 提高性能的算子使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 Shuffle​ 大多数spark作业的性能主要就消耗在shuffle环节，因为shuffle中包含了大量的磁盘IO、序列化、网络数据传输等操作。但是影响一个spark作业性能的主要因素还是代码开发、资源参数、以及数据倾斜，shuffle调优在优化spark作业性能中只能起较小的作用。 shuffle操作速度慢的原因 ​ Pyspark使用过程中的一些小Tips： 1、RDD.repartition(n)可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，可以减少下面操作的时间。其中的n值看cpu的个数，一般大于2倍cpu，小于1000。 2、Action不能够太多，每一次的action都会将以上的taskset划分一个job，这样当job增多，而其中task并不释放，会占用更多的内存，使得gc拉低效率。 3、在shuffle前面进行一个过滤，减少shuffle数据，并且过滤掉null值，以及空值。 4、groupBy尽量通过reduceBy替代。reduceBy会在work节点做一次reduce，在整体进行reduce，相当于做了一次hadoop中的combine操作，而combine操作和reduceBy逻辑一致，这个groupBy不能保证。 5、做join的时候，尽量用小RDD去join大RDD. 6、避免collect的使用。因为collect如果数据集超大的时候，会通过各个work进行收集，io增多，拉低性能，因此当数据集很大时要save到HDFS。 7、RDD如果后面使用迭代，建议cache，但是一定要估计好数据的大小，避免比cache设定的内存还要大，如果大过内存就会删除之前存储的cache，可能导致计算错误，如果想要完全的存储可以使用persist（MEMORY_AND_DISK），因为cache就是persist（MEMORY_ONLY）。 8、设置spark.cleaner.ttl，定时清除task，因为job的原因可能会缓存很多执行过去的task，所以定时回收可能避免集中gc操作拉低性能。 ​]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——逻辑回归]]></title>
    <url>%2F2018%2F10%2F22%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归模型 ​ 逻辑回归算法是一种根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类的模型。逻辑回归首先赋予每个特征相同的回归参数，然后使用梯度下降算法来不断优化各个回归参数，最终根据回归参数来对新样本进行进行预测。 注意：虽然名叫逻辑回归，但是实际上是一种分类模型 工作原理 12345每个回归系数初始化为 1重复 R 次: 计算整个数据集的梯度 使用 步长 x 梯度 更新回归系数的向量(梯度下降)返回回归系数 逻辑回归算法的特点 优点：计算代价低，可解释性强 缺点：容易欠拟合，分类精度可能不高 使用数据类型：数值型数据和标称型数据(只存在是和否两种结果的将数据) sigmod函数 ​ sigmod是一种近似的越阶函数，可以将任意的输入值，然后将其映射为0到1之间的值，其公式和函数图像如下图： ​ 在逻辑回归中先使用每个特征乘以一个回归系数，将其乘积作为sigmod函数中的z，即 ​ 然后将其得到的值用sigmod函数映射到0到1，可以理解为被分为1类的概率。 梯度上升算法 ​ 要找到某个函数的最大值，最好的方式就是沿着梯度方向不断地去寻找，如果梯度记做▽ ，则函数 f(x, y) 的梯度由下式表示: 这个梯度意味着要沿 x 的方向移动 ，沿 y 的方向移动 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。 ​ 上图展示了整个梯度上升的过程，梯度上升算法在到到每个点后都会从新估计移动的方向，而这个方向就是梯度方向，移动的速度大小由参数α控制。 训练过程 ​ 训练算法：使用梯度上升寻找最佳参数 123456&gt; 每个回归系数初始化为 1&gt; 重复 R 次:&gt; 计算整个数据集的梯度&gt; 使用 步长 x 梯度 更新回归系数的向量(梯度下降)&gt; 返回回归系数&gt; ​ 其中步长为超参数alpha，而梯度的计算如下： 即每个点的数据和其输入数据相同。因此权重的更新可以使用： ​ w:=w+α error x 其中α为常数步长，error为在当前参数值下与目标值的误差经过sigmod函数处理后的值，x为当当前样本的输入 123456789101112131415161718192021222324import numpy as npdef sigmod(x): return 1/1+np.exp(-x)def gradAscend(dataSet,labelSet,alpha,maxCycles): #将输入的数据转为向量格式 dataMat = np.mat(dataSet) labelMat = np.mat(labelSet).tramsponse() #获取输入数据的维度 m,n = np.shape(dataMat) #初始化回归系数 weights = np.ones((n,1)) #对回归系数进行迭代更新 for i in range(maxCycles): #计算使用当前回归系数LR的hx值，结果为(m,1)维向量 h = sigmod(dataMat*weights) #计算误差 error = labelMat-h #根据梯度进行回归系数更新 weights = weights + alpha*dataMat.transponse()*error return weights 随机梯度上升算法 ​ 随机梯度上升算法起到的作用和一般的梯度上升算法是一样的，只是由于一般的梯度上升算法在每次更新回归系数时需要遍历整个数据集，因此当数据量变动很大时，一般的梯度上升算法的时间消耗将会非常大，因此提出了每次只使用一个样本来进行参数更新的方式，随机梯度上升（下降）。 随机梯度上升算法的特点： ​ 1.每次参数更新只使用一个样本，速度快 ​ 2.可进行在线更新，是一个在线学习算法（也是由于每次回归系数更新只使用一个样本） 工作原理： 12345所有回归系数初始化为 1对数据集中每个样本 计算该样本的梯度 使用 alpha x gradient 更新回归系数值返回回归系数值 初步随机梯度下降代码： 1234567891011121314def stocgradAscend(dataSet,labelSet): #1.这里没有转换成矩阵的过程，整个过程完全都是在Numpy数据完成的 alpha = 0.01 m,n = np.shape(dataSet) weights = np.ones((n,1)) #2.回归系数更新过程中的h、error都是单个值，而在一般梯度上升算法中使用的是矩阵操作 for i in range(m): h = np.sigmod(dataSet[i]*weights) error = h - labelSet[i] weights = weights + alpha*error*dataSet[i] return weights 但是这种随机梯度上升算法在在实际的使用过程出现了参数最后难以收敛，最终结果周期性波动的问题，针对这种问题我们对这个问题将随机梯度下降做了下面两种优化 ​ 1.改进为 alpha 的值，alpha 在每次迭代的时候都会调整。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。 ​ ​ 2.修改randomIndex的值，从以前顺序的选择样本更改为完全随机的方式来选择用于回归系数的样本，每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。 最终版随机梯度下降： 1234567891011121314151617181920def stocgradAscend(dataSet,labelSet,numsIter=150): m,n = np.shape(dataSet) weights = np.ones(n,1) alpha = 0.01 for i in range(numsIter): #生成数据的索引 dataIndex = range(m) for i in range(m): #alpha会随着i和j的增大不断减小 alpha = 4/(i+j+1.0)+0.001 # alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001 #生成随机选择要进行回归系数更新的数据索引号 randomIndex = np.random.uniform(0,len(dataIndex)) h = sigmod(np.sum(dataSet[dataIndex[randomIndex]]*weights)) error = h - dataSet[dataIndex[randomIndex]]*weights weights = weights + alpha*error*dataSet[dataIndex[randomIndex]] #在数据索引中删除 del(dataIndex[randomIndex]) return weights 预测过程 ​ LR模型的预测过程很简单，只需要根据训练过程训练出的参数，计算sigmod(w*x),如果这个值大于0.5，则分为1，反之则为0 123456def classfyLR:(inX,weights) prob = sigmod(np.sum(weights*inX)) if prob&gt;=0.5 return 1 else: return 0 注：这里的阈值其实是可以自行设定的 一些其他相关问题 1.LR模型和最大熵模型 ​ (1).logistic回归模型和最大熵模型都属于对数线性模型 ​ (2).当最大熵模型进行二分类时，最大熵模型就是逻辑回归模型 ​ (3) 学习他们的模型一般采用极大似估计或正则化的极大似然估计 ​ (4)二者可以形式化为无约束条件下的最优化问题 2.LR模型的多分类 ​ 逻辑回归也可以作用于多分类问题，对于多分类问题，处理思路如下：将多分类问题看做多个二分类，然后在各个sigmod得到的分数中区最大的值对应的类作为最终预测标签。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布 ​ 基本概率图模型主要包括贝叶斯网络、马尔科夫网络和隐马尔科夫网络三种类型。 ​ 基本的Graphical Model 可以大致分为两个类别：贝叶斯网络(Bayesian Network)和马尔可夫随机场(Markov Random Field)。它们的主要区别在于采用不同类型的图来表达变量之间的关系：贝叶斯网络采用有向无环图(Directed Acyclic Graph)来表达因果关系，马尔可夫随机场则采用无向图(Undirected Graph)来表达变量间的相互作用。这种结构上的区别导致了它们在建模和推断方面的一系列微妙的差异。一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而对于马尔可夫场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。 概率图模型的表示理论​ 概率图模型的表示由参数和结构两部分组成。根据边有无方向性，可分为下面三种： ​ a、有向图模型—贝叶斯网络 ​ b、无向图模型—马尔科夫网络 ​ c、局部有向模型—条件随机场和链图]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最优化问题]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、概述最优化问题主要分为 ​ 1.无约束优化问题 ​ 2.等式优化问题 ​ 3.含不等式优化问题 ​ 对于无约束问题常常使用的方法就是Fermat定理，即求取f(x)的倒数然后另其为0，可求得候选最优质，如果函数为凸函数则直接为最优值。 ​ 在求取有约束条件的优化问题时，拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是非常重要的两个求取方法。二者各有其使用范围： ​ 拉格朗日乘子法：等式约束的最优化问题 ​ KTT条件：不等式约束下的最优化问题 对于一般的任意问题而言，这两种方法求得的解是使一组解成为最优解的必要条件，只有当原问题是凸问题的时候，求是求得的解是最优解的充分条件。 二、有约束最优化问题的求解​ 1.拉格朗日乘子法 ​ 对于等式约束，我们可以通过一个拉格朗日系数a 把等式约束和目标函数组合成为一个式子L(a, x) = f(x) + a*h(x), 这里把a和h(x)视为向量形式，a是横向量，h(x)为列向量，然后求取最优值，可以通过对L(a,x)对各个参数求导取零，联立等式进行求取。 ​ 2.KTT条件 ​ 对于含有不等式的约束条件最优化问题，我们可以把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + ag(x)+bh(x)，KKT条件是说最优值必须满足以下条件： L(a, b, x)对x求导为零； h(x) =0; a*g(x) = 0;（g(x)为不等式约束） 求取这三个等式之后就能得到候选最优值。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习——正则化]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化相关问题 1.实现参数的稀疏有什么好处吗？ 121.可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了。2.参数变少可以使整个模型获得更好的可解释性。 2.参数值越小代表模型越简单吗？ 1是的。这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。 3.模型简单包括什么？ 121.参数少2.参数值小 4.从贝叶斯角度看L1和L2正则化分贝数与什么分布？ ​ 对于频率学派，认为要将参数θ作为未知的定值，而样本X是随机的，其着眼点在样本空间，参数θ虽然我们不知道是什么，但是他是固定的，我们需要通过随机产生的样本去估计这个参数，所以才有了最大似然估计这些方法。 ​ 对于贝叶斯学派，把参数θ也视为满足某一个分布的随机变量，而X是固定的，其着眼点在参数空间，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布，核心是 ​ L1正则化相当于先验分布是拉普拉斯分布，L2正则化相当于先验概率是正态分布。拉普拉斯分布的计算公式： 正态分布概率密度分布公式： 正则化 机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作L1-norm和L2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归） 概念： L1正则化是指权值向量绝对值之和，通常表示为||w||1 L2正则化是指全职向量w中各个元素的平方和让后再求平方根，通常表示为||w||2 下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。 下图是Python中Ridge回归的损失函数，式中加号后面一项α||w||22 即为L2正则化项 注：1.上面的两个函数前半部分可以为任意的线性函数的损失函数，组合成的函数都可以成为Lasso回归会Ridge回归2.上面两个式子中的α为正则化系数，后续通过交叉验证确定 注：上面两个式子中的α为正则化系数，后续通过交叉验证确定) L1正则化与L2正则化的作用： L1正则化可产生稀疏权值矩阵，即产生一个稀疏模型，可用用于特征选择 L2正则化主要用于防止过拟合 L1正则化 L1正则化的标准形式： ​ 其中J0是原始的损失函数，加好后面是L1正则化项。机器学习的最终目就是找出损失函数的最小值，当我们在原本的损失函数后面加上L1正则化后，相当于对J0做了一个约束，另L1正则化项等于L，则 J=J0+L，任务转化为在L1的约束下求J0最小值的解。​ 考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： ​ 从图中可看出j0与L相交于L的一个顶点处，这个顶点即为最优解。注意这个顶点的值为（w1,w2）=(0,w)，可以想象，在更多维的情况下，L将会有很多突出的角，而J与这些叫接触的几率将远大于与L其他部位接触的概率，而这些角上将会有许多权值为0，从而产生系数矩阵，进而用于特征选择。 1234567891011from sklearn.linear_model import Lassofrom sklearn.preprocessing import StandardScaler from sklearn.datasets import load_bostonboston=load_boston() scaler=StandardScaler() X=scaler.fit_transform(boston["data"])Y=boston["target"]names=boston["feature_names"]lasso=Lasso(alpha=.3)lasso.fit(X,Y)print"Lasso model: ",pretty_print_linear(lasso.coef_,names,sort=True) L2正则化 L2正则化的标准形式 ​ 和L1正则化相同，任务转化为在L2的约束下求J0最小值的解。考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： 机器学习过程中权值尽可能小的原因： 试想对于一个模型，当参数很大时，只要数据偏移一点点，就会对结果造成很大的影响，如果参数较小，则数据偏移的多一点，也不会对结果产生多大的影响，抗扰动能力强 为什么L2正则化可以使权值尽可能的小? 对于损失函数不带L2正则化项的梯度下降时参数更新公式为： 加入L2正则化项，参数更新公式为： 根据两个公式之间的差别，我们可以明显的看到，加入正则化以后的梯度下降在进行参数更新时，要先将原有的参数值乘以一个小于1的值，因此权值也会变得比不带的参数小]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM模型]]></title>
    <url>%2F2018%2F10%2F19%2FSVM%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.SVM模型的超参数 ​ SVM模型主要包括C和gamma两个超参数。 ​ C是惩罚系数，也就是对误差的宽容度，C越大代表越不能容忍出现误差，越容易出现过拟合； ​ gamma是选择RBF核时，RBF核自带的一个参数，隐含的决定数据映射到新空间的后的分布，gamma越大，支持向量越少。 支持向量的个数影响训练和预测的个数 gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大,σ会很小，σ很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率 2.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量解析过程中常用的一些工具]]></title>
    <url>%2F2018%2F10%2F12%2F%E6%B5%81%E9%87%8F%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[在实际流量解析过程中一般使用 1.url编码解码-urllib ​ python使用urllib包来进行url编码和解码，对于python3： 123456789101112import urllibrawurl="xxc=B&amp;z0=GB2312&amp;z1=%E4%B8%AD%E5%9B%BD"#python2url = url.unquote(rawurl)#python3url=urllib.parse.unquote(rawurl)output: 'xxc=B&amp;z0=GB2312&amp;z1=中国' 2.字符串转十六进制 ​ 字符串转十六进制可以分为两种：1.对于已经是十六进制格式，但是已经被转为字符串，例如：”” 123import binascii#python3 3.原始字节串和十六进制字节串之间的转化—binascii 12345678910import binasciidata_bytes = b"cfb5cdb3d5d2b2bbb5bdd6b8b6a8b5c4c2b7beb6a1a3"data_hex = b'\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xbd\xd6\xb8\xb6\xa8\xb5\xc4\xc2\xb7\xbe\xb6\xa1\xa3'#原始字节串==&gt;十六进制字节串binascii.hexlify(data_bytes)#十六进制字节串==&gt;原始字节串binascii.unhexlify(data_bytes) ​ ​]]></content>
      <categories>
        <category>流量相关</category>
      </categories>
      <tags>
        <tag>流量相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式]]></title>
    <url>%2F2018%2F10%2F10%2F%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.匹配一个指定字符串，指定字符串前后不能有任何字母和数字内容 12#以c99关键字为例[]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——朴素贝叶斯]]></title>
    <url>%2F2018%2F10%2F09%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[​ 逻辑回归通过拟合曲线实现分类，决策树通过寻找最佳划分特征进而学习样本路径实现分类，支持向量机通过寻找分类超平面进而最大化类间间隔实现分类，而朴素贝叶斯通过 朴素贝叶斯思想 ​ 朴素贝叶斯是一种最简单的概率图模型，通过根据训练样本统计出样本的概率分布，基于贝叶斯定理和条件独立假设来进行建模预测的模型。 朴素贝叶斯概率图 贝叶斯定理 12345678p(AB)=P(A/B)P(B) =P(B/A)P(A)在贝叶斯模型中用到的是下面的形式： P(Ci/W) = P(W|Ci)*P(Ci)/P(W)其中，W为向量，有的多个值组成，Ci为标签，也就是上式可以写成下面的形式 P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)*P(Ci)/P(W)里面的P(Ci/w0,w1,..,w)就是机器学习建模最终的目标，在一定条件下是某一类的概率 条件独立假设 12345 条件独立假设认为：每个事件的发生都相互独立，互相之间没有影响。由于这个假设，上面的式子可以改为： P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)/P(Ci) = P(w0/Ci)P(w1/Ci)...P(wn/Ci)*P(Ci)/p(W) 到这里，我们可以知道，要求的最终的结果，只需要在训练集中求得P(Ci)以及在P(w0/Ci)...P(wn/Ci)即可 模型训练 因此在NB算法训练时，只需要在训练集样本中到下面三个概率分布： ​ 1.P(Ci)，在训练集中标签1出现的概率(二分类只需要统计一个，n分类就需要n-1个) ​ 2.P(wj/Ci),在训练集中属于各个标签的条件下第n个特征是i的概率 注意：这里不需要统计P(W)的概率，因为最终属于各个类型的概率都需要除以相同的P(W)，因此约掉 训练代码： 123456789101112131415161718192021222324252627def trainNB(dataSetList,labels): dataSetVec = np.array(dataSetList) #计算Pc sampleNums = len(dataSetVec) pc = np.sum(datasetVec)/sampleNums #计算p(wj/Ci),这里是二分类 p0Nums = 0 p1Nums = 0 #这里涉及到初始化问题 p0Vecs = np.ones(len(dataSetVec[0])) p1Vecs = np.ones(len(dataSetVec[0])) for i in range(len(labels)): if labels[i]==0: p0Vecs += dataSetVec[0] p0Nums += 1 else: p1Vecs += dataSetVec[0] p1Nums += 1 p0Vecs = p0Vecs/p0Nums p1Vecs = p1Vecs/p1Nums return pc,p0Vecs,p1Vecs 初始化问题： ​ 再利用贝叶斯分类器进行分类时，要计算多个概率等乘积以计算文档属于某个分类的概率，即计算： ​ P(w0|c=1)P(w1|c=1)….P(wn|c=1) ​ 如果其中任意一项为0，那么最终的成绩也将等于0。为了降低这种情况造成的影响，可以将所有词初始化为1. 预测过程 ​ NB模型的预测过程就是使用上面统计得到的概率分布与输入数据进行关联后，计算出新的样本属于各个类型的概率，然后选择其中概率最大的类型作为模型预测类型的过程。预测过程中需要关注的一个关键问题需要重点关注，那就是python的下溢出问题。 ​ 下溢出问题：在python中当多个很小的数相乘时会产生下溢出问题(最后四舍五入得到0) ​ 解决办法：取自然对数。因为自然对数和原来的数怎增减性相同，极值点也相同 ​ 使用自然对数后，上面的式可以转换成： ​ P(Ci/w0,w1,..,w) = P(w0/Ci)P(w1/Ci)…P(wn/Ci)/P(Ci) –&gt;P(Ci/w0,w1,..,w) = log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) 预测代码： ​ 预测过程中将已知的概率分布与输入数据进行关联的方式： ​ log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) ——&gt;log(P(w0/Ci))x0+…+log(P(wn/Ci))xn+log(P(Ci) ​ 这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来. 123456789def classfyNB(input_data,pc,p0Vecs,p1Vecs): #这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来 #这里之所以没有除以pw，是因为对每个类型的pw是一致的，就没有必要所有都除了 p0 = sum(input_data*np.log(p0Vecs))+math.log(pc) p1 = sum(input_data*np.log(p1Vecs))+math.log(1-pc) if p0&gt;p1: return 0 else: return 1 ​]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github基本使用]]></title>
    <url>%2F2018%2F10%2F03%2Fgithub%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[github相关知识： ​ github项目使用终端进行管理时分为三个区域：工作目录、暂存区（本地库）、github上的远程项目，当我们要更新以及操作一个项目时，要遵循以下的格式： 1.先从github上面pull下远程的项目分支 2.本地的项目文件夹中的文件进行更新（更新工作目录中的文件） 3.使用add将更新的索引添加到本地库 4.使用commit工作目录中的文件提交到暂存区(本地库) 5.将文件push到远程分支或merge到远程分支 基本操作 git clone “ssh项目地址” 克隆远程项目 git pull origin master 取回远程主机或本地的某个分支的更新，再与本地分支进行合并(这种写法是origin主机的master分支与本地当前分支进行合并) git push origin master 将本地的当前分支push到origin主机的master分支上 git add “文件名” 将指定文件提交到本地库 git commit -m “描述信息” 将本地的全部文件都提交到本地库 git log 打印该项目的版本操作信息 git status 查看到那个钱仓库状态 更新github 123456789101112131415161718192021222324252627282930313233343536373839#将项目从github上面拉下来(本地已经有的可以跳过,已有则直接进入该文件夹)git clone github链接#查看项目状态git statusoutput: On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean #创建或者导入新文件到工作区touch "文件1"#将文件工作目录的文件提交到暂存区git add "文件1" #提交指定文件git add -A #一次提交工作目录中的全部文件#查看项目状态 git status #第一次提交描述时需要设置账户信息git config --global user.name "John Doe"git config --global user.email johndoe@example.com#添加描述git commit -m "此次添加的描述信息"#查看项目状态git status output: On branch master Your branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)#将修改从暂存区提交到远程分支git push origin master 删除已经提交到github上面的文件 123456789#在本地拉取远程分支git pull original master#在本地删除对应的文件git rm filename#添加描述上传到远程分支git commit -m "删除文件filename"git push original master 已经提交到github上的文件进行版本回退 12345678910#先通过git log获取版本号git log#然后使用git revert 版本号来回退到指定版本git retvert 版本号#然后:x保存退出就可以了撤回到指定的版本了#最后再将本地分支push到github上git push origin master 分支切换 123456git checkout ... #创建分支git checkout -b ... #创建并且换到分支git checkout ... #切换到分支 12345git branch #查看本地分支git branch -a #查看本地和远程的全部分支git push origin --delete dev2 #删除远程分支]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark ML库]]></title>
    <url>%2F2018%2F09%2F30%2Fpyspark-ML%E5%BA%93%2F</url>
    <content type="text"><![CDATA[​ pyspark的ML软件包主要用于针对spark dataframe的建模（MLlib主要还是针对RDD，准备废弃）,ML包主要包含了转化器Transformer、评估器Estimater和管道Pipline三个部分。 1.转化器​ 转换器通常通过将一个新列附加到DataFrame来转化数据，每个转化器都必须实现.transform()方法。 ​ 使用在预处理截断 .transorm()方法常用的参数： ​ 1.dataframe 这是唯一一个强制性参数（也可以不理解为参数） ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 ​ 要使用转化器首先需要引入宝feature 1from pyspark.ml.feature import ... (1)Binarizer ​ 根据指定阈值将连续变量进行二值化 注：这里需要输入的那一列的数据类型为DoubleType,InterType和FloatType都不支持 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)binarizer = Binarizer(threshold=4.0,inputCol='id',outputCol='binarizer_resulit')binarizer.transform(df).show()output: +---+---+-----------------+ | id|age|binarizer_resulit| +---+---+-----------------+ |2.0| a| 0.0| |1.0| b| 0.0| |4.0| b| 0.0| #当值与阈值相同的时候向下取 |9.0| b| 1.0| |4.3| c| 1.0| +---+---+-----------------+ (2)Bucketizer ​ 根据阈值列表将连续变量值离散化 注：splits一定要能包含该列所有值 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)bucketizer = Bucketizer(splits=[0,2,4,10],inputCol='id',outputCol='bucketizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (3)QuantileDiscretizer ​ 根据数据的近似分位数来将离散变量转化来进行离散化 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)quantile_discretizer = QuantileDiscretizer(numBuckets=3,inputCol='id',outputCol='quantile_discretizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (4)Ngram ​ 将一个字符串列表转换为ngram列表，以空格分割两个词,一般要先使用算法来先分词，然后再进行n-gram操作。 注：1.空值将被忽略，返回一个空列表 ​ 2.输入的列必须为一个ArrayType(StringType()) 1234567df = spark.createDataFrame([ [[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;]], [[&apos;s&apos;,&apos;d&apos;,&apos;u&apos;,&apos;y&apos;]]],[&apos;word&apos;])ngram = NGram(n=2,inputCol=&quot;word&quot;,outputCol=&quot;ngram_result&quot;)ngram.transform(df).show() (5)RegexTokener ​ 正则表达式分词器，用于将一个字符串根据指定的正则表达式来进行分词。 参数包括： ​ pattern：用于指定分词正则表达式，默认为遇到任何空白字符则分词 ​ minTokenLength: 最小分词长度过滤，小于这个长度则过滤掉 12 (6)VectorIndexer ​ VectorIndexer是对数据集特征向量中的类别（离散值）特征进行编号。它能够自动判断那些特征是离散值型的特征，并对他们进行编号，具体做法是通过设置一个maxCategories，特征向量中某一个特征不重复取值个数小于maxCategories，则被重新编号为0～K（K&lt;=maxCategories-1）。某一个特征不重复取值个数大于maxCategories，则该特征视为连续值，不会重新编号 主要作用：提升决策树、随机森林等ML算法的效果 参数： ​ 1.MaxCategories 是否被判为离散类型的标准 ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 12345678910111213141516171819+-------------------------+-------------------------+|features |indexedFeatures |+-------------------------+-------------------------+|(3,[0,1,2],[2.0,5.0,7.0])|(3,[0,1,2],[2.0,1.0,1.0])||(3,[0,1,2],[3.0,5.0,9.0])|(3,[0,1,2],[3.0,1.0,2.0])||(3,[0,1,2],[4.0,7.0,9.0])|(3,[0,1,2],[4.0,3.0,2.0])||(3,[0,1,2],[2.0,4.0,9.0])|(3,[0,1,2],[2.0,0.0,2.0])||(3,[0,1,2],[9.0,5.0,7.0])|(3,[0,1,2],[9.0,1.0,1.0])||(3,[0,1,2],[2.0,5.0,9.0])|(3,[0,1,2],[2.0,1.0,2.0])||(3,[0,1,2],[3.0,4.0,9.0])|(3,[0,1,2],[3.0,0.0,2.0])||(3,[0,1,2],[8.0,4.0,9.0])|(3,[0,1,2],[8.0,0.0,2.0])||(3,[0,1,2],[3.0,6.0,2.0])|(3,[0,1,2],[3.0,2.0,0.0])||(3,[0,1,2],[5.0,9.0,2.0])|(3,[0,1,2],[5.0,4.0,0.0])|+-------------------------+-------------------------+结果分析：特征向量包含3个特征，即特征0，特征1，特征2。如Row=1,对应的特征分别是2.0,5.0,7.0.被转换为2.0,1.0,1.0。我们发现只有特征1，特征2被转换了，特征0没有被转换。这是因为特征0有6中取值（2，3，4，5，8，9），多于前面的设置setMaxCategories(5)，因此被视为连续值了，不会被转换。特征1中，（4，5，6，7，9）--&gt;(0,1,2,3,4,5)特征2中, (2,7,9)--&gt;(0,1,2) (7)StringIndexer ​ 将label标签进行重新设置，出现的最多的标签被设置为0，最少的设置最大。 1234567891011121314151617按label出现的频次，转换成0～num numOfLabels-1(分类个数)，频次最高的转换为0，以此类推：label=3，出现次数最多，出现了4次，转换（编号）为0其次是label=2，出现了3次，编号为1，以此类推+-----+------------+|label|indexedLabel|+-----+------------+|3.0 |0.0 ||4.0 |3.0 ||1.0 |2.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||1.0 |2.0 |+-----+------------+ (8)StringToIndex ​ 功能与StringIndexer完全相反，用于使用StringIndexer后的标签进行训练后，再将标签对应会原来的标签 作用：恢复StringIndexer之前的标签 参数： ​ 1.inputCol 输入列名 ​ 2.outputCol 输出列名 123456789101112|label|prediction|convetedPrediction|+-----+----------+------------------+|3.0 |0.0 |3.0 ||4.0 |1.0 |2.0 ||1.0 |2.0 |1.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||1.0 |2.0 |1.0 | 2.评估器​ 评估器就是机器学习模型，通过统计数据从而进行预测工作，每个评估器都必须实现.fit主要分为分类和回归两大类，这里只针对分类评估器进行介绍。 ​ ​ Pyspark的分类评估器包含以下七种： 1.LogisticRegression ​ 逻辑回归模型 2.DecisionTreeClassifier ​ 决策树模型 3.GBTClassifier ​ 梯度提升决策树 4.RandomForestClassifier ​ 随机森林 5.MultilayerPerceptronClassifier ​ 多层感知机分类器]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe基础操作]]></title>
    <url>%2F2018%2F09%2F28%2Fpyspark-dataframe%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.select ​ select用于列选择，选择指定列，也可以用于和udf函数结合新增列 ​ 列选择： 12data_set.select("*").show() #选择全部列data_set.select("file_name","webshell").show() #选择file_name，webshell列 ​ 与udf函数组合新增列： 123456from pysaprk.sql.function import udfdef sum(col1,col2): return col1+col2udf_sun = udf(sum,IntergerType())data_set.select("*",udf_sum("a1","a2")).show() #新增一列a1和a2列加和后的列 2.filter ​ filter用于行选择，相当于使用前一半写好的sql语句进行查询。 ​ 查询某个列等于固定值： 1data_set.filter("file_name=='wp-links-opml.php'").show() ​ 查询符合某个正则表达式所有行:​ 1data_set.filter("file_name regexp '\.php$') #选择所有.php结尾文件 3.join ​ pyspark的dataframe横向连接只能使用join进行连接，需要有一列为两个dataframe的共有列 1df_join = df1.join(df,how="left",on='id').show() #id为二者的共有列 4.agg ​ 使用agg来进行不分组聚合操作，获得某些统计项 1234567#获取数据中样本列最大的数值#方式一：df.agg(&#123;"id":"max"&#125;).show() #方式二：import pyspark.sql.functions as fn df.agg(F.min("id")).show() 5.groupby ​ 使用groupby可以用来进行分组聚合。 1df.groupby(&quot;) 6.printSchema ​ 以数的形式显示dataframe的概要 123456df.printSchema()output: root |-- id: integer (nullable = true) |-- age: integer (nullable = true) 7.subtract ​ 找到那些在这个dataframe但是不在另一个dataframe中的行，返回一个dataframe 1234df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)df1 = spark.createDataFrame([[2.0,'a'],[4.3,'c']],schema=schema)df.subtract(df1).show() #找到在df中但是不在df1中的行 8.cast ​ 指定某一列的类型 12 9.sort ​ 按照某列来进行排序 参数： ​ columns: 可以是一列，也可以是几列的列表 ​ ascending：升序，默认是True 1data.sort(['count'],ascending=False).show()]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe操作进阶]]></title>
    <url>%2F2018%2F09%2F26%2Fpyspark-dataframe%E6%93%8D%E4%BD%9C%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[​ 这一节主要讲的是spark在机器学习处理过程中常用的一列操作，包括获得各种预处理。 1.将多列转化成一列 ​ pyspark可以直接使用VectorAssembler来将多列数据直接转化成vector类型的一列数据。 1234567891011121314151617181920212223242526272829303132from pyspark.ml.feature import VectorAssemblerdiscretization_feature_names = [ 'discretization_tag_nums', 'discretization_in_link_nums', 'discretization_out_link_nums', 'discretization_style_nums', 'discretization_local_img_nums', 'discretization_out_img_nums', 'discretization_local_script_nums', 'discretization_in_script_nums', 'discretization_out_script_nums']vecAssembler = VectorAssembler(inputCols=discretization_feature_names, outputCol="feature_vec_new")data_set = vecAssembler.transform(dataset) #会返回一个在原有的dataframe上面多出来一列的新dataframedata_set.printscheama()output： root |-- discretization_tag_nums: double (nullable = true) |-- discretization_in_link_nums: double (nullable = true) |-- discretization_out_link_nums: double (nullable = true) |-- discretization_style_nums: double (nullable = true) |-- discretization_local_img_nums: double (nullable = true) |-- discretization_out_img_nums: double (nullable = true) |-- discretization_local_script_nums: double (nullable = true) |-- discretization_in_script_nums: double (nullable = true) |-- discretization_out_script_nums: double (nullable = true) |-- feature_vec_new: vector (nullable = true) #多出来的 2.连续数据离散化 ​ pyspark中提供QuantileDiscretizer来根据分位点来进行离散化的操作，可以根据数据整体情况来对某一列进行离散化。 常用参数： ​ numBuckets：将整个空间分为几份，在对应的分为点处将数据进行切分 ​ relativeError： ​ handleInvalid： 12345678from pyspark.ml.feature import QuantileDiscretizerqds = QuantileDiscretizer(numBuckets=3,inputCol=inputCol, outputCol=outputCol, relativeError=0.01, handleInvalid="error")#这里的setHandleInvalid是代表对缺失值如何进行处理#keep表示保留缺失值dataframe = qds.setHandleInvalid("keep").fit(dataframe).transform(dataframe) 3.增加递增的id列 ​ monotonically_increasing_id() 方法给每一条记录提供了一个唯一并且递增的ID。 123from pyspark.sql.functions import monotonically_increasing_iddf.select("*",monotonically_increasing_id().alias("id")).show() 4.指定读取或创建dataframe各列的类型 ​ pyspark可以支持使用schema创建StructType来指定各列的读取或者创建时的类型，一个StructType里面包含多个StructField来进行分别执行列名、类型、是否为空。 12345678910111213from pyspark.sql.types import *schema = StructType([ StructField("id",IntegerType(),True), StructField("name",StringType(),True)])df = spark.createDataFrame([[2,'a'],[1,'b']],schema)df.printSchema()output: root |-- id: integer (nullable = true) |-- name: string (nullable = true) 5.查看各类缺失值情况 123import pyspark.sql.functions as fndata_set.agg(*[(1-(fn.count(i)/fn.count('*'))).alias(i+"_missing") for i in data_set.columns]).show() ​ 注：在其中agg()里面的 ”\“代表将该列表处理为一组独立的参数传递给函数 6.使用时间窗口来进行分组聚合 ​ 这也是pyspark比pandas多出来的一个时间窗口聚合的使用 12src_ip_feature = feature_data.groupby(&quot;srcIp&quot;,F.window(&quot;time&quot;, &quot;60 seconds&quot;)).agg( F.count(&quot;distIp&quot;).alias(&quot;request_count&quot;), 7.过滤各种空值 ​ （1）过滤字符串类型的列中的某列为null行 ​ 这里要借助function中的isnull函数来进行 12345import pyspark.sql.function as Fdf.filter(F.isnull(df["response_body"])).show() #只留下response_body列为null的df.filter(~F.isnull(df["response_body"])).show() #只留下response_body列不为null的 ​ 8.列名重命名 ​ 借助selectExpr可以实现在select的基础上使用sql表达式来进行进一步的操作这一特性，将列名进行修改 12#将count列重命名为no_detection_nums,webshelll_names列名不变df = df.selectExpr("webshell_names","count as no_detection_nums")]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM入门以及在webshell检测中的应用汇]]></title>
    <url>%2F2018%2F09%2F24%2FHMM%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8A%E5%9C%A8webshell%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%B1%87%2F</url>
    <content type="text"><![CDATA[HMM一、HMM五元素 ​ ​ 其中： ​ N：隐藏状态数 hidden states ​ M：观测状态数 observed states ​ A： 状态转移矩阵 transition matrix ​ B：发射矩阵 emission matrix ​ pi：初始隐状态向量 initial state vector HMM全称隐马尔科夫链，常用与异常检测，在大量正常的模式中找出异常的模式。 ​ 隐马尔科夫链模型相关的问题主要分为三类： 1.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链 2.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 3.已知隐含状态数量、可以观察到多个可见状态链，求因此状态的转移矩阵和发射概率 1.求隐藏状态链问题​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链(也就是最大概率的转移序列) ​ 应用场景：语音识别解码问题 ​ 方法：Viterbi algorithm ​ 举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。 ​ 首先，如果我们只掷一次骰子：看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8. ​ 把这个情况拓展，我们掷两次骰子：结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是 ​ 同样的，我们可以计算第二个骰子是D4或D8时的最大概率。我们发现，第二个骰子取到D6的概率最大。而使这个概率最大时，第一个骰子为D4。所以最大概率骰子序列就是D4 D6。​ 继续拓展，我们掷三次骰子：同样，我们计算第三个骰子分别是D6，D4，D8的最大概率。我们再次发现，要取到最大概率，第二个骰子必须为D6。这时，第三个骰子取到D4的最大概率是​ 同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。 写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。 ​ ​ 我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来,这就是Viterbi算法。 2.求得出某个可见状态链的概率​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 ​ 应用场景：检测观察到的结果与我们已知的模型是否吻合，即异常检测 ​ 方法:前向算法（forward algorithm） ​ 要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算（即在当前的HMM下可能出啊先找个状态链的概率）。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。​ 我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。​ 首先，如果我们只掷一次骰子，看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18： ​ 把这个情况拓展，我们掷两次骰子，看到结果为1，6，总概率为0.05： ​ 继续拓展，我们掷三次骰子，看到结果为1，6，3，计算总概率为0.03： ​ 同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。 3. 求状态转移矩阵和发射概率（训练过程）​ 该问题是在： 已知隐含状态数量、可以观察到多个可见状态链 ​ 应用场景：有大量该问题的已知观测序列，想训练一个HMM模型 ​ 方法：Baum-Welch算法 HMM在webshell检测中的应用缺陷： 由于HMM的训练和预测的时间复杂度都是O(n^2)，因此在数据量特别大的时候速度非常慢 HMM并没有spark版本，无法直接部署到spark集群上（这点是个大坑！） HMM在在webshell检测中比较局限，只能部署在生成文件名绝大部分都有固定模式的网站上来进行检测，对于各个页面随意进行命名的网站并没有检测效果 使用RNN来代替HMM​ 在数据量很大时，我们可以使用RNN来代替HMM，RNN在实现过程中可以采用并行的方式来进行梯度下降，大大加快了整体的速度。 ​ 而且大部分时候使用RNN的效果都会优于HMM，详细见下篇：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker操作]]></title>
    <url>%2F2018%2F09%2F23%2FDocker%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[-d 容器在后台运行 -P 将容器内部的端口映射到我们的主机上 12docker ps #查看全部正在开启的dockerdocker ps ####1.进入以及退出docker ​ 进入docker命令主要分为两种，attach和exec命令，但是由于exec命令退出后容器继续运行，因此更为常用。 12345678910#首先查看正在运行的docker，在其中选择想要进入的docker namedocker ps#然后使用exec进行进入dockerdocker exec --it docker_name /bin/bash/#进行各种操作#退出dockerexit或Ctrl+D 2.docker和宿主主机之间传输文件​ docker 使用docker cp 命令来进行复制，无论容器有没有进行运行，复制操作都可以进行执行。 12345#从docker中赋值文件到宿主主机docker cp docker_name:/docker_file_path local_path#从宿主主机复制到dockerdocker cp local_path docker_name:/docker_file_path]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmux基本操作]]></title>
    <url>%2F2018%2F09%2F23%2Ftmux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[tmux 是一款终端复用命令行工具，一般用于 Terminal 的窗口管理。 tmux核心功能 1.tmux可以在一个窗口中创建多个窗格 2.终端软件重启后通过命令行恢复上次的session 在tmux中快捷键都需要在使用之前先按前缀快捷键(mac默认⌃b,windows默认control)，以下是常用的集中快捷键列表: 1. 窗格操作 % 左右平分出两个窗格 &quot; 上下平分出两个窗格 x 关闭当前窗格 { 当前窗格前移 } 当前窗格后移 ; 选择上次使用的窗格 o 选择下一个窗格，也可以使用上下左右方向键来选择 space 切换窗格布局，tmux 内置了五种窗格布局，也可以通过 ⌥1 至 ⌥5来切换 z 最大化当前窗格，再次执行可恢复原来大小 q 显示所有窗格的序号，在序号出现期间按下对应的数字，即可跳转至对应的窗格 2.会话操作 ​ 在shell中每次输入tmux都会创建一个tmux会话(session)，在tmux中常用的tmux操作包括： $ 重命名当前会话 s 选择会话列表 d detach 当前会话，运行后将会退出 tmux 进程，返回至 shell 主进程 ​ 在shell准进程中也可以进行直接对session进行操作： tmux new -s foo #创建名为foo的会话 tmux ls #列出所有的tmux tmux a #恢复至上一次回话 tmux a -t foo #恢复会话名称为foo的会话 tmux kill-session -t foo #删除会话名称为foo的会话 tmux kill-server #删除所有会话 除了上面常用的快捷键以外，还可以直接使用前缀快捷键⌃b加?来查看所有快捷键列表]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch向量转化操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.cat ​ 对数据沿着某一维度进行拼接，cat后数据的总维数不变。 12345678910111213141516171819202122import torchtorch.manual_seed(1)x = torch.randn(2,3)y = torch.randn(1,3)s = torch.cat((x,y),0)print(x,y)print(s)output: 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 [torch.FloatTensor of size 2x3] -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 1x3] 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 3x3] 注：torch.cat和torch.concat作用用法完全相同，只是concat的简写形式 2.unsequeeze和sequeeze ​ torch.sequeeze主要用于维度压缩，去除掉维数为1的维度。 12345678#1.删除指定的维数为1的维度 #方式一： torch.sequeeze(a,2) #方式二： a.sequeeze(2)#2.删除全部维度为1的维度 torch.sequeeze(a) ​ torch.unsequeeze主要用于维度拓展，增加维数为1的维度。 1torch.unsequeeze(a,2) #在维度2增加维数为1的维度]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch运算操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E8%BF%90%E7%AE%97%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.transponse ​ torch.transponse操作是转置操作，是一种在矩阵乘法中最常用的几种操作之一。 12#交换两个维度torch.transponse(dim1,dim2) 2.matmul和bmm ​ matmul和bmm都是实现batch的矩阵乘法操作。 123456789a = torch.rand((2,3,10))b = torch.rand((2,2,10))res1 = torch.matmul(a,b.transpose(1,2))#res1 = torch.matmul(a,b.transose(1,2))print res1.size()output: [torch.FloatTensor of size 2x3x2]]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2F%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[常见机器学习机基本问题1.参数模型和非参数模型的区别？ 参数模型：在进行训练之前首先对目标函数的进行假设，然后从训练数据中学的相关函数的系数 典型的参数模型：LR、LDA(线性判别分析)、感知机、朴素贝叶斯、简单神经网络 参数模型的优点： 简单：容易理解和解释结果 快速：训练速度快 数据需求量少 参数模型的局限性： 模型的目标函数形式假设大大限制了模型 由于参数模型复杂度一般不高，因此更适合简单问题 非参数模型：不对目标函数的形式做出任何强烈的假设的算法，可以在训练集中自由的学习任何函数形式 典型的非参数模型:KNN、决策树、SVM 非参数学习模型的优点： 灵活性强，可拟合各种不同形式的样本 性能：模型效果一般较好 非参数学习模型的局限性 训练数据需求量大 训练速度慢，因为一般非参数模型要训练更多的参数 可解释性差 更容易出过拟合 2.生成模型和判别模型 由生成方法生成的模型成为生成模型，由判别方法产生的模型成为生成模型。下面重点介绍两种方法。 生成方法:由数据学联合概率分布P(X,Y)，然后求出条件概率P(Y|X)作为预测模型,即生成模型。（之所以称为生成方法是因为模型表示了给定输入X产生出Y的生成关系） 典型的生成模型:朴素贝叶斯、隐马尔科夫链 生成方法特点: 可还原联合概率分布 收敛速度更快 生成方法可处理隐变量，但判别方法不能 判别方法：由数据直接学习决策函数f(x)或者条件概率分布f(Y|X)作为预测模型，即判别模型。 典型的判别模型:KNN、感知机、决策树、LR 判别模型的特点： 直接学习决策函数或条件概率，直接面对预测，准确率更高 由于直接学习决策函数或条件概率，可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题 3.常见损失函数以及应用？ 逻辑斯特损失函数： 对数似然损失 合页损失 指数损失 4.朴素贝叶斯“朴素”表现在哪里？“朴素”主要表现在它假设所有特征在数据集中的作用是同样且独立的，而在真实世界中这种假设是不成立的，因此称之为朴素贝叶斯。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化之散点图和折线图]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8B%E6%95%A3%E7%82%B9%E5%9B%BE%E5%92%8C%E6%8A%98%E7%BA%BF%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[数据可视化之散点图和折线图画图基本常用参数plt.figure(figure_size=(30,20)) 指定图片大小 plt.style.use(&apos;ggplot&apos;) 指定图片风格 plt.title(&quot;image title&quot;,fontsize=30) 指定图片标题 指定坐标轴相关my_x_trick = np.arrange(0,200,10) plt.xtricks(my_x_trick,fontsize=20,rotation) 指定x轴，fontsize指定坐标轴字体，rotation指定文字旋转的角度 plt.ytricks(fontsize=20) 指定y轴 指定画图类型1.折线图plt.plot(x,y) #不指定画图种类时默认为折线图 plt.legend(loc = &quot;best&quot;,fontsize=40,shadow=1) #进行图例格式设定 plt.show() 折线图中plot可用参数：1.color=’red’ 指定折线的颜色2.label=’price’ 指定改颜色的图例表示3.marker=’-‘ 设置折现格式，默认为’-‘,注意这里设置范围不要越界，当设置越界时转换其他图 在一个文件中多次调用plt.plot(),使用不同的数据指定不同颜色和label可在一个图中画多条折线进行对比 2.散点图方式一： plt.scatter(x1,x2,marker=&apos;o&apos;) #指定画散点图，marker为点的形状 plt.show() 方式二： plt.plot(x1,x2,marker=&apos;o&apos;) #plot使用marker=‘o’则为散点图 plt.show() 在实际情况中第二种方式更为灵活，因此我们下重点介绍第二种方式的参数情况。 散点图中常用参数（方式二）： markerfacecolor 散点内部颜色 markeredgecolor 散点边缘颜色 markersize 散点大小 下面我们以DBSCAN聚类后的结果进行将为可视化为例进行效果展示： from sklearn.manifold import TSNE #使用TSNE进行降维 tsne = TSNE(learning_rate=100) x = tsne.fit_transform(input) labels = dbscan.labels_ #获取最终的预测结果 unique_labels = set(dbscan.labels_) colors = plt.cm.Spectral(np.linspace(0,1,len(set(dbscan.labels_)))) #生成和标签种类数相同的颜色数组 core_samples_mask =np.zeros_like(dbscan.labels_,dtype=bool) core_samples_mask[dbscan.core_sample_indices_] = True #将核心对象点对应的位置置true plt.style.use(&apos;ggplot&apos;) plt.figure(figsize=(30,20)) for k,col in zip(unique_labels,colors): if k==-1: col=&apos;k&apos; class_member_mask = (labels==k) xy = x[class_member_mask &amp; core_samples_mask] plt.plot(xy[:,0],xy[:,1],&apos;o&apos;,markerfacecolor=col,markeredgecolor=&apos;k&apos;,markersize=10)]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2FDBSCAN%E5%92%8CKMeans%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[DBSCAN和KMeans相关资源和理解1.DBSCANDBSCAN是是一种典型的密度聚类算法，算法主要的思是由密度可达关系导出最大的密度相连样本集合，将其作为一个类。​ 主要参数： 最小分类样本数 半径 DBSCAN算法为有参数算法，聚类的最终结果很大程度上取决于参数的设定 DBSCAN算法不需要指定聚类个数，聚类个数根据算法和数据情况自动获得 DBSCAN聚类过程 首先根据半径画每个点的邻域，当点的邻域内点的个数大于最小样本数时，该点位为核心对象（原始数据集重点的变为核心对象和一般点） 随机确定一个核心点作为初始点，将该初始点全部的最大密度相连的点作为一类。 将分好类样本从原始的样本集中除去，从新选择核心对象作为聚类中心，再进行2.3操作，直至全部核心对象都被分类 DBSCAN代码实现from sklearn.cluster import DBSCAN dbcscan = DBSCAN(min_samples=30,eps=1.8) predict = dbscan.fit_predict(imput) 2.K-MeansKMeans是一种原始性聚类算法，算法主要思想是通过迭代过程把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使生成的每个聚类内紧凑，类间独立。这一算法不适合处理离散型数据，对连续性数据具有良好的效果 Kmeans为无参数算法，算法执行过程中不需要进行调参 Kmeans算法需要指定聚类个数K，这在实际问题中是很难进行确定的 KMeans聚类过程 根据指定的K值随机寻找K个点作为初始中心，将其他样本分别分给这些中心 由分好的类计算均值作为其该类新的中心，重新对各个样本分到距离最近的中心，重复这一过程，直至中心不再变化 Kmeans代码实现from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=8) predict = kmeans.fit_predict(input) Kmeans、DBSCAN优缺点对比DBSCAN的主要优点有： 1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 DBSCAN的主要缺点有： 1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。 3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树模型之CART树和C5.0]]></title>
    <url>%2F2018%2F09%2F23%2F%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E4%B9%8BCART%E6%A0%91%E5%92%8CC5-0%2F</url>
    <content type="text"><![CDATA[决策树模型之CART树和C5.0树模型基本思想：计算结点的纯度来选择最具显著性的切分不同树模型之间的差异：差异在于衡量纯度变化的标准不同 CART树：Gini系数C5.0树：信息熵增益 1.回归树(CART树)回归树也成为分类回归树，是一种既可用于分类也可用于回归的算法。 CART树分类的主要步骤：1. 决策树的生成：递归的构建而决策树的过程，基于训练数据生成决策树，生成的决策树数量应尽量大。 自上而下的从根开始建立节点，在每个节点处选择一个最好的属性来分类，使子节点红的训练集尽可能的顿。 不同算法使用不同的指标来衡量“最好”： 分类算法：一般选择Gini系数 回归算法：使用最小二乘偏差（LSD）或最小绝对偏差（LSA） 2.决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树这时损失函数最小做为标准 分类树的生成 对每个特征A，对它所有的可能取值a，将数据集划分为A=a和A!=a两个部分计算集合D的基尼指数： 遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。 对上述两个子节点递归调用步骤(1)(2), 直到满足停止条件。 生成CART树 基尼指数： 是一种不等度的度量 是介于0~1之间的数，0-完全相等，1-完全不相等 总体内包含的类别越杂乱，Gini指数就越大 分类问题中，假设存在K个类，样本属于第k个类的概率为pk，则概率分布的Gini指数为： 样本集合D的Gini指数为：​ 当在数据集D上根据某一取值a进行分割，得到D1、D2两部分后，那么特征A下集合D的Gini指数为：​ 算法停止条件： 节点中样本个数小于预定阈值 样本的Gini指数小于阈值 没有更多特征 剪枝在完整的的决策树上，减掉一些完整的子支是决策树变小，从而防止决策树过拟合。 决策树很容易产生过拟合，改善的方式包括： 通过阈值控制终止条件，防止分支过细 对决策树进行剪枝 建立随机森林 2.C5.0节点分裂标准：信息增益比 C系列决策树发展过程： 阶段一：ID3 节点选择标准：信息增益 缺陷：1. 方法会倾向与属性值比较多的变量（如省份字段存在31个水平，性别由两个水平，信息增益会考虑选择省份做特征节点 2.构造树时不能很好地处理连续变量 阶段二：C4.5 节点选择标准：信息增益比（避免了偏袒） 缺点：运行效率很低 阶段三：C5.0 商业版的C4.5，提升了算法效率，但没有公布具体算法细节 C5.0算法特点1.C5.0是一种多叉树。 如果根节点或者中间节点为连续变量，则改变量一定会一分为二成为两个分支；如果根节点或者中间节点为离散变量，则分开离散变量水平数个分支；因此一个变量一旦被使用，后面节点都不会在使用该变量。 2.C5.0生成树采用信息增益比进行分裂节点的选择3.剪枝采用“减少误差”法和“减少损失”法进行。 减少误差法：核心思想是比较剪枝前后的误差率 误差率的计算：如果第i个节点中包含N个样本，其中预测错误的样本量为M，则该节点的错误率为f=M/N 减少损失法：该方法结合损失矩阵对树进行剪枝，核心是剪枝前后的的损失量。 4.C5.0只能解决分类问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github+hexo博客使用]]></title>
    <url>%2F2018%2F09%2F23%2FGithub-hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.创建新博文页面 创建博文页面首先要cd 到博客的根目录下，然后运行命令： 1hexo new &quot;页面名称&quot; 这样则在博客站点的source/_posts/文件夹下创建了指定的“页面名称.md”文件，可以直接对其进行编辑。完成编辑后就可已使用下面的同步命令来将修改提交到GIthub上： 1hexo d -g #生成静态文件并部署到服务器，g是generate代表生成文件，d是depoly代表部署到服务器]]></content>
      <categories>
        <category>博客相关</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F09%2F23%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[文件管理1.查看当前文件夹中文件 #显示文件夹中全部文件和目录数 ls | wc -l #显示包含指定内容的文件和目录 ls *3094 | wc -l #显示文件夹中文件的个数 find . -type f | wc -l 注：wc 是统计前面管道输出的东西，-l表示按照行统计 磁盘管理1.查看文件夹中文件的总大小 #查看当前文件夹中文件总大小 du -h # -h表示以人可理解的方式输出 #查看指定文件夹中文件的总大小 du /home/yhk/ -h ​2.查看磁盘各个分区大小及使用情况​ df -h 内存、Cpu使用情况查看1.Cpu个数 逻辑Cpu个数查看： 1.方式一： 先使用top密令进入top界面，在界面中按1，即可出现cpu个数以及使用情况 2.方式二： cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l ​ 物理CPU个数查看： cat /proc/cpuinfo |grep &quot;physical id&quot;|sort |uniq|wc -l 一个物理CPU数几核：​ cat /proc/cpuinfo |grep “cores”|uniq 2.CPU内存使用情况 top 实用程序以忽略挂起信号方式执行​ nohup command &gt; myout.file 2&gt;&amp;1 &amp; #文件产生的输出将重定向到myout.file ​​​​ ​​​]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>服务器管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch入门学习心得]]></title>
    <url>%2F2018%2F09%2F23%2FPytorch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[全连接1.DropoutDrop是一种现在在深度学习中使用最为广泛的防止过拟合的方式 核心思想:再训练神经网格时的时候依据概率P保留每个神经元的权重，也就是说每次训练的时候都会有一些神经元被置0，这样就保证神经网络神经网络不会过度学习 注意：我们只是在训练的时候使用dropout去使一些神经元不参与训练，但是在预测阶段会使用全部的神经元参与预测 使用情况：卷积神经网路只在最后的全连接层中使用dropout，循环神经网络一般只在不同层循环结构体之间使用dropout ２.Batch Normalization核心思想：将标准化应用的整个前向传播和反向传播的过程中。传统的标准化一般只是在输入数据之前将数据进行标准化处理，而批标准化是在神经网络的训练过程中对每层的输入数据都进行一个标准化 使用位置:线性层和非线性层之间 作用：1.加快收敛速度 2.防止过拟合 ​ ３.从神经网络角度看线性回归和逻辑回归的区别？ 丛神经网络角度上看，逻辑回归只是在线性回归的基础上计入了一层Sigmod激活函数。 ４.全连接网络设计趋势1. 使用线性层和非线性激活函数交替的结果来代替线性层交替的结构往往能大大提升准确率2. 在线性层和非线性激活函数之间加入批标准化处理加快收敛速度 CNN卷积层卷积层可以看作是多个滤波器的集合，滤波器在深度上要和输入数据保持一致，让每个滤波器在宽度和深度高度上进行滑动卷积，然后计算整个滤波器和输入数据任意一处的内积，输出数据的深度和滤波器的个数保持一致 1.卷积层为什么有效？(1).局部性判断题图片的类型并不是根据整张图片来决定的，而是由一定的局部区域决定的 (2).相同性对于不同的图片，如果他们属于同一类，他们将具有相同的特征，但这些特征可能属于图片的不同位置，但是在不同位置的检测方式几乎是一样的 (3).不变性当我们对图片进行下采样时，图片的基本性质不变 2.卷积层的参数 关键参数: in_channels ：特征的管道数，彩色图片为3，黑白图片则为1 out_channels : 输出管道个数，也就是滤波器个数 kernel_size : 卷积核大小 可选参数： padding:边界填充0的层数 stride:步长 bias: 是否使用偏置，默认是True 3.卷积层的输入输出 输入： 卷积层的输入格式为(batch,channels,width,heights) 输出： 卷积层的输出取决于输入数据大小W、卷积核大小F、步长S、0填充个数P等四个方面，计算公式如下： W-F+2P/S + 1 这里在介绍几种常用的卷积层参数设置： 卷积核大小 0填充层数 步长 卷积层输出 3 1 1 保持输入维度不变 3 0 1 输入维度减2 一般卷积核大小不超过5 4.卷积层的参数共享基于特征的相同性，因此可以使用相同的滤波器来检测不同位置的相同特征，参数共享共享机制有效的减少卷积层的参数个数，加快了卷积神经网络的训练速度。 使用参数共享机制的CNN层参数个数为： 滤波器个数（out_dim） 神经元大小（kernel_size kernel_size * input_dim） 例如：当卷积层的的输出是20 20 32，窗口大小是3 3，输入数据深度是10时，当不适用参数共享时神经元个数为20 20 32，每个神经元个数为3 3 10，总参数个数为12800 900 =11520000个参数。但当我们使用参数共享时，因为输出深度为32，所以存在32个滤波器，每个滤波器存在参数3 3 10个，而总参数个数即为90 * 32个，大大减少了参数的个数 池化层1.使用池化层有什么作用？ 有效的缩小矩阵的尺寸 加快计算速度 防止过拟合 2.池化层的参数设置 关键参数： kernel_size ：池化层的大小 池化层也可也进行0填充，但是几乎不用 池化层最常用的池化方式以及参数设置： 池化类型 卷积核大小 步长 池化层输出 MaxPooling 2 2 输入维度的一半 注意：池化层只能改变高度和宽度，不能改变深度；卷积层即可改变数据的宽度和高度也可以改变数据的深度 经典卷积设计的趋向1. 使用小滤波器2. 多个卷积层和非线性激活层交替的结构比单一的卷积层结构能更加有效的提取出更深层次的特征，并且参数个数更少 RNN1.RNN模型的超参数 关键参数： input_size:输入的维度 hidden_size：隐藏层维度，也是最终输出的维度 num_layers: RNN层数可选参数: batch_first : 将输入输出的batch放在最前面，顺序为（batch,seq,feature） bidirectional: True表示双向循环神经网络，默认为False dropout: 参数接受一个0~1之间的一个值，会将网路中出最后一层外加入dropout层 2.RNN模型的输入 RNN模型的输入为:(seq,batch,feature),这里要重点注意，在建立模型时可使用batch_first将顺序变成正常的(batch,seq,feature)． 其中的含义为: batch: 样本个数 seq: 每个样本依据附近的样本个数 feature: 每个样本特征数 其实RNN的网络中需要两个输入，上面的序列输入是主要输入，必须进行人工指定，还有一个起始状态输入，可选进行输入，不指定则默认为全0 3.RNN模型的输出 RNN的模型输出分为实际输出output和记忆状态h两部分。其中各自的形式和表达如下： 实际输出output: 维度：(seq,batch,hiddendirection)记忆状态： 维度：(layers direction,bactch,hidden) 注：使用batch_first可将batch放在最前面 4.RNN使用作词性判断 因为RNN可根据上下文进行输出，因此使用RNN模型根据上下文的词性判断某个词的词性比直接根据该单词判断效果更好。 训练集：​ 输入：给定的句子​ 标签：句子中每个单词的词性 基本原理： 首先我们使用word_embedding将句子中的每个词进行词向量化，如：The dog ate apple 转化成4 word_dim 的词向量 x = x.embedding(x) 因为lstm需要的输入形式为3维，因此我们要将其转换为1 4 word_dim x = x.unsqueeze(0) 再将其输入到lstm模型中，得到模型的实际输出维度为：batch seq * hidden_dim output 因为我们需要判断是的最后一个词的词性，因此我们只需要取最后一个seq就好了 output[:,-1,:] 因为一个单词的词性不只与其上下文关系有关，还与其单词的字母排列情况有关，因此我们可以上面的基础上增加字符角度的lstm来进行表征其字母排列情况。 完善方案： 遍历句子The dog ate apple中的每个单词： 将单词中的每个字母进行词向量表示，如apple转化成5 char_dim的词向量 char_info = nn.embedding(x) 将其转换为３维：１ 5 char_dim char_info = char_info.unsqueeze(0) 将模型输入lstm模型，但这里取记忆状态作为输出,输出状态是h0维度为(1,1,hidden_dim) _,h = char_lstm(char_info) h[0] 将各个单词的输出组合成一个向量，按照seq进行拼接,形成一个1 4 hidden_dim的向量 for word in words: char_seq = make_sequeece(word,char_to_idx) char_seq = self.char_lstm(char_seq) word_set.append(char_seq) char = torch.stack(word_set,1) 根据前面基本方法将单词进行向量化，得到1 4 word_dim维向量，将其与字符级别的lstm结果从feature维度进行拼接，得到1 4 char_hidden+word_dim维向量 x = torch.cat((x,char),dim=2) 最后将两个角度的得到的特征一起输入的最终的lstm模型，在经过全连接层得到最终结果]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
</search>
