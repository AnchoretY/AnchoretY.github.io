<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机试-二叉树遍历]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%9C%BA%E8%AF%95-%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[​ 二叉树最常用的遍历算法主要分为下面几种： ​ 1.先序遍历 ​ 2.中序遍历 ​ 3.后序遍历 ​ 4.层次遍历 ​ 下面我们将针对这些遍历算法的递归与非递归实现分别给出代码实现以及特点。 先序遍历​ 先序遍历指在二叉树便利过程中首先输出根节点，然后再分别输出左右节点的遍历方式。 #####递归实现 123456789101112131415def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ def core(result,root): if root==None: return result.append(root.val) core(result,root.left) core(result,root.right) result = [] core(result,root) return result 非递归实现12345678910111213141516171819202122def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] stack = [] res = [] stack = [root] while stack: node = stack.pop() res.append(node.val) #注意这里的顺序一定是先右后左，和一般的相反 if node.right!=None: stack.append(node.right) if node.left!=None: stack.append(node.left) return res 中序遍历​ 二叉树的中序遍历是指现先遍历左节点，中间遍历根节点，最后在遍历右节点的便利方式。 递归实现1234567891011121314def Core(root): if root==None: return [] Core(root.left) result.append(root.val) Core(root.right) return result result = [] Core(root) return result 非递归实现12345678910111213141516171819202122def inorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if root==None: return [] stack = [] result = [] pos = root while stack or pos: if pos: stack.append(pos) pos = pos.left else: pos = stack.pop() result.append(pos.val) pos = pos.right return result 后序遍历]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-回文子串相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95-%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[回文子串 例：给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被计为是不同的子串。 123456789101112131415def countSubstrings(self, s): """ :type s: str :rtype: int """ length = len(s) result = 0 for i in range(length): for j in range(i+1,length+1): #这里注意循环的范围为range(i+1,length+1) if s[i:j]==s[i:j][::-1]: result += 1 return result 最长回文子串​ 最长回文子串也是回文串中常见的一中题目，下面是例题 例：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 思路一：Manacher算法 ​ 首先先将字符串首尾以及字符和字符之间采用”#“进行补齐，补齐后的字符串总长度2n+1(n为原始字符串长度)。然后从第一个非#字符 12345678910111213141516171819202122232425262728293031323334def get_length(string, index): # 循环求出index为中心的最长回文字串 length = 0 seq = "" if string[index]!="#": seq = string[index] length = 1 string_len = len(string) for i in range(1,index+1): if index+i&lt;string_len and string[index-i]==string[index+i]: # print(string[index-i],seq+string[index+i]) if string[index-i]!="#": length +=2 seq = string[index-i]+seq+string[index+i] else: break return length,seq s_list = [i for i in s] string = "#"+"#".join(s)+"#" length = len(string) max_length = 0 max_seq = "" for index in range(0,length): # print("====") tmp_len,tmp_seq = get_length(string,index) # print(tmp_len,tmp_seq) if tmp_len&gt;max_length: max_length = tmp_len max_seq = tmp_seq return max_seq 思路二：动态规划 ​ 这里的动态规划的核心思路就是从头开始向后进行遍历，每次想看头尾同时加入比最大之前最大回文子串的长多+1字符串是不是回文子串(注意但是首部索引不能超过0)，如果是则记录起始节点start，max_len的值+2；否则判断只在尾部进行字符串加1的字符串时不是回文子串（这里之说以不必尝试在头部加1，因为再从头开始遍历的过程中已经尝试了头部加1），如果是记录start节点，max_len的值+2 ​ f(x+1) 12345678910111213141516171819def longestPalindrome(self, s): """ :type s: str :rtype: str """ length = len(s) max_len = 0 start = 0 for i in range(length): if i-max_len&gt;=1 and s[i-max_len-1:i+1]==s[i-max_len-1:i+1][::-1]: start = i-max_len-1 max_len += 2 elif i-max_len&gt;=0 and s[i-max_len:i+1]==s[i-max_len:i+1][::-1]: start = i-max_len max_len += 1 return s[start:start+max_len] 最长回文子序列516​ z]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-含环链表相关]]></title>
    <url>%2F2019%2F03%2F12%2F%E6%9C%BA%E8%AF%95-%E5%90%AB%E7%8E%AF%E9%93%BE%E8%A1%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[​ 在含环的问题中，存在一些关键性的结论，在解决问题时非常有帮助，下面是一些相关的总结。 1.判断链表是否有环​ 结论：一个速度为1的low指针和一个速度为2的fast指针同时从头向前走，如果其中fast指针为None，那么则为无环，如果两个只能指向的元素相等，那么链表有环。 2.判断链表的环入口节点​ 结论：函数一样的双指针进行遍历，如果fast指针为None,那么则为无环。如果两个指针指向的的元素相同，那么这个节点到链表入口点的长度和链表头到链表入口点的长度相等。 推导过程： ​ 设链表头到入口节点的长度为a ​ 链表入口节点到相遇节点的长度为b ​ 相遇节点到链表入口节点的长度为c ​ 那么因为fast的速度为2，low的速度为1，因此可以认为low入环时走在前面，每次fast和low之间的距离缩小1，因此，必定会在第一圈完成之前相遇。所以有 ​ low 在环内位置: (a+b)-a mod (b+c) -&gt; b mod (b+c) ​ fast 在环内位置：2(a+b)-a mod (b+c) -&gt; a+2b mod (b+c) 二者应该相等，因此得出 a+b mod (b+c) = 0 即a = c ​ 利用这个结论，我们可以先判断判断链表是否有环，如果有环，那么先找到相间的节点，然后再用一个新指针从头开始以速度为1和low指针从相交节点同时开始遍历，当两个点相交的节点即为环入口节点。 例题：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null. 12345678910111213141516def detectCycle(head): """ :type head: ListNode :rtype: ListNode """ low,fast = head,head while fast and fast.next and fast.next: low, fast = low.next, fast.next.next if fast==low: p = head while p!=low: p = p.next low = low.next return p return None 3.变形型题目​ 有一类题目不会明显的说让解决环的问题，但是使用环来解决，往往会起到意想不到的效果。 例题：编写一个程序，找到两个单链表相交的起始节点。 123456789101112131415161718192021222324252627282930313233343536373839def getIntersectionNode(headA, headB): """ :type head1, head1: ListNode :rtype: ListNode """ if headA==None or headB==None: return None #相判断两个是否相交 pA = headA pB = headB while pA.next: pA = pA.next while pB.next: pB = pB.next if pA!=pB: return None #将PA首尾相接 tail = pA pA.next = headA fast = headB low = headB while True: fast = fast.next.next low = low.next if fast==low: s = headB while s!=low: low = low.next s = s.next tail.next = None return s ​ 这道题利用了和上一道题目完全一样的规律解决]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试-搜索算法总结]]></title>
    <url>%2F2019%2F03%2F09%2F%E6%9C%BA%E8%AF%95-%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1.顺序查找​ 这里不做过多的叙述，就是多次遍历的暴力查找方式。 2.二分查找​ 二分查找是一种只能运用在有序数据上的查找算法，其时间复杂度为O(n^2) 1234567891011def find_data(nums,start,end,target): while start&lt;=end: middle = start+((end-start)&gt;&gt;1) if nums[middle]==target: return middle elif nums[middle]&gt;target: end = middle-1 else: start = middle+1 return -1 拓展：在一些题目中经常出现的中各种各样形式的隐晦的搜索，下面是一道腾讯机试题目。 题目:小Q的父母要出差N天，走之前给小Q留下了M块巧克力。小Q决定每天吃的巧克力数量不少于前一天吃的一半，但是他又不想在父母回来之前的某一天没有巧克力吃，请问他第一天最多能吃多少块巧克力? 答案：这道题直接使用循序查找会导致超时，优化采用二分查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445import sys #m块巧克力，出差n天#第一天最多吃的块数def get_eat_sum(eat_first,n): eat_sum = 0 last_eat = 0 for i in range(n): if last_eat==0: eat = eat_first else: eat = last_eat//2+last_eat%2 eat_sum += eat last_eat = eat return eat_sum def fun(m,n): m = int(m) n = int(n) max_eat = m-n+1 start = 1 end = m-n+1 while start&lt;=end: middle = start+((end-start)&gt;&gt;1) eat_sum = get_eat_sum(middle,n) if eat_sum==m: return middle elif eat_sum&gt;m: end = middle-1 else: start = middle+1 #这里是很关键的一步判断，因为这个问题会出现不一定正好相等，但是有解的情况 if get_eat_sum(middle,n)&lt;m: return middle else: return middle-1 for line in sys.stdin: a = line.split(" ") m,n = a[0],a[1] print(fun(n,m))]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP-transformer模型]]></title>
    <url>%2F2019%2F02%2F28%2FNLP-transformer%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ transformer模型来自于Google的经典论文Attention is all you need，在这篇论文中作者采用Attention来取代了全部的RNN、CNN，实现效果效率的双丰收。 ​ 现在transformer在NLP领域已经可以达到全方位吊打CNN、RNN系列的网络，网络处理时间效率高，结果稳定性可靠性都比传统的CNN、RNN以及二者的联合网络更好，因此现在已经呈现出了transformer逐步取代二者的大趋势。 ​ 下面是三者在下面四个方面的对比试验结果 ​ 1.远距离特征提取能力 ​ 2.语义特征提取能力 ​ 3.综合特征提取能力 ​ 4.特征提取效率 下面是从一系列的论文中获取到的RNN、CNN、Transformer三者的对比结论： ​ 1.从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。 ​ 2.速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。(主流经验上transformer和CNN速度差别不大，RNN比前两者慢3倍到几十倍) Transformer模型具体细节​ transformer模型整体结构上主要Encoder和Decoder两部分组成，Encoder主要用来将数据进行特征提取，而Decoder主要用来实现隐向量解码出新的向量表示(原文中就是新的语言表示)，由于原文是机器翻译问题，而我们要解决的问题是类文本分类问题，因此我们直接减Transformer模型中的Encoder部分来进行特征的提取。其中主要包括下面几个核心技术模块： ​ 1.残差连接 ​ 2.Position-wise前馈网络 ​ 3.多头self-attention ​ 4.位置编码 ​ 1.采用全连接层进行Embedding （Batch_size,src_vocab_size,model_dim） ​ 2.在进行位置编码，位置编码和Embedding的结果进行累加 ​ 3.进入Encoder_layer进行编码处理(相当于特征提取) ​ (1) ​ 1.位置编码（PositionalEncoding）​ 大部分编码器一般都采用RNN系列模型来提取语义相关信息，但是采用RNN系列的模型来进行语序信息进行提取具有不可并行、提取效率慢等显著缺点，本文采用了一种 Positional Embedding方案来对于语序信息进行编码，主要通过正余弦函数， ​ 在偶数位置，使用正弦编码;在奇数位置使用余弦进行编码。 为什么要使用三角函数来进行为之编码？ ​ 首先在上面的公式中可以看出，给定词语的pos可以很简单其表示为dmodel维的向量，也就是说位置编码的每一个位置每一个维度对应了一个波长从2π到10000*2π的等比数列的正弦曲线，也就是说可以表示各个各个位置的绝对位置。 ​ 在另一方面，词语间的相对位置也是非常重要的，这也是选用正余弦函数做位置编码的最主要原因。因为 ​ sin(α+β) = sinαcosβ+cosαsinβ ​ cos(α+β) = cosαcosβ+sinαsinβ ​ 因此对于词汇间位置偏移k，PE(pos+k)可以表示为PE(pos)和PE(k)组合的形式，也就是具有相对位置表达能力 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class PositionalEncoding(nn.Module): """ 位置编码层 """ def __init__(self, d_model, max_seq_len): """ 初始化 Args: d_model: 一个标量。模型的维度，论文默认是512 max_seq_len: 一个标量。文本序列的最大长度 """ super(PositionalEncoding, self).__init__() # 根据论文给的公式，构造出PE矩阵 position_encoding = np.array([ [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)] for pos in range(max_seq_len)]) # 偶数列使用sin，奇数列使用cos position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2]) position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2]) position_encoding = torch.Tensor(position_encoding) # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似 # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐， # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码 pad_row = torch.zeros([1, d_model]) position_encoding = torch.cat((pad_row, position_encoding)) # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码， # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似 self.position_encoding = nn.Embedding(max_seq_len + 1, d_model) self.position_encoding.weight = nn.Parameter(position_encoding, requires_grad=False) def forward(self, input_len,max_len): """ 神经网络的前向传播。 Args: input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。 param max_len:数值，表示当前的词的长度 Returns: 返回这一批序列的位置编码，进行了对齐。 """ # 找出这一批序列的最大长度 tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor # 对每一个序列的位置进行对齐，在原序列位置的后面补上0 # 这里range从1开始也是因为要避开PAD(0)的位置 input_pos = tensor( [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len.tolist()]) return self.position_encoding(input_pos) 2.scaled Dot-Product Attention​ scaled代表着在原来的dot-product Attention的基础上加入了缩放因子1/sqrt(dk)，dk表示Key的维度，默认用64. 为什么要加入缩放因子？ ​ 在dk(key的维度)很大时，点积得到的结果维度很大，使的结果处于softmax函数梯度很小的区域，这是后乘以一个缩放因子，可以缓解这种情况的发生。 ​ Dot-Produc代表乘性attention，attention计算主要分为加性attention和乘性attention两种。加性 Attention 对于输入的隐状态 ht 和输出的隐状态 st直接做 concat 操作，得到 [ht:st] ，乘性 Attention 则是对输入和输出做 dot 操作。 ​ Attention又是什么呢？通俗的解释Attention机制就是通过query和key的相似度确定value的权重。论文中具体的Attention计算公式为： ​ 在这里采用的scaled Dot-Product Attention是self-attention的一种，self-attention是指Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。就是下面来具体说一下K、Q、V具体含义： 在一般的Attention模型中，Query代表要进行和其他各个位置的词做点乘运算来计算相关度的节点，Key代表Query亚进行查询的各个节点，每个Query都要遍历全部的K节点，计算点乘计算相关度，然后经过缩放和softmax进行归一化的到当前Query和各个Key的attention score，然后再使用这些attention score与Value相乘得到attention加权向量 在self-attention模型中，Key、Query、Value均来自相同的输入 在transformer的encoder中的Key、Query、Value都来自encoder上一层的输入，对于第一层encoder layer，他们就是word embedding的输出和positial encoder的加和。 query、key、value来源： ​ 他们三个是由原始的词向量X乘以三个权值不同的嵌入向量Wq、Wk、Wv得到的，三个矩阵尺寸相同 Attention计算步骤： 如上文，将输入单词转化成嵌入向量； 根据嵌入向量得到 q、k、v三个向量； 为每个向量计算一个score： score = q*k 为了梯度的稳定，Transformer使用了score归一化，即除以 sqrt(dk)； 对score施以softmax激活函数； softmax点乘Value值 v ，得到加权的每个输入向量的评分 v； 相加之后得到最终的输出结果Sum(z) ： 。 1234567891011121314151617181920212223242526272829303132333435363738class ScaledDotProductAttention(nn.Module): """ 标准的scaled点乘attention层 """ def __init__(self, attention_dropout=0.0): super(ScaledDotProductAttention, self).__init__() self.dropout = nn.Dropout(attention_dropout) self.softmax = nn.Softmax(dim=2) def forward(self, q, k, v, scale=None, attn_mask=None): """ 前向传播. Args: q: Queries张量，形状为[B, L_q, D_q] k: Keys张量，形状为[B, L_k, D_k] v: Values张量，形状为[B, L_v, D_v]，一般来说就是k scale: 缩放因子，一个浮点标量 attn_mask: Masking张量，形状为[B, L_q, L_k] Returns: 上下文张量和attention张量 """ attention = torch.bmm(q, k.transpose(1, 2)) if scale: attention = attention * scale if attn_mask is not None: # 给需要 mask 的地方设置一个负无穷 attention = attention.masked_fill(attn_mask,-1e9) # 计算softmax attention = self.softmax(attention) # 添加dropout attention = self.dropout(attention) # 和V做点积 context = torch.bmm(attention, v) return context, attention 3.多头Attention​ 论文作者发现将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention 效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。 ​ 这里采用了四个全连接层+有个dot_product_attention层(也可以说是8个)+layer_norm实现。 为什么要使用多头Attention？ ​ 1.”多头机制“能让模型考虑到不同位置的Attention ​ 2.”多头“Attention可以在不同的足空间表达不一样的关联 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MultiHeadAttention(nn.Module): """ 多头Attention层 """ def __init__(self, model_dim=512, num_heads=8, dropout=0.0): super(MultiHeadAttention, self).__init__() self.dim_per_head = model_dim // num_heads self.num_heads = num_heads self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads) self.dot_product_attention = ScaledDotProductAttention(dropout) self.linear_final = nn.Linear(model_dim, model_dim) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, key, value, query, attn_mask=None): # 残差连接 residual = query dim_per_head = self.dim_per_head num_heads = self.num_heads batch_size = key.size(0) # 线性层 (batch_size,word_nums,model_dim) key = self.linear_k(key) value = self.linear_v(value) query = self.linear_q(query) # 将一个切分成多个(batch_size*num_headers,word_nums,word//num_headers) """ 这里用到了一个trick就是将key、value、query值要进行切分不需要进行真正的切分，直接将其维度整合到batch_size上，效果等同于真正的切分。过完scaled dot-product attention 再将其维度恢复即可 """ key = key.view(batch_size * num_heads, -1, dim_per_head) value = value.view(batch_size * num_heads, -1, dim_per_head) query = query.view(batch_size * num_heads, -1, dim_per_head) #将mask也复制多份和key、value、query相匹配 （batch_size*num_headers,word_nums_k,word_nums_q） if attn_mask is not None: attn_mask = attn_mask.repeat(num_heads, 1, 1) # 使用scaled-dot attention来进行向量表达 #context:(batch_size*num_headers,word_nums,word//num_headers) #attention:(batch_size*num_headers,word_nums_k,word_nums_q) scale = (key.size(-1)) ** -0.5 context, attention = self.dot_product_attention( query, key, value, scale, attn_mask) # concat heads context = context.view(batch_size, -1, dim_per_head * num_heads) # final linear projection output = self.linear_final(context) # dropout output = self.dropout(output) # 这里使用了残差连接和LN output = self.layer_norm(residual + output) return output, attention 4.残差连接​ 在上面的多头的Attnetion中，还采用了残差连接机制来保证网络深度过深从而导致的精度下降问题。这里的思想主要来源于深度残差网络(ResNet)，残差连接指在模型通过一层将结果输入到下一层时也同时直接将不通过该层的结果一同输入到下一层，从而达到解决网络深度过深时出现精确率不升反降的情况。 为什么残差连接可以在网络很深的时候防止出现加深深度而精确率下降的情况？ ​ 神经网络随着深度的加深会会出现训练集loss逐渐下贱，趋于饱和，然后你再加深网络深度，训练集loss不降反升的情况。这是因为随着网络深度的增加，在深层的有效信息可能变得更加模糊，不利于最终的决策网络做出正确的决策，因此残差网络提出，建立残差连接的方式来将低层的信息也能传到高层，因此这样实现的深层网络至少不会比浅层网络差。 5.Layer normalizationNormalization​ Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 #####Batch Normalization(BN) ​ 应用最广泛的Normalization就是Batch Normalization，其主要思想是:在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 Layer normalization(LN)​ LN 是在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差. Layer normalization在pytorch 0.4版本以后可以直接使用nn.LayerNorm进行调用 6.Mask​ mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。 ​ 在我们使用的Encoder部分，只是用了padding mask因此本文重点介绍padding mask。 padding mask​ 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。 123456789101112131415def padding_mask(seq_k, seq_q): """ param seq_q:(batch_size,word_nums_q) param seq_k:(batch_size,word_nums_k) return padding_mask:(batch_size,word_nums_q,word_nums_k) """ # seq_k和seq_q 的形状都是 (batch_size,word_nums_k) len_q = seq_q.size(1) # 找到被pad填充为0的位置(batch_size,word_nums_k) pad_mask = seq_k.eq(0) #(batch_size,word_nums_q,word_nums_k) pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1) # shape [B, L_q, L_k] return pad_mask p ####3.Position-wise 前馈网络 ​ 这是一个全连接网络，包含两个线性变换和一个非线性函数(实际上就是 ReLU) ​ 这里实现上用到了两个一维卷积。 1234567891011121314151617181920class PositionalWiseFeedForward(nn.Module): &quot;&quot;&quot; 前向编码，使用两层一维卷积层实现 &quot;&quot;&quot; def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0): super(PositionalWiseFeedForward, self).__init__() self.w1 = nn.Conv1d(model_dim, ffn_dim, 1) self.w2 = nn.Conv1d(ffn_dim, model_dim, 1) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, x): output = x.transpose(1, 2) output = self.w2(F.relu(self.w1(output))) output = self.dropout(output.transpose(1, 2)) # add residual and norm layer output = self.layer_norm(x + output) return output]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试总结-Python和C语言中的一些不同]]></title>
    <url>%2F2019%2F02%2F27%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93-Python%E5%92%8CC%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%8D%E5%90%8C%2F</url>
    <content type="text"><![CDATA[###1.python在除法和C语言中的一点区别 ​ 在Python3中，除法有 “/” 以及 “//” 两种，这两个有着明显的区别，具体区别看代码： 12print(12//10)print(12/10) 这两行代码的输出如下： 1211.2 当被除数是负数的时候，又是另一种情况： 12345678print(-12/10) #不补整print(int(-12/10)) #向正方向进行补整print(-13//10) #向负方向进行补整output: -1.2 -1 -2 ​ 因此，综合前面的正负两种情况，我们可以看出当我们想要达到和C++同样的向上取整，只能使用int(a/b)方式。 2.python在求余时和C的一点区别​ 对于正数求余运算，python和C++完全相同，但是对于负数求余运算，python和C++存在着较大的差别，下面我们通过例子来说明二者的差别。 123456789#C++count&gt;&gt;-123%10;output: -3#pythonprint(-123%10)output: -7 #这里是向下取10的余数 ​ 为了实现和C++相同效果的取余运算，我们只能采用如下方式进行取余运算 1234if a&gt;=0 print(a%10)else: print(a%-10)]]></content>
      <tags>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机试——排序算法总结]]></title>
    <url>%2F2019%2F02%2F10%2F%E6%9C%BA%E8%AF%95%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[​ 机试中，排序算法是主要面临的一类算法，很久都没有接触机试的题了，解决的时候感觉有点思路不是很清楚了，因此写了这一片博客，来整理下常见的排序算法以及各种常见算法的效率稳定性等特点。 在机试中常用的排序算法主要包含下面几种： ​ 1.插入排序 ​ 2.选择排序 ​ 3.快速排序(最常用的排序) ​ 4.冒泡排序 ​ 5.归并排序 ​ 6.桶排序 下面我将具体介绍各种排序算法： 1.插入排序​ 每次从头到尾选择一个元素，并且将这个元素和整个数组中的所有已经排序的元素进行比较，然后插入到合适的位置。 ​ 注意：插入排序的核心点就是两两比较时从后向前进行比较，如果比插入值大，那么将其向后移动，直到找到比插入值小的。 12345678910def insertion_sort(arr): length = len(arr) for i in range(1,length): #从第一个元素开始依次进行排序 tmp = arr[i] j = i while arr[j-1]&gt;tmp and j&gt;0: #从当前元素从后向前向前开始遍历，寻找第一个比当前元素更小的元素 arr[j] = arr[j-1] #再找比当前小的元素位置的同时，只要扫描到的位置比当前元素大，那么将该元素后移一维 j -= 1 arr[j] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 为什么插入排序是稳定的排序算法？ ​ 当前从头到尾选择元素进行排序时，当选择到第i个元素时，前i-1个元素已经排好了续，取出第i个元素，从i-1开始向前开始比较，如果小于，则将该位置元素向后移动，继续先前的比较，如果不小于，那么将第i个元素放在当前比较的元素之后。 2.选择排序​ 选择排序主要采用了从头到尾依次确定各个位置的方式来进行排序，首先遍历一次整个数组，如果遇到比第一个元素小的元素那么交换位置，一次遍历完成那么第一个位置就已经是整个数组中最小的元素了，经过n次遍历，确定全部位置的元素。 123456789def selection_sort(arr): length = len(arr) for i in range(length): for j in range(i,length): if arr[i]&gt;arr[j]: tmp = arr[i] arr[i] = arr[j] arr[j] = tmp return arr 稳定性：不稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 3.冒泡排序​ 冒泡排序额是实现是不停地进行两两比较，将较大的元素换到右侧，然后继续进行两两比较，直到比较完全全部元素，每进行完一轮两两比较，确定一个元素的位置。例如：第一轮两两比较确定最大的值，第二轮比较确定次大元素。 1234567891011def bubble_sort(arr): length = len(arr) for i in range(0,length): for j in range(1,length-i): if arr[j]&lt;arr[j-1]: tmp = arr[j] arr[j] = arr[j-1] arr[j-1] = tmp return arr 稳定性：稳定 时间复杂度：O(n^2) 空间复杂度：O(1) 冒泡排序在原始冒泡排序算法的基础上还能做哪些优化？ ​ 1.设置是否已经排好序的flag。如果在某一轮的便利中没有出现任何的交换发生，这说明已经都排好序,那么直接将flag置True，每轮结束时检测flag，如果为True则直接返回 ​ 2.某一轮的结束为止为j，但这一轮最后一次交换发生在lastSwap位置，那么说明lastSwap到j之间已经排好序，下次遍历的结束点就不需要再到j—而是直接到lastSwap即可。 4.快速排序​ 快速排序的的主要思想是先找到一个任意一个元素作为基准元素pivot（一般都采用第一个元素作为基准），然后从右向左搜索，如果发现比pivot小，那么和pivot交换,然后从右向左进行搜索，如果发现比pviot大，那么进行交换，遍历一轮后pivot左边的元素都比它小，右边的元素都比他大，此时pivot的位置就是排好序后他也应该在的位置。然后继续用递归算法分别处理pivot左边的元素和右边的元素。 对于大的乱序数据快速排序被认为是最快速的排序方式d 123456789101112131415161718192021222324252627282930313233#方式一：递归def quick_sort(arr,l,r): if(l&lt;r): q = mpartition(arr,l,r) quick_sort(arr,l,q-1) #前面经过一次mpartion后q位置已经排好序，因此递归时两部分跳过q位置 quick_sort(arr,q+1,r) return arr def mpartition(arr,l,r): """ 递归子函数，povit放到指定位置 return l:最终标志元素被放置的位置，本轮确定了的元素位置 """ poviot = arr[l] while l&lt;r: while l&lt;r and arr[r]&gt;=poviot: r -= 1 if l&lt;r: arr[l] = arr[r] l += 1 while l&lt;r and arr[l]&lt;poviot: l += 1 if l&lt;r: arr[r] = arr[l] r -= 1 arr[l] = poviot return l 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#方式二：非递归，利用栈def partition(nums,low,high): #确定nums数组中指定部分low元素的位置，左边都比它小，右边都比他大 pivot = nums[low] high_flag = True #这里之所以设置这两个flag是为了确保交叉进行，否则可能会出现最大索引值处没有值或者最大索引值处一直付给各个low low_flag = False while low&lt;high and low&lt;len(nums) and high&lt;len(nums): if high_flag: if nums[high]&lt;pivot: nums[low]=nums[high] high_flag = False low_flag = True else: high -= 1 if low_flag: if nums[low]&gt;pivot: nums[high] = nums[low] low_flag = False high_flag = True else: low += 1 nums[low] = pivot return low def quick_sort(nums): low = 0 high = len(nums)-1 stack = [] #存储每次遍历起始索引和结束索引 if low&lt;high: #先手动将找到第一个节点的最终位置，将原数组分为左右两个数组，分别左右索引入栈 mid = partition(nums,low,high) if low&lt;mid-1: stack.append(low) stack.append(mid-1) if high&gt;mid+1: stack.append(mid+1) stack.append(high) #取出之前入栈的一个数组，来进行确定最终位置，分为左右两个子数组，分别左右索引入栈的操作，重复直到所有元素都已经排好序 while stack: #这里写的是属于右半部都排好后左半部 r = stack.pop() l = stack.pop() mid = partition(nums,l,r) if l&lt;mid-1: stack.append(l) stack.append(mid-1) if r&gt;mid+1: stack.append(mid+1) stack.append(r) return nums 稳定性：不稳定（排序过程中不停地交换元素位置造成了排序算法不稳定） 时间复杂度： ​ 平均时间O(nlogn) ​ 最坏情况：O(n^2) 空间复杂度：O(nlogn) 5.归并排序​]]></content>
      <tags>
        <tag>机试</tag>
        <tag>算法总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——Attion相关]]></title>
    <url>%2F2019%2F01%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Attion%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[通过encoder，把 映射为一个隐层状态 ，再经由decoder将 映射为 。这里精妙的地方就在于，Y中的每一个元素都与H中的所有元素相连，而每个元素通过不同的权值给与Y不同的贡献。 1.关键在于红框里面的部分，即attention，后面再讲。 2.红框下面是encoder，输入 ，通过一个双向LSTM得到两组和 （向左和向右），然后concat起来得到最终的 。 3.红框上面是decoder。以 时刻为例，输入共有三个： ， ， 。 ​ 其中,是上一个时刻的hidden state（一般用 表示encoder的hidden state，用 表示decoder的hidden state）； ​ 是上一个时刻的输出，用来当做这个时刻的输入； ​ 在图中没有，就是红框里得到的加权和，叫做context，即由所有的encoder output（即 ，不定长）得到一个定长的向量，代表输入序列的全局信息，作为当前decoder step的context（上下文）。 ​ 计算方法为： ，其中是权重，又称为alignment； 就是encoder所有step的hidden state，又叫做value或者memory； 代表decoder step， 代表encoder step。 那么权重矩阵又是怎么来的呢？ ​ 其中 其实上面的公式就是对e的求softmax，因为我们要计算一组权重，而这组权重的和为1；下面的e的表达式就是代表着对 和 的相关程度进行计算的函数，即对于某个既定的decoder step，计算上个时刻的hidden state和所有encoder step的输出的相关程度，并且用softmax归一化；这样，相关度大的 权重就大，在整个context里占得比重就多，decoder在当前step做解码的时候就越重视它（这就是attention的思想）。 具体的相关程度的计算，主要实现方式 对 做一个线性映射，得到的向量叫做query，记做 ； 对 做一个线性映射，得到的向量叫做key，记做 ； 。和的维度必须相同，记为 ； 是一个 的向量。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python进阶-面向对象编程]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.__slots__ ​ 用于指定class 实例能够指定的属性 注意：__slots__只对当前类起作用，对其子类无效 12345678910import tracebackclass Myclass(object): __slots__ = ["name","set_name] s = MyClass()s.name = "john" #这里可以进行正常的赋值，因为包含在__slots__中try: s.age = 2 #这里不能进行正常赋值except AttributeError: traceback.print_exc() Output: 2.@property属性 ​ @property 可以实现比较方便的属性set、get设置 1.使用@property相当于讲将一个函数变为get某个属性值2.@属性名称.setter可以实现设置一个属性的set条件 ​ 使用上面的两种修饰符，可以实现 ​ 1.对写入属性的限制，只有符合规范的才允许写入 ​ 2.设置只读属性，只能够读取，不能写入，只能从其他属性处计算出 下面的就是对score属性的写操作进行了一些限制，将double_score属性设置为只读属性 123456789101112131415161718192021222324252627282930313233343536373839404142class MyClass(object): @property def score(self): return self._score @score.setter def score(self,value): #不是int类型时引发异常 if not isinstance(value,int): raise ValueError("not int") #raise的作用是显示的引发异常 #超出范围时引发异常 elif (value&lt;0) or (value&gt;100): raise ValueError("score must in 0 to 100") self._score = value @property def double_score(self): return self._score*2 s = MyClass()s.score = 3print(s.score)try: s.score = 2300except ValueError: traceback.print_exc() try: s.score = "dfsd"except ValueError: traceback.print_exc() print(s.double_score)try: s.double_score = 2except Exception: traceback.print_exc() 描述器，主要是用来读写删除类的行为 函数可以直接使用__name__属性来获取函数名称 1234567def now(): print("2012")print(now.__name__)output: "now" ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[python进阶-生成器和迭代器]]></title>
    <url>%2F2019%2F01%2F08%2Fpython%E8%BF%9B%E9%98%B6-%E7%94%9F%E6%88%90%E5%99%A8%E5%92%8C%E8%BF%AD%E4%BB%A3%E5%99%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pythonic程序规范化]]></title>
    <url>%2F2019%2F01%2F07%2Fpythonic%E7%A8%8B%E5%BA%8F%E8%A7%84%E8%8C%83%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.常量名称全部大写 2.代码长度一行不能超过80字符 3.类名使用驼峰式命名，一般不超过3 4.函数名称]]></content>
  </entry>
  <entry>
    <title><![CDATA[python2和python3的不同点]]></title>
    <url>%2F2019%2F01%2F07%2Fpython2%E5%92%8Cpython3%E7%9A%84%E4%B8%8D%E5%90%8C%E7%82%B9%2F</url>
    <content type="text"><![CDATA[​ 因为系统移植过程中一直出现python3程序向python2转化的问题，因此这里记录下我在程序移植过程中遇到过的坑。 1.python2和python3的url编码解码函数接口 2.python2和python3向文件中写入中文时指定编码方式 ​ 对于python3来说，要在写入文件时指定编码方式是十分简单的，只需要下面的方式即可： 12with open(filename,'a',encoding='utf-8') as f: f.write("中文") ​ 但对于python2，要在写入文件时,手动添加utf-8文件的前缀 123456import sysreload(sys)sys.setdefaultencoding('utf-8')with open(r'd:\sss.txt','w') as f: f.write(unicode("\xEF\xBB\xBF", "utf-8"))#函数将\xEF\xBB\xBF写到文件开头，指示文件为UTF-8编码。 f.write(u'中文') 3.python2和python3外置函数区别 在python3中外置函数文件可以直接进行调用，如在下面的文件结构 12|--main.py tools—— 在python2中外置函数文件目录下必须要有init.py 空文件，否则无法进行加载 12 4.python2和python3文件中中文问题 ​ 在python3中，输出和备注等一切位置都可以直接使用中文，不需要任何额外的代码，在python2中，必须要在包含中文的python文件中加入 1#coding=utf-8 ​ 才能出现中文，否则报错。]]></content>
      <categories>
        <category>python 进阶操作</category>
      </categories>
      <tags>
        <tag>python进阶操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch建模基本操作]]></title>
    <url>%2F2019%2F01%2F02%2Fpytorch%E5%BB%BA%E6%A8%A1%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.查看模型结构 12]]></content>
  </entry>
  <entry>
    <title><![CDATA[python不显示警告信息设置]]></title>
    <url>%2F2019%2F01%2F02%2Fpython%E4%B8%8D%E6%98%BE%E7%A4%BA%E8%AD%A6%E5%91%8A%E4%BF%A1%E6%81%AF%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[再使用python处理多线程或者循环次数较多时，常常会因为系统爆出一些警告信息而影响结果的查看，比如下面的警告： 十分影响美观，造成结果混乱，很难找到有效的信息，下面我们使用python自带的warning设置，设置过滤warn级别的告警 12import warningswarnnings.filterwarnings("ignore") 结果变为：]]></content>
      <tags>
        <tag>python进阶操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2018%2F12%2F30%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ python多进程之前一直在在写一些小程序，这次正好需要写一个比较正式的多进行处理，正好系统的总结下python多进行的一些核心知识。 ​ 首先python中常用的提高效率的方式主要主要包括多线程、多进程两种，但是在python中的多线程并不是正正的多线程，只有在网络请求密集型的程序中才能有效的提升效率，在计算密集型、IO密集型都不是很work甚至会造成效率下降，因此提升效率的方式就主要以多进程为主。 ​ python中多进程需要使用python自带包multiprocessing，multiprocessing支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件 1from multiprocessing import Lock,Value,Queue,Pool 创建进程类1.单个进程类的创建​ 1.创建单进程 ​ 12 ####使用进程池创建多进程 ​ Pool类可以提供指定数量的进程供用户调用，当有新的请求提交到Pool中时，如果池还没有满，就会创建一个新的进程来执行请求。如果池满，请求就会告知先等待，直到池中有进程结束，才会创建新的进程来执行这些请求。 ​ 常用方法： ​ 1.apply ​ 用于传递不定参数，同python中的apply函数一致，主进程会被阻塞直到函数执行结束（不建议使用，并且3.x以后不在出现）。 ​ 2.apply_async ​ 和apply类似，非阻塞版本，支持结果返回后进行回调 ​ 3.map ​ 函数原型:map(func, iterable[, chunksize=None]) ​ Pool中的map和python内置的map用法基本上一致，会阻塞直到返回结果 ​ 4.map_async ​ 函数原型：map_async(func, iterable[, chunksize[, callback]]) ​ 和map用法一致，但是它是非阻塞的 ​ 5.close ​ 关闭进程池，使其不再接受新的任务 ​ 6.terminal ​ 结束工作进程，不再处理未处理的任务 ​ 7.join ​ 主进程阻塞等待子进程退出，join方法要在close或terminal方法后面使用 ​ 1234from mutiprocess import Poolprocess_nums = 20pool = Pool(process_nums) 使用Lock来避免冲突​ lock主要用于多个进程之间共享资源时，避免资源访问冲突，主要包括下面两个操作： ​ 1.look.acquire() 获得锁 ​ 2.lock.release() 释放锁 ​ 下面是不加锁时的程序： 1234567891011121314151617import multiprocessingimport timedef add(number,value,lock): print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 12345678910111213main endinit add1 number = 0init add3 number = 0add1 number = 1add3 number = 3add1 number = 2add3 number = 6add1 number = 3add3 number = 9add1 number = 4add3 number = 12add1 number = 5add3 number = 15 ​ 两个进程交替的来对number进行加操作，下面是加锁后的程序： 1234567891011121314151617181920212223import multiprocessingimport timedef add(number,value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in xrange(1, 6): number += value time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(value, number)) except Exception as e: raise e finally: lock.release()if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 结果为： 1234567891011121314main endinit add1 number = 0 #add1优先抢到锁，优先执行add1 number = 1add1 number = 2add1 number = 3add1 number = 4add1 number = 5init add3 number = 0 #add3被阻塞，等待add1执行完成，释放锁后执行add3add3 number = 3add3 number = 6add3 number = 9add3 number = 12add3 number = 15#注意观察上面add3部分，虽然在add1部分已经将number加到了5，但是由于number变量只是普通变量，不能在各个进程之间进行共享，因此add3开始还要从0开始加 ####使用Value和Array来进行内存之中的共享通信 ​ 一般的变量在进程之间是没法进行通讯的，multiprocessing 给我们提供了 Value 和 Array 模块，他们可以在不通的进程中共同使用。 ​ 1.Value多进程共享变量 ​ 将前面加锁的程序中的变量使用multiprocessing提供的共享变量来进行 123456789101112131415161718192021222324import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release() if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print ("main end") ​ 输出结果为： 123456789101112131415161718192021222324#add3开始时是在add1的基础上来进行加的main endinit add1 number = 0***************add1 has added***********add1 number = 1***************add1 has added***********add1 number = 2***************add1 has added***********add1 number = 3***************add1 has added***********add1 number = 4***************add1 has added***********add1 number = 5init add3 number = 5***************add3 has added***********add3 number = 8***************add3 has added***********add3 number = 11***************add3 has added***********add3 number = 14***************add3 has added***********add3 number = 17***************add3 has added***********add3 number = 20 ​ 2.Array实现多进程共享内存变量 12345678910111213141516171819202122232425262728293031323334import multiprocessingimport timedef add(number,add_value,lock): lock.acquire() try: print ("init add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) for i in xrange(1, 6): number.value += add_value print ("***************add&#123;0&#125; has added***********".format(add_value)) time.sleep(1) print ("add&#123;0&#125; number = &#123;1&#125;".format(add_value, number.value)) except Exception as e: raise e finally: lock.release()def change(arr): for i in range(len(arr)): arr[i] = -arr[i] if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) arr = multiprocessing.Array('i', range(10)) print (arr[:]) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p3 = multiprocessing.Process(target=change,args=(arr,)) p1.start() p2.start() p3.start() p3.join() print (arr[:]) print ("main end") ​ 输出结果为： 12345678910111213141516171819202122232425[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]init add3 number = 0***************add3 has added***********[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]main endadd3 number = 3***************add3 has added***********add3 number = 6***************add3 has added***********add3 number = 9***************add3 has added***********add3 number = 12***************add3 has added***********add3 number = 15init add1 number = 15***************add1 has added***********add1 number = 16***************add1 has added***********add1 number = 17***************add1 has added***********add1 number = 18***************add1 has added***********add1 number = 19***************add1 has added***********add1 number = 20 ####使用Queue来实现多进程之间的数据传递 ​ Queue是多进程安全队列，可以使用Queue来实现进程之间的数据传递，使用的方式： 1.put 将数据插入到队列中 ​ 包括两个可选参数：blocked和timeout ​ (1)如果blocked为True（默认为True）,并且timeout为正值，该方法会阻塞队列指定时间，直到队列有剩余，如果超时，会抛出Queue.Full ​ (2)如果blocked为False，且队列已满，那么立刻抛出Queue.Full异常 2.get 从队列中读取并删除一个元素 ​ 包括两个可选参数:block和timeout ​ （1）blocked为True，并且timeout为正值，那么在等待时间结束后还没有取到元素，那么会抛出Queue.Empty异常 ​ （2）blocked为False，那么对列为空时直接抛出Queue.Empty异常]]></content>
      <categories>
        <category>python进阶操作</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop使用]]></title>
    <url>%2F2018%2F12%2F25%2Fhadoop%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.查看hadoop某个目录下的文件 1sudo hadoop fs -ls path 2.从hdfs上下拉文件到本地 1sudo hdfs fs -get file 3.获取部署在docker中的hadoop的挂载信息等元数据 1sudo docker inspect hdp-server]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题之Cpp基础知识回顾]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%88%B7%E9%A2%98%E4%B9%8BCpp%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[由于C++已经多 1.动态数组的声明]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题心得]]></title>
    <url>%2F2018%2F12%2F23%2F%E5%88%B7%E9%A2%98%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[1.在只需要考虑是不是存在元素的个数，只可考虑是否存在的情况下，先将list装换成set可以非常有效的提升计算效率 2.对Int类型数值的范围要保持敏感​ Int类型数值范围为 ​ Max 0x7fffffff 2^31-1 2147483647 ​ Min 0x80000000 2^31 -2147483648 ​ 注意：负数的范围会比正数的范围大一，这按需要特别注意 3.常见数学问题要考虑的情况​ 1.是否有负数 ​ 2.是否有小数 ​ 3.是否考虑错误输入？如何进行处理 ​ 4.数据范围极端值 ​ 5.0或空如何进行处理 ​ .]]></content>
  </entry>
  <entry>
    <title><![CDATA[刷题记录]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[#####1.除自身以外的数组乘积 给定长度为 n 的整数数组 nums，其中 n &gt; 1，返回输出数组 output ，其中 output[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。 示例: 12输入: [1,2,3,4]输出: [24,12,8,6] 说明：不能使用除法，在O(n)时间复杂度内解决此问题 解决思路： ​ 可以使用先从左到右进行遍历，记录每个位置左面的元素相乘获得的值，存储在output的对应位置，再从右到左进行遍历，记录每个位置右侧的元素乘积，再和output中已经存储的该位置左侧的元素乘积相乘，就可以得到最终结果，时间复杂度为O(n) 12345678910111213141516171819202122232425class Solution: def productExceptSelf(self, nums): """ 完美解 :type nums: List[int] :rtype: List[int] """ left = 1 right = 1 len_nums = len(nums) output = [0]*len_nums #从左到右进行一次遍历，在output中对应位置记录该值左面的元素乘积 for i in range(0,len_nums): output[i] = left left = left*nums[i] #从右到左进行一次遍历，记录每个值右面元素的乘积，和output中已经进行存储的左面乘积相乘，得到各个位置最终的结果 for j in range(len_nums-1,-1,-1): output[j] *= right right *= nums[j] return output 2.缺失数字 给定一个包含 0, 1, 2, ..., n 中 n 个数的序列，找出 0 .. n 中没有出现在序列中的那个数。 示例 1: 12输入: [3,0,1]输出: 2 示例 2: 12输入: [9,6,4,2,3,5,7,0,1]输出: 8 说明:你的算法应具有线性时间复杂度。你能否仅使用额外常数空间来实现? 思路一：最常见的思路应该是先排序，然后顺序遍历，对不上则为缺失位置 1234567891011class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ for key,value in emumerate(nums): if key!=value: return key else: return key+1 思路二：另一种比较巧妙地思路就是直接利用数学的方法来解决这个问题，仔细研究题目，我们可以发现题目中所给的nums数组内所有元素的加和可以看做等差数列的加和减去缺失数，因此我们可以直接计算等差数列的加和(n*(n-1))/2，然后减去nums数组的加和，二者相减即为缺失的数. 1234567class Solution: def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ return (int(len(nums)*(len(nums)-1)/2)- sum(nums)) 爬楼梯问题（动态规划）假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 12345输入： 2输出： 2解释： 有两种方法可以爬到楼顶。1. 1 阶 + 1 阶2. 2 阶 示例 2： 123456输入： 3输出： 3解释： 有三种方法可以爬到楼顶。1. 1 阶 + 1 阶 + 1 阶2. 1 阶 + 2 阶3. 2 阶 + 1 阶 解题思路：首先经过题目分析我们最自然的可以想到，要想问到第n层楼梯的走法，那么一定为到第n-1和第n-2层楼梯走法之和，因此我们可以清楚地可以看出这是一道递归问题。即n(i) = n(i-1)+n(i-2) 1234567891011121314class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; def f(n): if n==0|n==1: return 1 else: return f(n-1)+f(n-2) if n&gt;=2: return f(n) return 1 转换成非递归问题（其实本质就是讲递归问题由系统储存的信息改为程序储存，从而改编程序的运行方式，提高程序的运行效率） 12345678910class Solution(object): def climbStairs(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; way = [0,1,2] for i in range(3,n+1): way.append(way[i-1]+way[i-2]) return way[n]]]></content>
      <categories>
        <category>机试</category>
      </categories>
      <tags>
        <tag>机试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化部署pyspark程序记录]]></title>
    <url>%2F2018%2F12%2F07%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2pyspark%E7%A8%8B%E5%BA%8F%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[​ 项目要求需要在pyspark的集群中将一部分程序做集群的自动化部署，本文记录了程序部署过程中，使用到的一些技术以及遇到的一些问题。 ####1.SparkSession创建时设置不生效 ​ 首先，要进行程序的自动化部署首先要将程序封装成python文件，在这个过程中可能会出现sparkSession谁知不能生效的问题，不论对SparkSession进行什么设置，都不会生生效 这种问题是由于SparkSession的创建过程不能写在主程序中，必须要写在所有函数的外层，并且进行的在文件的初始部分穿创建 2.python 文件传入获取参数​ python文件也可以和shell脚本一样进行运行时传入参数，这里主要使用的的是python自带的sys和getopt包 1234567891011要接受参数的python文件：import sysimport getoptopts,args = getopt.getopt(sys.argv[1:],"d:",["d:"])for opt,arg in opts: if opt in ("-d","--d"): input_file = arg#后续可以直接使用input——file获取的变量名进行操作 ####3.将python文件执行封装到shell脚本中 ​ 这里之所以将python文件进行封装主要是为了方便移植，其实也可以直接设置将python脚本文件执行设置成定时任务，这里是一波瞎操作。主要为了练习和方便移植 123456789101112#首先在这个shell重要实现获取当前日期或前n天的日期date = `date -d "1 days ago"+%Y-%m-%d`#然后在将date作为参数后台执行这个程序并且生成日志python ***.py -d date &gt; /path/$&#123;date&#125;.log 2&gt;&amp;1 &amp;#=====================注意==============================#上面直接使用python执行时可能会出现系统中存在多个python导致部署时使用的python和之前测试使用的python不是一个python环境导致的，那么如何确定测试时使用的python环境呢？#要解决上述问题可以先从新进入到测试用的python环境，然后进行下面操作import sysprint(sys.execyutable)#然后将python目录改为上面的python目录]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习训练基本经验]]></title>
    <url>%2F2018%2F11%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E5%9F%BA%E6%9C%AC%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[1.在各个隐藏层和激活函数之间加入Batch Normalization层可以大大缩短训练时间，而且还存在隐藏效果，比如出现还可以改善效果。 调用： ​ Normalization(num_features) 参数设置： ​ CNN后接Batch Normalization: nums_feeatures为CNN感受野个数(即输出深度) ​ 全连接层后接Batch Normalization：num_features为输出的特征个数 2.Batch Normalization和Dropout层不要一起使用，因为BN以及具备了dropout的效果，一起使用不但起不到效果，而且会产生副作用 常见副作用: ​ 1.只能使用特别小的速率进行训练，使用较大的速率进行训练时，出现梯度消失，无法进行下降 ​ 2.最终与训练集拟合程度不高，例如与训练集的拟合程度只能达到90% 若一定要将dropout和BN一起使用，那么可以采用下面方式： ​ 1.将dropout放在BN后面进行使用 ​ 2.修改Dropout公式(如高斯Dropout)，使其对对方差不那么敏感 总体思路:降低方差偏移 3.深度学习不收敛问题 ​ 1.最常见的原因可能是由于学习速率设置的过大，这种情况一般先准确率不断上升，然后就开始震荡 ​ 2.当训练样本较小，而向量空间较大时，也可能会产生不收敛问题，这种情况一般从一开始就开始震荡，机会没有准确率上升的过程 ​ 3.训练网络问题。当面对的问题比较复杂，而使用的网络较浅时，可能会产生无法收敛问题 ​ 4.数据没有进行归一化。数据输入模型之前如果没有进行归一化，很有可能会产生收敛慢或者无法进行收敛的问题 注意：收敛与否主要是看损失函数是否还在下降，而不是准确率是否还在上升，存在很多情况损失函数在迭代过程中还是在不断地下降，但是准确率基本上处于停滞状态，这种情况也是一种未完全拟合的表现，经过一段时间损失函数的下降后准确率还可能会迎来较大的提升]]></content>
  </entry>
  <entry>
    <title><![CDATA[Normalization]]></title>
    <url>%2F2018%2F11%2F26%2FNormalization%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pyspark-spark.ml.linalg包]]></title>
    <url>%2F2018%2F11%2F21%2Fpyspark-spark-ml-linalg%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pyspark 的pyspark.ml.linalg包主要提供了向量相关(矩阵部分不是很常用因此本文不提)的定义以及计算操作 主体包括： ​ 1.Vector ​ 2.DenseVector ​ 3.SparseVector 1.Vector​ 是下面所有向量类型的父类。我们使用numpy数组进行存储，计算将交给底层的numpy数组。 主要方法： ​ toArray() 将向量转化为numpy的array ###2.DenseVector ​ 创建时，可以使用list、numpy array、等多种方式进行创建 常用方法： ​ dot() 计算点乘,支持密集向量和numpy array、list、SparseVector、SciPy Sparse相乘 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 12345678910111213141516171819202122232425262728293031323334#密集矩阵的创建v = Vectors.dense([1.0, 2.0])u = DenseVector([3.0, 4.0])#密集矩阵计算v + uoutput: DenseVector([4.0, 6.0]) #点乘v.dot(v) #密集向量和密集向量之间进行点乘output: 5.0v.dot(numpy.array([1,2])) #使用密集向量直接和numpy array进行计算output: 5.0 #计算非零元素个数DenseVector([1,2,0]).numNonzeros()#计算两个元素之间的平方距离a = DenseVector([0,0])b = DenseVector([3,4])a.squared_distance(b)output: 25.0 #密集矩阵转numpy arrayv = v.toArray()voutput: array([1., 2.]) 3.SparseVector​ 简单的系数向量类，用于将数据输送给ml模型。 ​ Sparkvector和一般的scipy稀疏向量不太一样，其表示方式为，（数据总维数，该数据第n维存在值列表，各个位置对应的值列表） 常用方法： ​ dot() SparseVector的点乘不仅可以在SparseVector之间还可以与numpy array相乘 ​ indices 有值的条目对应的索引列表，返回值为numpy array ​ size 向量维度 ​ norm() 计算范数 ​ numNonzeros() 计算非零元素个数 ​ squared_distance() 计算两个元素的平方距离 ​ .toArray() 转换为numpy array ​ values 返回一个list 注：加粗部分为SparseVector特有的 1234567891011121314151617181920#创建稀疏向量a = SparseVector(4, [1, 3], [3.0, 4.0])a.toArray()output: array([0., 3., 0., 4.])#计算点乘a.dot(array([1., 2., 3., 4.]))output: 22.0 #获得存值得对应的索引列表a.indicesoutput: array([1, 3], dtype=int32)#获取向量维度a.sizeoutput: 4]]></content>
      <tags>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark-向量化技术]]></title>
    <url>%2F2018%2F11%2F20%2Fpyspark-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[​ 在pyspark中文本的向量化技术主要在包pyspark.ml.feature中，主要包括以下几种： 1.Ngram 2.tf-idf 3.Word2Vec 1.Ngram​ 2.tf-idf​ 在pyspark中tf和idf是分开的两个步骤 ​ （1）tf ​ 整个tf的过程就是一个将将各个文本进行计算频数统计的过程，使用前要先使用也定的算法来对语句进行分词，然后指定统计特征的数量再进行tf统计 ​ 常用参数： ​ 1.numsFeatures 统计的特征数量，这个值一般通过ParamGridBuilder尝试得出最合适的值 ​ 2.inputCol 输入列，输入类列为ArrayType的数据 ​ 3.outputCol 输出列 ,输出列为Vector类型的数据 123456789101112df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])hashingTF = HashingTF(numFeatures=10, inputCol="words", outputCol="tf")hashingTF.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+|words |tf |+-----------------------------------+--------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|+-----------------------------------+--------------------------------+ ​ 其中，10代表了特征数，[1,3]代表了this和is对应的哈希值，[2.0,1.0]代表了this和is出现的频数. ​ (2)idf 常用参数： ​ 1.minDocFreq 最少要出现的频数，如果超过minDocFreq个样本中出现了这个关键词，这个频数将不tf-idf特征，直接为0 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 123456789101112idf = IDF(inputCol="tf",outputCol="tf-idf")idf_model = idf.fit(df)idf_model.transform(df).show(10,False)output:+-----------------------------------+--------------------------------+--------------------------------------------------------------+|words |tf |tf-idf |+-----------------------------------+--------------------------------+--------------------------------------------------------------+|[this, is, apple] |(10,[1,3],[2.0,1.0]) |(10,[1,3],[0.0,0.0]) ||[this, is, apple, watch, or, apple]|(10,[1,2,3,7],[3.0,1.0,1.0,1.0])|(10,[1,2,3,7],[0.0,0.4054651081081644,0.0,0.4054651081081644])|+-----------------------------------+--------------------------------+--------------------------------------------------------------+ 3.CountVec​ CountVec是一种直接进行文本向量，直接词频统计的向量化方式，可以 常用参数包括： ​ minDF：要保证出现词的代表性。当minDF值大于1时，表示词汇表中出现的词最少要在minDf个文档中出现过，否则去除掉不进入词汇表；当minDF小于1，表示词汇表中出现的词最少要在包分之minDF*100个文档中出现才进入词汇表 ​ minTF：过滤文档中出现的过于罕见的词，因为这类词机乎不在什么文本中出现因此作为特征可区分的样本数量比较少。当minTF大于1时，表示这个词出现的频率必须高于这个才会进入词汇表；小于1时，表示这个大于一个分数时才进入词汇表 ​ binary: 是否只计算0/1,即是否出现该词。默认值为False。 ​ inputCol:输入列名，默认为None ​ outputCol:输出列名，默认为None 1234567891011121314151617181920212223242526df = spark.createDataFrame([(["this", "is", "apple"],),(["this", "is", "apple","watch","or","apple"],)], ["words"])#使用Word2Vec进行词向量化countvec = CountVectorizer(inputCol='words',outputCol='countvec')countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[2.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+#使用CountVec的binary模式进行向量化，countvec = CountVectorizer(inputCol='words',outputCol='countvec',binary=True)countvec_model = countvec.fit(df)countvec_model.transform(df).show(10,False)output:+-----------------------------------+----------------------------------------+-------------------------------------+|words |tf |countvec |+-----------------------------------+----------------------------------------+-------------------------------------+|[this, is, apple] |(20,[1,11,13],[1.0,1.0,1.0]) |(5,[0,1,2],[1.0,1.0,1.0]) ||[this, is, apple, watch, or, apple]|(20,[1,2,7,11,13],[1.0,1.0,1.0,2.0,1.0])|(5,[0,1,2,3,4],[1.0,1.0,1.0,1.0,1.0])|+-----------------------------------+----------------------------------------+-------------------------------------+ ###4.Word2Vec ​ Word2Vec 是一种常见的文本向量化方式,使用神经网络讲一个词语和他前后的词语来进行表示这个这个词语，主要分为CBOW和Skip- ​ 特点：Word2Vec主要是结合了前后词生成各个词向量，具有一定的语义信息 在pyspark.ml.feature中存在Word2Vec和Word2VecModel两个对象，这两个对象之间存在什么区别和联系呢？ ​ Word2Vec是Word2Vec基本参数设置部分，Word2VecModel是训练好以后的Word2Vec，有些函数只有Word2VecModel训练好以后才能使用 常见参数： ​ 1.vectorSize 生成的词向量大小 ​ 2.inputCol 输入列 ​ 3.ouputCol 输出列 ​ 4.windowSize 输出的词向量和该词前后多少个词与有关 ​ 5.maxSentenceLength 输入句子的最大长度，超过改长度直接进行进行截断 ​ 6.numPartitions 分区数，影响训练速度 常用函数： ​ 这里的常见函数要对Word2VecModel才能使用 ​ getVectors() 获得词和词向量的对应关系,返回值为dataframe ​ transform() 传入一个dataframe，将一个词列转换为词向量 ​ save() 保存模型 使用要先使用训练集对其进行训练： 123456789101112输入数据： 已经使用一定的分词方式已经进行分词后的ArrayType数组输出： 当前句子各个词进行word2vec编码后的均值，维度为vectorSize word2vec = Word2Vec(vectorSize=100,inputCol="word",outputCol="word_vector",windowSize=3,numPartitions=300)word2vec_model = word2vec.fit(data)#features将会在data的基础上多出一列word_vector，为vectorSize维数组features = word2vec.trandform(data)word2vec_model.save("./model/name.word2vec") Word2Vec如何查看是否已经训练的很好： ​ 1.选择两个在日常生活中已知词义相近的两个词A、B，再选一个与A词义不那么相近但也有一定相似度的词C ​ 2.计算A和B以及A和C的余弦距离 ​ 3.比较其大小，当满足AB距离小于AC时，重新选择三个词重复上过程多次都满足，那么认为模型已经训练完毕；若不满足上述过程，那么继续加入样本进行训练 过程中可能用的： 12345#获得某个词对应的词向量word2vec_model.getVectors().filter("word=='0eva'").collect()[0]['vector']#计算两个词向量之间距离平方a1.squared_distance(a2)]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux——恢复误删除文件]]></title>
    <url>%2F2018%2F11%2F16%2Flinux%E2%80%94%E2%80%94%E6%81%A2%E5%A4%8D%E8%AF%AF%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[​ ububtu默认情况下会存在一个默认回收站，当使用的文件被误删除需要找回时，可以进入到回收站找到该文件，将其恢复出来即可 回收站位置： ​ ~/.local/share/Trash 找回方式： ​ 进入该目录，直接将该目录中的文件考出即可]]></content>
  </entry>
  <entry>
    <title><![CDATA[pandas常用设置]]></title>
    <url>%2F2018%2F11%2F12%2Fpandas%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.]]></content>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——HMM]]></title>
    <url>%2F2018%2F11%2F07%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94HMM%2F</url>
    <content type="text"><![CDATA[马尔科夫模型假设： ​ 1.马尔科夫模型认为每一时刻的表现x有一个状态z与其对应 ​ 2.观测独立性假设：每个时刻的输出都只与当前的状态有关 ​ 3.贝叶斯公式P(o|λ) = P(λ|o)P(o) / P(λ) 马尔科夫模型的推导过程​ 对于马尔科夫模型，求解的最终目的是 1maxP(o1o2...on|λ1λ2...λn) ​ 由于p(o|λ)是个关于2n各变量的条件概率，并且n不固定，因此没办法进行精确计算。因此马尔科夫链模型采用了一种更加巧妙地方式来进行建模。 ​ 首先，根据贝叶斯公式可以得： 1P(o|λ) = P(λ|o)P(o) / P(λ) ​ λ为给定的输入，因此P(λ)为常数，因此可以忽略.因此 1P(o|λ) = P(λ|o)P(o) ​ 而根据观测独立性假设，可以得到 12P(o1o2...on|λ1λ2...λn) = P(o1|λ1)P(o2|λ2)...P(on|λn) P(o) = P(o1)P(o2|o1)P(o3|o1,o2)...P(on|on-1,on-2,...,o1) ​ 而由于其次马尔科夫假设，每个输出仅与上一个一个输出有关，那么 1P(o) = p(o1)P(o2|o1)....P(on|on-1) ​ 因此最终可得： 1P(o|λ) ~ p(o1)P(o1|o2)P(λ|o2)P(o2|o3)P(λ3|o3)...P(on|on-1)P(λ|on) ​ 其中，P(λk|ok)为发射概率，P(ok|ok-1)为转移概率。 维特比算法​ 维特比算法是隐马尔科夫模型最终求解当前表现链最可能对应的状态链使用的一种动态规划算法。主要思想是： HMM模型训练后的输出为：初始状态概率矩阵、状态转移矩阵、发射概率矩阵三个结果，用来后续进行预测 概率图模型大部分都是这样，输出为几个概率矩阵 而LR模型的输出就为系数矩阵了 我们可以看出来，LR在使用训练好的模型进行与测试效率较高，而HMM使用训练好的模型进行训练时效率较低]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习和深度学习在实践中的一些经验]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[####使用GBDT算法构造特征 ​ Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题。[1]GBDT思想对于发现多种有区分性的特征和组合特征具有天然优势，可以用来构造新的组合特征。 ​ 在这篇论文中提出可以使用GBDT各棵数输出节点的索引号来作为新的特征，对各个树渠道的索引号做one-hot编码，然后与原始的特征一起新的特征输入到模型中往往会起到不错的效果。 实践情况 ​ 1.本人使用这种方式在ctr预估中已经进行过实验，准确率提升2% ​ 2.美团在外卖预计送达时间预测中进行了实验，各个时段平均偏差减少了3% (1) 超参数选择 a. 首先为了节点分裂时质量和随机性，分裂时所使用的最大特征数目为√n。b. GBDT迭代次数（树的数量）。 树的数量决定了后续构造特征的规模，与学习速率相互对应。通常学习速率设置较小，但如果过小，会导致迭代次数大幅增加，使得新构造的特征规模过大。 通过GridSearch+CrossValidation可以寻找到最合适的迭代次数+学习速率的超参组合。c. GBDT树深度需要足够合理，通常在4~6较为合适。 虽然增加树的数量和深度都可以增加新构造的特征规模。但树深度过大，会造成模型过拟合以及导致新构造特征过于稀疏。 （2）训练方案 ​ 将训练数据随机抽样50%，一分为二。前50%用于训练GBDT模型，后50%的数据在通过GBDT输出样本在每棵树中输出的叶子节点索引位置，并记录存储，用于后续的新特征的构造和编码，以及后续模型的训练。如样本x通过GBDT输出后得到的形式如下：x → [25,20,22,….,30,28] ，列表中表示样本在GBDT每个树中输出的叶子节点索引位置。 ​ 由于样本经过GBDT输出后得到的x → [25,20,22,….,30,28] 是一组新特征，但由于这组新特征是叶子节点的ID，其值不能直接表达任何信息，故不能直接用于ETA场景的预估。为了解决上述的问题，避免训练过程中无用信息对模型产生的负面影响，需要通过独热码（OneHotEncoder）的编码方式对新特征进行处理，将新特征转化为可用的0-1的特征。 ​ 以图5中的第一棵树和第二棵树为例，第一棵树共有三个叶子节点，样本会在三个叶子节点的其中之一输出。所以样本在该棵树有会有可能输出三个不同分类的值，需要由3个bit值来表达样本在该树中输出的含义。图中样本在第一棵树的第一个叶子节点输出，独热码表示为{100}；而第二棵树有四个叶子节点，且样本在第三个叶子节点输出，则表示为{0010}。将样本在每棵树的独热码拼接起来，表示为{1000010}，即通过两棵CART树构造了7个特征，构造特征的规模与GBDT中CART树的叶子节点规模直接相关。 Wide&amp;Deep在推荐中应用 【参考文献】 He X, Pan J, Jin O, et al. Practical Lessons from Predicting Clicks on Ads at Facebook[C]. Proceedings of 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2014: 1-9.]]></content>
      <categories>
        <category>机器学习，深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web安全——CSRF和SSRF]]></title>
    <url>%2F2018%2F11%2F02%2Fweb%E5%AE%89%E5%85%A8%E2%80%94%E2%80%94CSRF%E5%92%8CSSRF%2F</url>
    <content type="text"><![CDATA[CSRF​ CSRF(Cross-site request Forgery，跨站请求伪造)通过伪装成受信任用户请求受信任的网站。 1注意:scrf漏洞并不需要获取用户的cookie等信息 目标：已经登陆了网站的用户 目的：以合法用户的身份来进行一些非法操作 需要条件： ​ 1.用户已经登陆了目标网站 ​ 2.目标用户访问了攻击者构造的url 攻击过程: ​ 1.找到存于登陆状态的存在csrf网站的合法用户，向其发送可以构造的恶意链接，诱使其点击 ​ 2.用户点击该链接，由该合法用户向服务器发出包含恶意链接里隐藏操作（如删除数据、转账等）的请求 ​ 3.服务器收到已经登录用户的请求，认为是合法用户的主动的操作行为，执行该操作 典型的csrf实例 ​ 当你使用网上银行进行转账时，首先需要登录网上银行，点击转账按钮后，会发出http://www.xxbank.com/pay.php?user=xx&amp;money=100请求，当存在攻击者想要对你进行csrf攻击时，他会向你发送一个邮件或者短信，其中包含可以构造的恶意链接 http://www.bank.com/pay,php?user=hack&amp;money=100,并且采用一定的伪装手段诱使你进行点击，当你点击后即向该hack转账100元。 流量中检测csrf的可行性 ​ 1.对于比较低级的csrf而言，可以直接通过检测请求的referer字段来进行确定是否为scrf。因为在正常scrf页面中应该是在主页等页面跳转得到，而csrf请求一般的referer是空白或者是其他网站，但是该方法可以被绕过。 ​ 2.完全的检测很难 csrf漏洞修复建议 ​ 1.验证请求的referer ​ 2.在请求中加入随机的token等攻击者不能伪造的信息 ####SSRF ​ SSRF(Server-Side Request Forgery，服务端请求伪造)是一种有由攻击者构造请求，服务器端发起请求的安全漏洞。 目标：外网无法访问的服务器系统 目的：获取内网主机或者服务器的信息、读取敏感文件等 形成原因：服务器端提供了从其他服务器获取数据的功能，但没有对目标地址做限制和过滤 攻击过程： ​ 1.用户发现存在ssrf漏洞的服务器a的页面访问的url，以及可使用SSRF攻击的参数 ​ 2.修改要请求参数要请求的文件，将其改成内网服务器b和文件，直接访问 ​ 3.服务器a接收到要访问的参数所包含的服务器b和文件名，去服务器b下载资源 ​ 3.对于服务器b，由于是服务器a发起的请求，直接将文件返回给服务器a ​ 4.服务器a将该文件或页面内容直接返回给用户 两种典型的ssrf攻击实例: ​ 本地存在ssrf漏洞的页面为：http://127.0.0.1/ssrf.php?url=http://127.0.0.1/2.php 原始页面的功能为通过GET方式获取url参数的值，然后显示在网页页面上。如果将url参数的值改为http://www.baidu.com ，这个页面则会出现百度页面内容。 ​ 因此利用这个漏洞，我们可以将url参数的值设置为内网址，这样可以做到获取内网信息的效果。 ​ 探测内网某个服务器是否开启 ​ 将url参数设置为url=”192.168.0.2:3306”时，可以获取大到该内网主机上是否存在mysql服务。 ​ 读取内网服务器文件 ​ 访问ssrf.php?url=file:///C:/Windows/win.ini 即可读取本地文件 流量中检测SSRF可行性分析： ​ 对于只能抓到外网向内网访问的流量的网口来说，从流量中检测SSRF只能从请求参数异常或返回包是否异常、是否包含敏感信息来进行检测。 SSRF漏洞修复建议: ​ 1.限制请求的端口只能是web端口，只允许访问http和https的请求 ​ 2.限制不能访问内网IP，以防止对内网进行攻击 ​ 3.屏蔽返回的信息详情]]></content>
      <categories>
        <category>web安全</category>
      </categories>
      <tags>
        <tag>web安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP—关键词提取算法]]></title>
    <url>%2F2018%2F11%2F01%2FNLP%E2%80%94%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关键词提取算法 tf-idf(词频-逆文档频率)​ ​ 其中count(w)为关键词出现的次数，|Di|为文档中所有词的数量。 ​ ​ 其中，N为所有文档的总数，I(w,Di)表示文档Di是否包含该关键词，，包含则为1，不包含则为0，若词在所有文档中均未出现，则IDF公式中分母则为0，因此在分母上加1做平滑(smooth) ​ 最终关键词在文档中的tf-idf值： ​ tf-idf特点： ​ 1.一个词在一个文档中的频率越高，在其他文档中出现的次数越少，tf-idf值越大 ​ 2.tf-idf同时兼顾了词频和新鲜度，可以有效地过滤掉常见词 ​ TextRank​ TextRank算法借鉴于Google的PageRank算法，主要在考虑词的关键度主要考虑链接数量和链接质量（链接到的词的重要度）两个因素。 ​ TextRank算法应用到关键词抽取时连个关键点：1.词与词之间的关联没有权重（即不考虑词与词是否相似） 2.每个词并不是与文档中每个次都有链接关系而是只与一个特定窗口大小内词与才有关联关系。 TextRank特点： ​ 1.不需要使用语料库进行训练，由一篇文章就可以提取出关键词 ​ 2.由于TextRank算法涉及到构建词图以及迭代计算，因此计算速度较慢 ​ 3.虽然考虑了上下文关系，但是仍然将频繁次作为关键词 ​ 4.TextRank算法具有将一定的将关键词进行合并提取成关键短语的能力 ​]]></content>
      <categories>
        <category>机器学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习——损失函数]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[​ 在机器机器学习和深度学习中有许多常见的损失函数，主要包括： ​ 1.平方差函数MSE（Mean Squared Error） ​ 2.绝对平方差函数（Mean Absolute Error） ​ 3.交叉熵函数（Cross Entory） 交叉熵 平方差函数 ​ 损失函数选择的方法：线性模型中使用平方误差函数，深度学习使用交叉熵函数 深度学习 ​ 效果最好的损失函数：交叉熵函数 为什么深度学习中使用交叉熵函数远远比使用在线性模型中表现良好的平方差误差函数好的多？ ​ 因为在深度学习中主要通过反向更新参数W和b的同时，激活函数Sigmod的导数在大部分取值时会落入左右两个饱和区间，造成参数更新异常缓慢。 ​ 具体推导如下： ​ 对于使用平方误差函数，MSE的定义为： ​ ​ 其中y是我们期望的输出，a为神经元的实际输出 a=σ(Wx+b)。因此在深度学习的反向传播机制中，权值W和b的修正公式被为： ​ 因为Sigmod函数的性质：在σ′(z) 在绝大部分区间内都趋近与0，因此会导致参数的更新回异常的缓慢。 ​ 而对于使用交叉熵函数，Crosss Entory的公式为： ​ ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习-——优化器optimzer]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E5%99%A8optimzer%2F</url>
    <content type="text"><![CDATA[优化器optimzer ​ 在机器学习和深度学习中，选择合适的优化器不仅可以加快学习速度，而且可以避免在训练过程中困到的鞍点。 1.Stochastic Gradient Descent(SGD)​ SGD是一种最常见的优化方法，这种方法在每次计算Mini-Batch的梯度，然后对其进行更新，其计算方法为： ​ 缺点：1.存在比较严重的震荡 ​ 2.容易收敛到局部最优点 2.Momentum 核心思想：用动量来进行加速 适用情况：善于处理稀疏数据 ​ 为了克服 SGD 振荡比较严重的问题，Momentum 将物理中的动量概念引入到SGD 当中，通过积累之前的动量来替代梯度。即: ​ 相较于 SGD，Momentum 就相当于在从山坡上不停的向下走，当没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。也就是说，在训练的时候，在梯度方向不变的维度上，训练速度变快，梯度方向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。 3.Adagrad 核心思想：对学习速率添加约束，前期加速训练，后期提前结束训练以避免震荡 适用情况：善于处理非平稳目标 ​ 相较于 SGD，Adagrad 相当于对学习率多加了一个约束，即： ​ Adagrad 的优点是，在训练初期，由于 gt 较小，所以约束项能够加速训练。而在后期，随着 gt 的变大，会导致分母不断变大，最终训练提前结束。 4.Adam 核心思想：结合了Momentum动量加速和Adagrad对学习速率的约束 适用情况：各种数据，前面两种优化器适合的数据Adam都更效果更好， ​ Adam 是一个结合了 Momentum 与 Adagrad 的产物，它既考虑到了利用动量项来加速训练过程，又考虑到对于学习率的约束。利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。其公式为: ​ ​ 其中： ####总结：在实际工程中被广泛使用，但是也可看到在一些论文里存在着许多使用Adagrad、Momentum的，杜对于SGD由于其需要更多的训练时间和鞍点问题，因此在实际工程中很少使用 参考文献:深度学习在美团点评推荐系统中的应用]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——KMeans]]></title>
    <url>%2F2018%2F10%2F27%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94KMeans%2F</url>
    <content type="text"><![CDATA[KMeans算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark学习心得]]></title>
    <url>%2F2018%2F10%2F23%2Fpyspark%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[​ ###持久化 ​ Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 ​ 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 ​ spark中的持久化操作主要分为两种：persist和cache。cache相当于使用MEMORY_ONLY级别的persist操作，而persist更灵活可以任意指定persist的级别。 如何选择一种最合适的持久化策略 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 提高性能的算子使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。 Shuffle​ 大多数spark作业的性能主要就消耗在shuffle环节，因为shuffle中包含了大量的磁盘IO、序列化、网络数据传输等操作。但是影响一个spark作业性能的主要因素还是代码开发、资源参数、以及数据倾斜，shuffle调优在优化spark作业性能中只能起较小的作用。 shuffle操作速度慢的原因 ​ Pyspark使用过程中的一些小Tips： 1、RDD.repartition(n)可以在最初对RDD进行分区操作，这个操作实际上是一个shuffle，可能比较耗时，但是如果之后的action比较多的话，可以减少下面操作的时间。其中的n值看cpu的个数，一般大于2倍cpu，小于1000。 2、Action不能够太多，每一次的action都会将以上的taskset划分一个job，这样当job增多，而其中task并不释放，会占用更多的内存，使得gc拉低效率。 3、在shuffle前面进行一个过滤，减少shuffle数据，并且过滤掉null值，以及空值。 4、groupBy尽量通过reduceBy替代。reduceBy会在work节点做一次reduce，在整体进行reduce，相当于做了一次hadoop中的combine操作，而combine操作和reduceBy逻辑一致，这个groupBy不能保证。 5、做join的时候，尽量用小RDD去join大RDD. 6、避免collect的使用。因为collect如果数据集超大的时候，会通过各个work进行收集，io增多，拉低性能，因此当数据集很大时要save到HDFS。 7、RDD如果后面使用迭代，建议cache，但是一定要估计好数据的大小，避免比cache设定的内存还要大，如果大过内存就会删除之前存储的cache，可能导致计算错误，如果想要完全的存储可以使用persist（MEMORY_AND_DISK），因为cache就是persist（MEMORY_ONLY）。 8、设置spark.cleaner.ttl，定时清除task，因为job的原因可能会缓存很多执行过去的task，所以定时回收可能避免集中gc操作拉低性能。 ​]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法——逻辑回归]]></title>
    <url>%2F2018%2F10%2F22%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归模型 ​ 逻辑回归算法是一种根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类的模型。逻辑回归首先赋予每个特征相同的回归参数，然后使用梯度下降算法来不断优化各个回归参数，最终根据回归参数来对新样本进行进行预测。 注意：虽然名叫逻辑回归，但是实际上是一种分类模型 工作原理 12345每个回归系数初始化为 1重复 R 次: 计算整个数据集的梯度 使用 步长 x 梯度 更新回归系数的向量(梯度下降)返回回归系数 逻辑回归算法的特点 优点：计算代价低，可解释性强 缺点：容易欠拟合，分类精度可能不高 使用数据类型：数值型数据和标称型数据(只存在是和否两种结果的将数据) sigmod函数 ​ sigmod是一种近似的越阶函数，可以将任意的输入值，然后将其映射为0到1之间的值，其公式和函数图像如下图： ​ 在逻辑回归中先使用每个特征乘以一个回归系数，将其乘积作为sigmod函数中的z，即 ​ 然后将其得到的值用sigmod函数映射到0到1，可以理解为被分为1类的概率。 梯度上升算法 ​ 要找到某个函数的最大值，最好的方式就是沿着梯度方向不断地去寻找，如果梯度记做▽ ，则函数 f(x, y) 的梯度由下式表示: 这个梯度意味着要沿 x 的方向移动 ，沿 y 的方向移动 。其中，函数f(x, y) 必须要在待计算的点上有定义并且可微。下图是一个具体的例子。 ​ 上图展示了整个梯度上升的过程，梯度上升算法在到到每个点后都会从新估计移动的方向，而这个方向就是梯度方向，移动的速度大小由参数α控制。 训练过程 ​ 训练算法：使用梯度上升寻找最佳参数 123456&gt; 每个回归系数初始化为 1&gt; 重复 R 次:&gt; 计算整个数据集的梯度&gt; 使用 步长 x 梯度 更新回归系数的向量(梯度下降)&gt; 返回回归系数&gt; ​ 其中步长为超参数alpha，而梯度的计算如下： 即每个点的数据和其输入数据相同。因此权重的更新可以使用： ​ w:=w+α error x 其中α为常数步长，error为在当前参数值下与目标值的误差经过sigmod函数处理后的值，x为当当前样本的输入 123456789101112131415161718192021222324import numpy as npdef sigmod(x): return 1/1+np.exp(-x)def gradAscend(dataSet,labelSet,alpha,maxCycles): #将输入的数据转为向量格式 dataMat = np.mat(dataSet) labelMat = np.mat(labelSet).tramsponse() #获取输入数据的维度 m,n = np.shape(dataMat) #初始化回归系数 weights = np.ones((n,1)) #对回归系数进行迭代更新 for i in range(maxCycles): #计算使用当前回归系数LR的hx值，结果为(m,1)维向量 h = sigmod(dataMat*weights) #计算误差 error = labelMat-h #根据梯度进行回归系数更新 weights = weights + alpha*dataMat.transponse()*error return weights 随机梯度上升算法 ​ 随机梯度上升算法起到的作用和一般的梯度上升算法是一样的，只是由于一般的梯度上升算法在每次更新回归系数时需要遍历整个数据集，因此当数据量变动很大时，一般的梯度上升算法的时间消耗将会非常大，因此提出了每次只使用一个样本来进行参数更新的方式，随机梯度上升（下降）。 随机梯度上升算法的特点： ​ 1.每次参数更新只使用一个样本，速度快 ​ 2.可进行在线更新，是一个在线学习算法（也是由于每次回归系数更新只使用一个样本） 工作原理： 12345所有回归系数初始化为 1对数据集中每个样本 计算该样本的梯度 使用 alpha x gradient 更新回归系数值返回回归系数值 初步随机梯度下降代码： 1234567891011121314def stocgradAscend(dataSet,labelSet): #1.这里没有转换成矩阵的过程，整个过程完全都是在Numpy数据完成的 alpha = 0.01 m,n = np.shape(dataSet) weights = np.ones((n,1)) #2.回归系数更新过程中的h、error都是单个值，而在一般梯度上升算法中使用的是矩阵操作 for i in range(m): h = np.sigmod(dataSet[i]*weights) error = h - labelSet[i] weights = weights + alpha*error*dataSet[i] return weights 但是这种随机梯度上升算法在在实际的使用过程出现了参数最后难以收敛，最终结果周期性波动的问题，针对这种问题我们对这个问题将随机梯度下降做了下面两种优化 ​ 1.改进为 alpha 的值，alpha 在每次迭代的时候都会调整。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。 ​ ​ 2.修改randomIndex的值，从以前顺序的选择样本更改为完全随机的方式来选择用于回归系数的样本，每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。 最终版随机梯度下降： 1234567891011121314151617181920def stocgradAscend(dataSet,labelSet,numsIter=150): m,n = np.shape(dataSet) weights = np.ones(n,1) alpha = 0.01 for i in range(numsIter): #生成数据的索引 dataIndex = range(m) for i in range(m): #alpha会随着i和j的增大不断减小 alpha = 4/(i+j+1.0)+0.001 # alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001 #生成随机选择要进行回归系数更新的数据索引号 randomIndex = np.random.uniform(0,len(dataIndex)) h = sigmod(np.sum(dataSet[dataIndex[randomIndex]]*weights)) error = h - dataSet[dataIndex[randomIndex]]*weights weights = weights + alpha*error*dataSet[dataIndex[randomIndex]] #在数据索引中删除 del(dataIndex[randomIndex]) return weights 预测过程 ​ LR模型的预测过程很简单，只需要根据训练过程训练出的参数，计算sigmod(w*x),如果这个值大于0.5，则分为1，反之则为0 123456def classfyLR:(inX,weights) prob = sigmod(np.sum(weights*inX)) if prob&gt;=0.5 return 1 else: return 0 注：这里的阈值其实是可以自行设定的 一些其他相关问题 1.LR模型和最大熵模型 ​ (1).logistic回归模型和最大熵模型都属于对数线性模型 ​ (2).当最大熵模型进行二分类时，最大熵模型就是逻辑回归模型 ​ (3) 学习他们的模型一般采用极大似估计或正则化的极大似然估计 ​ (4)二者可以形式化为无约束条件下的最优化问题 2.LR模型的多分类 ​ 逻辑回归也可以作用于多分类问题，对于多分类问题，处理思路如下：将多分类问题看做多个二分类，然后在各个sigmod得到的分数中区最大的值对应的类作为最终预测标签。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布 ​ 基本概率图模型主要包括贝叶斯网络、马尔科夫网络和隐马尔科夫网络三种类型。 ​ 基本的Graphical Model 可以大致分为两个类别：贝叶斯网络(Bayesian Network)和马尔可夫随机场(Markov Random Field)。它们的主要区别在于采用不同类型的图来表达变量之间的关系：贝叶斯网络采用有向无环图(Directed Acyclic Graph)来表达因果关系，马尔可夫随机场则采用无向图(Undirected Graph)来表达变量间的相互作用。这种结构上的区别导致了它们在建模和推断方面的一系列微妙的差异。一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而对于马尔可夫场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。 概率图模型的表示理论​ 概率图模型的表示由参数和结构两部分组成。根据边有无方向性，可分为下面三种： ​ a、有向图模型—贝叶斯网络 ​ b、无向图模型—马尔科夫网络 ​ c、局部有向模型—条件随机场和链图]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最优化问题]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、概述最优化问题主要分为 ​ 1.无约束优化问题 ​ 2.等式优化问题 ​ 3.含不等式优化问题 ​ 对于无约束问题常常使用的方法就是Fermat定理，即求取f(x)的倒数然后另其为0，可求得候选最优质，如果函数为凸函数则直接为最优值。 ​ 在求取有约束条件的优化问题时，拉格朗日乘子法（Lagrange Multiplier) 和KKT条件是非常重要的两个求取方法。二者各有其使用范围： ​ 拉格朗日乘子法：等式约束的最优化问题 ​ KTT条件：不等式约束下的最优化问题 对于一般的任意问题而言，这两种方法求得的解是使一组解成为最优解的必要条件，只有当原问题是凸问题的时候，求是求得的解是最优解的充分条件。 二、有约束最优化问题的求解​ 1.拉格朗日乘子法 ​ 对于等式约束，我们可以通过一个拉格朗日系数a 把等式约束和目标函数组合成为一个式子L(a, x) = f(x) + a*h(x), 这里把a和h(x)视为向量形式，a是横向量，h(x)为列向量，然后求取最优值，可以通过对L(a,x)对各个参数求导取零，联立等式进行求取。 ​ 2.KTT条件 ​ 对于含有不等式的约束条件最优化问题，我们可以把所有的不等式约束、等式约束和目标函数全部写为一个式子L(a, b, x)= f(x) + ag(x)+bh(x)，KKT条件是说最优值必须满足以下条件： L(a, b, x)对x求导为零； h(x) =0; a*g(x) = 0;（g(x)为不等式约束） 求取这三个等式之后就能得到候选最优值。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化相关问题 1.实现参数的稀疏有什么好处吗？ 121.可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了。2.参数变少可以使整个模型获得更好的可解释性。 2.参数值越小代表模型越简单吗？ 1是的。这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。 3.模型简单包括什么？ 121.参数少2.参数值小 正则化 机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作L1-norm和L2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归） 概念： L1正则化是指权值向量绝对值之和，通常表示为||w||1 L2正则化是指全职向量w中各个元素的平方和让后再求平方根，通常表示为||w||2 下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。 下图是Python中Ridge回归的损失函数，式中加号后面一项α||w||22 即为L2正则化项 注：1.上面的两个函数前半部分可以为任意的线性函数的损失函数，组合成的函数都可以成为Lasso回归会Ridge回归2.上面两个式子中的α为正则化系数，后续通过交叉验证确定 注：上面两个式子中的α为正则化系数，后续通过交叉验证确定) L1正则化与L2正则化的作用： L1正则化可产生稀疏权值矩阵，即产生一个稀疏模型，可用用于特征选择 L2正则化主要用于防止过拟合 L1正则化 L1正则化的标准形式： ​ 其中J0是原始的损失函数，加好后面是L1正则化项。机器学习的最终目就是找出损失函数的最小值，当我们在原本的损失函数后面加上L1正则化后，相当于对J0做了一个约束，另L1正则化项等于L，则 J=J0+L，任务转化为在L1的约束下求J0最小值的解。​ 考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： ​ 从图中可看出j0与L相交于L的一个顶点处，这个顶点即为最优解。注意这个顶点的值为（w1,w2）=(0,w)，可以想象，在更多维的情况下，L将会有很多突出的角，而J与这些叫接触的几率将远大于与L其他部位接触的概率，而这些角上将会有许多权值为0，从而产生系数矩阵，进而用于特征选择。 1234567891011from sklearn.linear_model import Lassofrom sklearn.preprocessing import StandardScaler from sklearn.datasets import load_bostonboston=load_boston() scaler=StandardScaler() X=scaler.fit_transform(boston["data"])Y=boston["target"]names=boston["feature_names"]lasso=Lasso(alpha=.3)lasso.fit(X,Y)print"Lasso model: ",pretty_print_linear(lasso.coef_,names,sort=True) L2正则化 L2正则化的标准形式 ​ 和L1正则化相同，任务转化为在L2的约束下求J0最小值的解。考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，二者图像首次相交处即为最优解，获得下图： 机器学习过程中权值尽可能小的原因： 试想对于一个模型，当参数很大时，只要数据偏移一点点，就会对结果造成很大的影响，如果参数较小，则数据偏移的多一点，也不会对结果产生多大的影响，抗扰动能力强 为什么L2正则化可以使权值尽可能的小? 对于损失函数不带L2正则化项的梯度下降时参数更新公式为： 加入L2正则化项，参数更新公式为： 根据两个公式之间的差别，我们可以明显的看到，加入正则化以后的梯度下降在进行参数更新时，要先将原有的参数值乘以一个小于1的值，因此权值也会变得比不带的参数小]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM模型]]></title>
    <url>%2F2018%2F10%2F19%2FSVM%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.SVM模型的超参数 ​ SVM模型主要包括C和gamma两个超参数。 ​ C是惩罚系数，也就是对误差的宽容度，C越大代表越不能容忍出现误差，越容易出现过拟合； ​ gamma是选择RBF核时，RBF核自带的一个参数，隐含的决定数据映射到新空间的后的分布，gamma越大，支持向量越少。 支持向量的个数影响训练和预测的个数 gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大,σ会很小，σ很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率 2.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量解析过程中常用的一些工具]]></title>
    <url>%2F2018%2F10%2F12%2F%E6%B5%81%E9%87%8F%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[在实际流量解析过程中一般使用 1.url编码解码-urllib ​ python使用urllib包来进行url编码和解码，对于python3： 123456789101112import urllibrawurl="xxc=B&amp;z0=GB2312&amp;z1=%E4%B8%AD%E5%9B%BD"#python2url = url.unquote(rawurl)#python3url=urllib.parse.unquote(rawurl)output: 'xxc=B&amp;z0=GB2312&amp;z1=中国' 2.字符串转十六进制 ​ 字符串转十六进制可以分为两种：1.对于已经是十六进制格式，但是已经被转为字符串，例如：”” 123import binascii#python3 3.原始字节串和十六进制字节串之间的转化—binascii 12345678910import binasciidata_bytes = b"cfb5cdb3d5d2b2bbb5bdd6b8b6a8b5c4c2b7beb6a1a3"data_hex = b'\xcf\xb5\xcd\xb3\xd5\xd2\xb2\xbb\xb5\xbd\xd6\xb8\xb6\xa8\xb5\xc4\xc2\xb7\xbe\xb6\xa1\xa3'#原始字节串==&gt;十六进制字节串binascii.hexlify(data_bytes)#十六进制字节串==&gt;原始字节串binascii.unhexlify(data_bytes) ​ ​]]></content>
      <categories>
        <category>流量相关</category>
      </categories>
      <tags>
        <tag>流量相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用正则表达式]]></title>
    <url>%2F2018%2F10%2F10%2F%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.匹配一个指定字符串，指定字符串前后不能有任何字母和数字内容 12#以c99关键字为例[]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型——朴素贝叶斯]]></title>
    <url>%2F2018%2F10%2F09%2F%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[​ 逻辑回归通过拟合曲线实现分类，决策树通过寻找最佳划分特征进而学习样本路径实现分类，支持向量机通过寻找分类超平面进而最大化类间间隔实现分类，而朴素贝叶斯通过 朴素贝叶斯思想 ​ 朴素贝叶斯是一种最简单的概率图模型，通过根据训练样本统计出样本的概率分布，基于贝叶斯定理和条件独立假设来进行建模预测的模型。 朴素贝叶斯概率图 贝叶斯定理 12345678p(AB)=P(A/B)P(B) =P(B/A)P(A)在贝叶斯模型中用到的是下面的形式： P(Ci/W) = P(W|Ci)*P(Ci)/P(W)其中，W为向量，有的多个值组成，Ci为标签，也就是上式可以写成下面的形式 P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)*P(Ci)/P(W)里面的P(Ci/w0,w1,..,w)就是机器学习建模最终的目标，在一定条件下是某一类的概率 条件独立假设 12345 条件独立假设认为：每个事件的发生都相互独立，互相之间没有影响。由于这个假设，上面的式子可以改为： P(Ci/w0,w1,..,w) = P(w0,w1,...,wn/Ci)/P(Ci) = P(w0/Ci)P(w1/Ci)...P(wn/Ci)*P(Ci)/p(W) 到这里，我们可以知道，要求的最终的结果，只需要在训练集中求得P(Ci)以及在P(w0/Ci)...P(wn/Ci)即可 模型训练 因此在NB算法训练时，只需要在训练集样本中到下面三个概率分布： ​ 1.P(Ci)，在训练集中标签1出现的概率(二分类只需要统计一个，n分类就需要n-1个) ​ 2.P(wj/Ci),在训练集中属于各个标签的条件下第n个特征是i的概率 注意：这里不需要统计P(W)的概率，因为最终属于各个类型的概率都需要除以相同的P(W)，因此约掉 训练代码： 123456789101112131415161718192021222324252627def trainNB(dataSetList,labels): dataSetVec = np.array(dataSetList) #计算Pc sampleNums = len(dataSetVec) pc = np.sum(datasetVec)/sampleNums #计算p(wj/Ci),这里是二分类 p0Nums = 0 p1Nums = 0 #这里涉及到初始化问题 p0Vecs = np.ones(len(dataSetVec[0])) p1Vecs = np.ones(len(dataSetVec[0])) for i in range(len(labels)): if labels[i]==0: p0Vecs += dataSetVec[0] p0Nums += 1 else: p1Vecs += dataSetVec[0] p1Nums += 1 p0Vecs = p0Vecs/p0Nums p1Vecs = p1Vecs/p1Nums return pc,p0Vecs,p1Vecs 初始化问题： ​ 再利用贝叶斯分类器进行分类时，要计算多个概率等乘积以计算文档属于某个分类的概率，即计算： ​ P(w0|c=1)P(w1|c=1)….P(wn|c=1) ​ 如果其中任意一项为0，那么最终的成绩也将等于0。为了降低这种情况造成的影响，可以将所有词初始化为1. 预测过程 ​ NB模型的预测过程就是使用上面统计得到的概率分布与输入数据进行关联后，计算出新的样本属于各个类型的概率，然后选择其中概率最大的类型作为模型预测类型的过程。预测过程中需要关注的一个关键问题需要重点关注，那就是python的下溢出问题。 ​ 下溢出问题：在python中当多个很小的数相乘时会产生下溢出问题(最后四舍五入得到0) ​ 解决办法：取自然对数。因为自然对数和原来的数怎增减性相同，极值点也相同 ​ 使用自然对数后，上面的式可以转换成： ​ P(Ci/w0,w1,..,w) = P(w0/Ci)P(w1/Ci)…P(wn/Ci)/P(Ci) –&gt;P(Ci/w0,w1,..,w) = log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) 预测代码： ​ 预测过程中将已知的概率分布与输入数据进行关联的方式： ​ log(P(w0/Ci))+…+log(P(wn/Ci))+P(Ci) ——&gt;log(P(w0/Ci))x0+…+log(P(wn/Ci))xn+log(P(Ci) ​ 这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来. 123456789def classfyNB(input_data,pc,p0Vecs,p1Vecs): #这里的input_data*np.log(p0Vecs)代表将每个出现的词和其出现在该类中出现该词的概率关联起来 #这里之所以没有除以pw，是因为对每个类型的pw是一致的，就没有必要所有都除了 p0 = sum(input_data*np.log(p0Vecs))+math.log(pc) p1 = sum(input_data*np.log(p1Vecs))+math.log(1-pc) if p0&gt;p1: return 0 else: return 1 ​]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github基本使用]]></title>
    <url>%2F2018%2F10%2F03%2Fgithub%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[github相关知识： ​ github项目使用终端进行管理时分为三个区域：工作目录、暂存区（本地库）、github上的远程项目，当我们要更新以及操作一个项目时，要遵循以下的格式： 1.先从github上面pull下远程的项目分支 2.本地的项目文件夹中的文件进行更新（更新工作目录中的文件） 3.使用add将更新的索引添加到本地库 4.使用commit工作目录中的文件提交到暂存区(本地库) 5.将文件push到远程分支或merge到远程分支 基本操作 git clone “ssh项目地址” 克隆远程项目 git pull origin master 取回远程主机或本地的某个分支的更新，再与本地分支进行合并(这种写法是origin主机的master分支与本地当前分支进行合并) git push origin master 将本地的当前分支push到origin主机的master分支上 git add “文件名” 将指定文件提交到本地库 git commit -m “描述信息” 将本地的全部文件都提交到本地库 git log 打印该项目的版本操作信息 git status 查看到那个钱仓库状态 更新github 123456789101112131415161718192021222324252627282930313233343536373839#将项目从github上面拉下来(本地已经有的可以跳过,已有则直接进入该文件夹)git clone github链接#查看项目状态git statusoutput: On branch master Your branch is up to date with 'origin/master'. nothing to commit, working tree clean #创建或者导入新文件到工作区touch "文件1"#将文件工作目录的文件提交到暂存区git add "文件1" #提交指定文件git add -A #一次提交工作目录中的全部文件#查看项目状态 git status #第一次提交描述时需要设置账户信息git config --global user.name "John Doe"git config --global user.email johndoe@example.com#添加描述git commit -m "此次添加的描述信息"#查看项目状态git status output: On branch master Your branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits)#将修改从暂存区提交到远程分支git push origin master 删除已经提交到github上面的文件 123456789#在本地拉取远程分支git pull original master#在本地删除对应的文件git rm filename#添加描述上传到远程分支git commit -m "删除文件filename"git push original master 已经提交到github上的文件进行版本回退 12345678910#先通过git log获取版本号git log#然后使用git revert 版本号来回退到指定版本git retvert 版本号#然后:x保存退出就可以了撤回到指定的版本了#最后再将本地分支push到github上git push origin master 分支切换 123456git checkout ... #创建分支git checkout -b ... #创建并且换到分支git checkout ... #切换到分支 12345git branch #查看本地分支git branch -a #查看本地和远程的全部分支git push origin --delete dev2 #删除远程分支]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark ML库]]></title>
    <url>%2F2018%2F09%2F30%2Fpyspark-ML%E5%BA%93%2F</url>
    <content type="text"><![CDATA[​ pyspark的ML软件包主要用于针对spark dataframe的建模（MLlib主要还是针对RDD，准备废弃）,ML包主要包含了转化器Transformer、评估器Estimater和管道Pipline三个部分。 1.转化器​ 转换器通常通过将一个新列附加到DataFrame来转化数据，每个转化器都必须实现.transform()方法。 ​ 使用在预处理截断 .transorm()方法常用的参数： ​ 1.dataframe 这是唯一一个强制性参数（也可以不理解为参数） ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 ​ 要使用转化器首先需要引入宝feature 1from pyspark.ml.feature import ... (1)Binarizer ​ 根据指定阈值将连续变量进行二值化 注：这里需要输入的那一列的数据类型为DoubleType,InterType和FloatType都不支持 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)binarizer = Binarizer(threshold=4.0,inputCol='id',outputCol='binarizer_resulit')binarizer.transform(df).show()output: +---+---+-----------------+ | id|age|binarizer_resulit| +---+---+-----------------+ |2.0| a| 0.0| |1.0| b| 0.0| |4.0| b| 0.0| #当值与阈值相同的时候向下取 |9.0| b| 1.0| |4.3| c| 1.0| +---+---+-----------------+ (2)Bucketizer ​ 根据阈值列表将连续变量值离散化 注：splits一定要能包含该列所有值 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)bucketizer = Bucketizer(splits=[0,2,4,10],inputCol='id',outputCol='bucketizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (3)QuantileDiscretizer ​ 根据数据的近似分位数来将离散变量转化来进行离散化 1234567891011121314df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)quantile_discretizer = QuantileDiscretizer(numBuckets=3,inputCol='id',outputCol='quantile_discretizer_result')bucketizer.setHandleInvalid("keep").transform(df).show()output: +---+---+-----------------+ | id|age|bucketizer_result| +---+---+-----------------+ |2.0| a| 1.0| |1.0| b| 0.0| |4.0| b| 2.0| |9.0| b| 2.0| |4.3| c| 2.0| +---+---+-----------------+ (4)Ngram ​ 将一个字符串列表转换为ngram列表，以空格分割两个词,一般要先使用算法来先分词，然后再进行n-gram操作。 注：1.空值将被忽略，返回一个空列表 ​ 2.输入的列必须为一个ArrayType(StringType()) 1234567df = spark.createDataFrame([ [[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;]], [[&apos;s&apos;,&apos;d&apos;,&apos;u&apos;,&apos;y&apos;]]],[&apos;word&apos;])ngram = NGram(n=2,inputCol=&quot;word&quot;,outputCol=&quot;ngram_result&quot;)ngram.transform(df).show() (5)RegexTokener ​ 正则表达式分词器，用于将一个字符串根据指定的正则表达式来进行分词。 参数包括： ​ pattern：用于指定分词正则表达式，默认为遇到任何空白字符则分词 ​ minTokenLength: 最小分词长度过滤，小于这个长度则过滤掉 12 (6)VectorIndexer ​ VectorIndexer是对数据集特征向量中的类别（离散值）特征进行编号。它能够自动判断那些特征是离散值型的特征，并对他们进行编号，具体做法是通过设置一个maxCategories，特征向量中某一个特征不重复取值个数小于maxCategories，则被重新编号为0～K（K&lt;=maxCategories-1）。某一个特征不重复取值个数大于maxCategories，则该特征视为连续值，不会重新编号 主要作用：提升决策树、随机森林等ML算法的效果 参数： ​ 1.MaxCategories 是否被判为离散类型的标准 ​ 2.inputCol 输入列名 ​ 3.outputCol 输出列名 12345678910111213141516171819+-------------------------+-------------------------+|features |indexedFeatures |+-------------------------+-------------------------+|(3,[0,1,2],[2.0,5.0,7.0])|(3,[0,1,2],[2.0,1.0,1.0])||(3,[0,1,2],[3.0,5.0,9.0])|(3,[0,1,2],[3.0,1.0,2.0])||(3,[0,1,2],[4.0,7.0,9.0])|(3,[0,1,2],[4.0,3.0,2.0])||(3,[0,1,2],[2.0,4.0,9.0])|(3,[0,1,2],[2.0,0.0,2.0])||(3,[0,1,2],[9.0,5.0,7.0])|(3,[0,1,2],[9.0,1.0,1.0])||(3,[0,1,2],[2.0,5.0,9.0])|(3,[0,1,2],[2.0,1.0,2.0])||(3,[0,1,2],[3.0,4.0,9.0])|(3,[0,1,2],[3.0,0.0,2.0])||(3,[0,1,2],[8.0,4.0,9.0])|(3,[0,1,2],[8.0,0.0,2.0])||(3,[0,1,2],[3.0,6.0,2.0])|(3,[0,1,2],[3.0,2.0,0.0])||(3,[0,1,2],[5.0,9.0,2.0])|(3,[0,1,2],[5.0,4.0,0.0])|+-------------------------+-------------------------+结果分析：特征向量包含3个特征，即特征0，特征1，特征2。如Row=1,对应的特征分别是2.0,5.0,7.0.被转换为2.0,1.0,1.0。我们发现只有特征1，特征2被转换了，特征0没有被转换。这是因为特征0有6中取值（2，3，4，5，8，9），多于前面的设置setMaxCategories(5)，因此被视为连续值了，不会被转换。特征1中，（4，5，6，7，9）--&gt;(0,1,2,3,4,5)特征2中, (2,7,9)--&gt;(0,1,2) (7)StringIndexer ​ 将label标签进行重新设置，出现的最多的标签被设置为0，最少的设置最大。 1234567891011121314151617按label出现的频次，转换成0～num numOfLabels-1(分类个数)，频次最高的转换为0，以此类推：label=3，出现次数最多，出现了4次，转换（编号）为0其次是label=2，出现了3次，编号为1，以此类推+-----+------------+|label|indexedLabel|+-----+------------+|3.0 |0.0 ||4.0 |3.0 ||1.0 |2.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||3.0 |0.0 ||2.0 |1.0 ||1.0 |2.0 |+-----+------------+ (8)StringToIndex ​ 功能与StringIndexer完全相反，用于使用StringIndexer后的标签进行训练后，再将标签对应会原来的标签 作用：恢复StringIndexer之前的标签 参数： ​ 1.inputCol 输入列名 ​ 2.outputCol 输出列名 123456789101112|label|prediction|convetedPrediction|+-----+----------+------------------+|3.0 |0.0 |3.0 ||4.0 |1.0 |2.0 ||1.0 |2.0 |1.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||3.0 |0.0 |3.0 ||2.0 |1.0 |2.0 ||1.0 |2.0 |1.0 | 2.评估器​ 评估器就是机器学习模型，通过统计数据从而进行预测工作，每个评估器都必须实现.fit主要分为分类和回归两大类，这里只针对分类评估器进行介绍。 ​ ​ Pyspark的分类评估器包含以下七种： 1.LogisticRegression ​ 逻辑回归模型 2.DecisionTreeClassifier ​ 决策树模型 3.GBTClassifier ​ 梯度提升决策树 4.RandomForestClassifier ​ 随机森林 5.MultilayerPerceptronClassifier ​ 多层感知机分类器]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe基础操作]]></title>
    <url>%2F2018%2F09%2F28%2Fpyspark-dataframe%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.select ​ select用于列选择，选择指定列，也可以用于和udf函数结合新增列 ​ 列选择： 12data_set.select("*").show() #选择全部列data_set.select("file_name","webshell").show() #选择file_name，webshell列 ​ 与udf函数组合新增列： 123456from pysaprk.sql.function import udfdef sum(col1,col2): return col1+col2udf_sun = udf(sum,IntergerType())data_set.select("*",udf_sum("a1","a2")).show() #新增一列a1和a2列加和后的列 2.filter ​ filter用于行选择，相当于使用前一半写好的sql语句进行查询。 ​ 查询某个列等于固定值： 1data_set.filter("file_name=='wp-links-opml.php'").show() ​ 查询符合某个正则表达式所有行:​ 1data_set.filter("file_name regexp '\.php$') #选择所有.php结尾文件 3.join ​ pyspark的dataframe横向连接只能使用join进行连接，需要有一列为两个dataframe的共有列 1df_join = df1.join(df,how="left",on='id').show() #id为二者的共有列 4.agg ​ 使用agg来进行不分组聚合操作，获得某些统计项 1234567#获取数据中样本列最大的数值#方式一：df.agg(&#123;"id":"max"&#125;).show() #方式二：import pyspark.sql.functions as fn df.agg(F.min("id")).show() 5.groupby ​ 使用groupby可以用来进行分组聚合。 1df.groupby(&quot;) 6.printSchema ​ 以数的形式显示dataframe的概要 123456df.printSchema()output: root |-- id: integer (nullable = true) |-- age: integer (nullable = true) 7.subtract ​ 找到那些在这个dataframe但是不在另一个dataframe中的行，返回一个dataframe 1234df = spark.createDataFrame([[2.0,'a'],[1.0,'b'],[4.0,'b'],[9.0,'b'],[4.3,'c']],schema=schema)df1 = spark.createDataFrame([[2.0,'a'],[4.3,'c']],schema=schema)df.subtract(df1).show() #找到在df中但是不在df1中的行 8.cast ​ 指定某一列的类型 12 9.sort ​ 按照某列来进行排序 参数： ​ columns: 可以是一列，也可以是几列的列表 ​ ascending：升序，默认是True 1data.sort(['count'],ascending=False).show()]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspark dataframe操作进阶]]></title>
    <url>%2F2018%2F09%2F26%2Fpyspark-dataframe%E6%93%8D%E4%BD%9C%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[​ 这一节主要讲的是spark在机器学习处理过程中常用的一列操作，包括获得各种预处理。 1.将多列转化成一列 ​ pyspark可以直接使用VectorAssembler来将多列数据直接转化成vector类型的一列数据。 1234567891011121314151617181920212223242526272829303132from pyspark.ml.feature import VectorAssemblerdiscretization_feature_names = [ 'discretization_tag_nums', 'discretization_in_link_nums', 'discretization_out_link_nums', 'discretization_style_nums', 'discretization_local_img_nums', 'discretization_out_img_nums', 'discretization_local_script_nums', 'discretization_in_script_nums', 'discretization_out_script_nums']vecAssembler = VectorAssembler(inputCols=discretization_feature_names, outputCol="feature_vec_new")data_set = vecAssembler.transform(dataset) #会返回一个在原有的dataframe上面多出来一列的新dataframedata_set.printscheama()output： root |-- discretization_tag_nums: double (nullable = true) |-- discretization_in_link_nums: double (nullable = true) |-- discretization_out_link_nums: double (nullable = true) |-- discretization_style_nums: double (nullable = true) |-- discretization_local_img_nums: double (nullable = true) |-- discretization_out_img_nums: double (nullable = true) |-- discretization_local_script_nums: double (nullable = true) |-- discretization_in_script_nums: double (nullable = true) |-- discretization_out_script_nums: double (nullable = true) |-- feature_vec_new: vector (nullable = true) #多出来的 2.连续数据离散化 ​ pyspark中提供QuantileDiscretizer来根据分位点来进行离散化的操作，可以根据数据整体情况来对某一列进行离散化。 常用参数： ​ numBuckets：将整个空间分为几份，在对应的分为点处将数据进行切分 ​ relativeError： ​ handleInvalid： 12345678from pyspark.ml.feature import QuantileDiscretizerqds = QuantileDiscretizer(numBuckets=3,inputCol=inputCol, outputCol=outputCol, relativeError=0.01, handleInvalid="error")#这里的setHandleInvalid是代表对缺失值如何进行处理#keep表示保留缺失值dataframe = qds.setHandleInvalid("keep").fit(dataframe).transform(dataframe) 3.增加递增的id列 ​ monotonically_increasing_id() 方法给每一条记录提供了一个唯一并且递增的ID。 123from pyspark.sql.functions import monotonically_increasing_iddf.select("*",monotonically_increasing_id().alias("id")).show() 4.指定读取或创建dataframe各列的类型 ​ pyspark可以支持使用schema创建StructType来指定各列的读取或者创建时的类型，一个StructType里面包含多个StructField来进行分别执行列名、类型、是否为空。 12345678910111213from pyspark.sql.types import *schema = StructType([ StructField("id",IntegerType(),True), StructField("name",StringType(),True)])df = spark.createDataFrame([[2,'a'],[1,'b']],schema)df.printSchema()output: root |-- id: integer (nullable = true) |-- name: string (nullable = true) 5.查看各类缺失值情况 123import pyspark.sql.functions as fndata_set.agg(*[(1-(fn.count(i)/fn.count('*'))).alias(i+"_missing") for i in data_set.columns]).show() ​ 注：在其中agg()里面的 ”\“代表将该列表处理为一组独立的参数传递给函数 6.使用时间窗口来进行分组聚合 ​ 这也是pyspark比pandas多出来的一个时间窗口聚合的使用 12src_ip_feature = feature_data.groupby(&quot;srcIp&quot;,F.window(&quot;time&quot;, &quot;60 seconds&quot;)).agg( F.count(&quot;distIp&quot;).alias(&quot;request_count&quot;), 7.过滤各种空值 ​ （1）过滤字符串类型的列中的某列为null行 ​ 这里要借助function中的isnull函数来进行 12345import pyspark.sql.function as Fdf.filter(F.isnull(df["response_body"])).show() #只留下response_body列为null的df.filter(~F.isnull(df["response_body"])).show() #只留下response_body列不为null的 ​ 8.列名重命名 ​ 借助selectExpr可以实现在select的基础上使用sql表达式来进行进一步的操作这一特性，将列名进行修改 12#将count列重命名为no_detection_nums,webshelll_names列名不变df = df.selectExpr("webshell_names","count as no_detection_nums")]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HMM入门以及在webshell检测中的应用汇]]></title>
    <url>%2F2018%2F09%2F24%2FHMM%E5%85%A5%E9%97%A8%E4%BB%A5%E5%8F%8A%E5%9C%A8webshell%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%B1%87%2F</url>
    <content type="text"><![CDATA[HMM一、HMM五元素 ​ ​ 其中： ​ N：隐藏状态数 hidden states ​ M：观测状态数 observed states ​ A： 状态转移矩阵 transition matrix ​ B：发射矩阵 emission matrix ​ pi：初始隐状态向量 initial state vector HMM全称隐马尔科夫链，常用与异常检测，在大量正常的模式中找出异常的模式。 ​ 隐马尔科夫链模型相关的问题主要分为三类： 1.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链 2.已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 3.已知隐含状态数量、可以观察到多个可见状态链，求因此状态的转移矩阵和发射概率 1.求隐藏状态链问题​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求出隐藏的状态链(也就是最大概率的转移序列) ​ 应用场景：语音识别解码问题 ​ 方法：Viterbi algorithm ​ 举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。 ​ 首先，如果我们只掷一次骰子：看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8. ​ 把这个情况拓展，我们掷两次骰子：结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是 ​ 同样的，我们可以计算第二个骰子是D4或D8时的最大概率。我们发现，第二个骰子取到D6的概率最大。而使这个概率最大时，第一个骰子为D4。所以最大概率骰子序列就是D4 D6。​ 继续拓展，我们掷三次骰子：同样，我们计算第三个骰子分别是D6，D4，D8的最大概率。我们再次发现，要取到最大概率，第二个骰子必须为D6。这时，第三个骰子取到D4的最大概率是​ 同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。 写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。 ​ ​ 我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来,这就是Viterbi算法。 2.求得出某个可见状态链的概率​ 该问题是在：已知隐含状态数量、隐含状态的转换矩阵、根据可见的状态链，求得出这个可见状态链的概率 ​ 应用场景：检测观察到的结果与我们已知的模型是否吻合，即异常检测 ​ 方法:前向算法（forward algorithm） ​ 要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算（即在当前的HMM下可能出啊先找个状态链的概率）。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。​ 我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。​ 首先，如果我们只掷一次骰子，看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18： ​ 把这个情况拓展，我们掷两次骰子，看到结果为1，6，总概率为0.05： ​ 继续拓展，我们掷三次骰子，看到结果为1，6，3，计算总概率为0.03： ​ 同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。 3. 求状态转移矩阵和发射概率（训练过程）​ 该问题是在： 已知隐含状态数量、可以观察到多个可见状态链 ​ 应用场景：有大量该问题的已知观测序列，想训练一个HMM模型 ​ 方法：Baum-Welch算法 HMM在webshell检测中的应用缺陷： 由于HMM的训练和预测的时间复杂度都是O(n^2)，因此在数据量特别大的时候速度非常慢 HMM并没有spark版本，无法直接部署到spark集群上（这点是个大坑！） HMM在在webshell检测中比较局限，只能部署在生成文件名绝大部分都有固定模式的网站上来进行检测，对于各个页面随意进行命名的网站并没有检测效果 使用RNN来代替HMM​ 在数据量很大时，我们可以使用RNN来代替HMM，RNN在实现过程中可以采用并行的方式来进行梯度下降，大大加快了整体的速度。 ​ 而且大部分时候使用RNN的效果都会优于HMM，详细见下篇：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker操作]]></title>
    <url>%2F2018%2F09%2F23%2FDocker%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[-d 容器在后台运行 -P 将容器内部的端口映射到我们的主机上 12docker ps #查看全部正在开启的dockerdocker ps ####1.进入以及退出docker ​ 进入docker命令主要分为两种，attach和exec命令，但是由于exec命令退出后容器继续运行，因此更为常用。 12345678910#首先查看正在运行的docker，在其中选择想要进入的docker namedocker ps#然后使用exec进行进入dockerdocker exec --it docker_name /bin/bash/#进行各种操作#退出dockerexit或Ctrl+D 2.docker和宿主主机之间传输文件​ docker 使用docker cp 命令来进行复制，无论容器有没有进行运行，复制操作都可以进行执行。 12345#从docker中赋值文件到宿主主机docker cp docker_name:/docker_file_path local_path#从宿主主机复制到dockerdocker cp local_path docker_name:/docker_file_path]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmux基本操作]]></title>
    <url>%2F2018%2F09%2F23%2Ftmux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[tmux 是一款终端复用命令行工具，一般用于 Terminal 的窗口管理。 tmux核心功能 1.tmux可以在一个窗口中创建多个窗格 2.终端软件重启后通过命令行恢复上次的session 在tmux中快捷键都需要在使用之前先按前缀快捷键(mac默认⌃b,windows默认control)，以下是常用的集中快捷键列表: 1. 窗格操作 % 左右平分出两个窗格 &quot; 上下平分出两个窗格 x 关闭当前窗格 { 当前窗格前移 } 当前窗格后移 ; 选择上次使用的窗格 o 选择下一个窗格，也可以使用上下左右方向键来选择 space 切换窗格布局，tmux 内置了五种窗格布局，也可以通过 ⌥1 至 ⌥5来切换 z 最大化当前窗格，再次执行可恢复原来大小 q 显示所有窗格的序号，在序号出现期间按下对应的数字，即可跳转至对应的窗格 2.会话操作 ​ 在shell中每次输入tmux都会创建一个tmux会话(session)，在tmux中常用的tmux操作包括： $ 重命名当前会话 s 选择会话列表 d detach 当前会话，运行后将会退出 tmux 进程，返回至 shell 主进程 ​ 在shell准进程中也可以进行直接对session进行操作： tmux new -s foo #创建名为foo的会话 tmux ls #列出所有的tmux tmux a #恢复至上一次回话 tmux a -t foo #恢复会话名称为foo的会话 tmux kill-session -t foo #删除会话名称为foo的会话 tmux kill-server #删除所有会话 除了上面常用的快捷键以外，还可以直接使用前缀快捷键⌃b加?来查看所有快捷键列表]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch向量转化操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.cat ​ 对数据沿着某一维度进行拼接，cat后数据的总维数不变。 12345678910111213141516171819202122import torchtorch.manual_seed(1)x = torch.randn(2,3)y = torch.randn(1,3)s = torch.cat((x,y),0)print(x,y)print(s)output: 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 [torch.FloatTensor of size 2x3] -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 1x3] 0.6614 0.2669 0.0617 0.6213 -0.4519 -0.1661 -1.5228 0.3817 -1.0276 [torch.FloatTensor of size 3x3] 注：torch.cat和torch.concat作用用法完全相同，只是concat的简写形式 2.unsequeeze和sequeeze ​ torch.sequeeze主要用于维度压缩，去除掉维数为1的维度。 12345678#1.删除指定的维数为1的维度 #方式一： torch.sequeeze(a,2) #方式二： a.sequeeze(2)#2.删除全部维度为1的维度 torch.sequeeze(a) ​ torch.unsequeeze主要用于维度拓展，增加维数为1的维度。 1torch.unsequeeze(a,2) #在维度2增加维数为1的维度]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch运算操作]]></title>
    <url>%2F2018%2F09%2F23%2Fpytorch%E8%BF%90%E7%AE%97%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.transponse** ​ torch.transponse操作是转置操作，是一种在矩阵乘法中最常用的几种操作之一。 12#交换两个维度torch.transponse(dim1,dim2) 2.matmul和bmm ​ matmul和bmm都是实现batch的矩阵乘法操作。 123456789a = torch.rand((2,3,10))b = torch.rand((2,2,10))res1 = torch.matmul(a,b.transpose(1,2))#res1 = torch.matmul(a,b.transose(1,2))print res1.size()output: [torch.FloatTensor of size 2x3x2]]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从boost到gbdt]]></title>
    <url>%2F2018%2F09%2F23%2F%E4%BB%8Eboost%E5%88%B0gbdt%2F</url>
    <content type="text"><![CDATA[从boost到gbdt1.提升算法(boost) ｂｏｏｓｔ通过不停地改变各个样本的权值分布，每次学习一个弱分类器，再将这些弱分类器的结果进行组合 提升算法核心问题： １. 在每轮学习过程中，如何改变样本的权重.２. 如何将各个弱分类器进行组合 特点： 多个分类器使用相同的训练集 boost有效的减小偏差bais 2.Adaboostboost算法最典型的代表是Adaboost算法，其在对提升算法核心问题的处理方式是： 1.改变样本权重的方式：提高上一轮分错的样本的权重，减小上一分类正确的样本的权重2.各个弱分类器组合方式:各个分类器进行的结果进行加权组合 算法流程： 初始化样本权重，将全部样本权重初始化为1/N得到初始权值D1 循环M次，生成M个弱分类。对m=1，2…,M a. 使用权值Dm的训练数据，生成弱分类器 Gm b. 计算Gm在训练数据集上的分类误差em c. 根据em计算当前分类器在最终表决中的权重αm ​ αm=1/2log(1-em/em) d. 更新训练集样本权值分布 得到最终分类器：G(x) = sign( ΣαmGm(x) ) 3.回归树()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2F%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[常见机器学习机基本问题1.参数模型和非参数模型的区别？ 参数模型：在进行训练之前首先对目标函数的进行假设，然后从训练数据中学的相关函数的系数 典型的参数模型：LR、LDA(线性判别分析)、感知机、朴素贝叶斯、简单神经网络 参数模型的优点： 简单：容易理解和解释结果 快速：训练速度快 数据需求量少 参数模型的局限性： 模型的目标函数形式假设大大限制了模型 由于参数模型复杂度一般不高，因此更适合简单问题 非参数模型：不对目标函数的形式做出任何强烈的假设的算法，可以在训练集中自由的学习任何函数形式 典型的非参数模型:KNN、决策树、SVM 非参数学习模型的优点： 灵活性强，可拟合各种不同形式的样本 性能：模型效果一般较好 非参数学习模型的局限性 训练数据需求量大 训练速度慢，因为一般非参数模型要训练更多的参数 可解释性差 更容易出过拟合 2.生成模型和判别模型 由生成方法生成的模型成为生成模型，由判别方法产生的模型成为生成模型。下面重点介绍两种方法。 生成方法:由数据学联合概率分布P(X,Y)，然后求出条件概率P(Y|X)作为预测模型,即生成模型。（之所以称为生成方法是因为模型表示了给定输入X产生出Y的生成关系） 典型的生成模型:朴素贝叶斯、隐马尔科夫链 生成方法特点: 可还原联合概率分布 收敛速度更快 生成方法可处理隐变量，但判别方法不能 判别方法：由数据直接学习决策函数f(x)或者条件概率分布f(Y|X)作为预测模型，即判别模型。 典型的判别模型:KNN、感知机、决策树、LR 判别模型的特点： 直接学习决策函数或条件概率，直接面对预测，准确率更高 由于直接学习决策函数或条件概率，可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题 3.常见损失函数以及应用？ 逻辑斯特损失函数： 对数似然损失 合页损失 指数损失 4.朴素贝叶斯“朴素”表现在哪里？“朴素”主要表现在它假设所有特征在数据集中的作用是同样且独立的，而在真实世界中这种假设是不成立的，因此称之为朴素贝叶斯。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化之散点图和折线图]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8B%E6%95%A3%E7%82%B9%E5%9B%BE%E5%92%8C%E6%8A%98%E7%BA%BF%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[数据可视化之散点图和折线图画图基本常用参数plt.figure(figure_size=(30,20)) 指定图片大小 plt.style.use(&apos;ggplot&apos;) 指定图片风格 plt.title(&quot;image title&quot;,fontsize=30) 指定图片标题 指定坐标轴相关my_x_trick = np.arrange(0,200,10) plt.xtricks(my_x_trick,fontsize=20,rotation) 指定x轴，fontsize指定坐标轴字体，rotation指定文字旋转的角度 plt.ytricks(fontsize=20) 指定y轴 指定画图类型1.折线图plt.plot(x,y) #不指定画图种类时默认为折线图 plt.legend(loc = &quot;best&quot;,fontsize=40,shadow=1) #进行图例格式设定 plt.show() 折线图中plot可用参数：1.color=’red’ 指定折线的颜色2.label=’price’ 指定改颜色的图例表示3.marker=’-‘ 设置折现格式，默认为’-‘,注意这里设置范围不要越界，当设置越界时转换其他图 在一个文件中多次调用plt.plot(),使用不同的数据指定不同颜色和label可在一个图中画多条折线进行对比 2.散点图方式一： plt.scatter(x1,x2,marker=&apos;o&apos;) #指定画散点图，marker为点的形状 plt.show() 方式二： plt.plot(x1,x2,marker=&apos;o&apos;) #plot使用marker=‘o’则为散点图 plt.show() 在实际情况中第二种方式更为灵活，因此我们下重点介绍第二种方式的参数情况。 散点图中常用参数（方式二）： markerfacecolor 散点内部颜色 markeredgecolor 散点边缘颜色 markersize 散点大小 下面我们以DBSCAN聚类后的结果进行将为可视化为例进行效果展示： from sklearn.manifold import TSNE #使用TSNE进行降维 tsne = TSNE(learning_rate=100) x = tsne.fit_transform(input) labels = dbscan.labels_ #获取最终的预测结果 unique_labels = set(dbscan.labels_) colors = plt.cm.Spectral(np.linspace(0,1,len(set(dbscan.labels_)))) #生成和标签种类数相同的颜色数组 core_samples_mask =np.zeros_like(dbscan.labels_,dtype=bool) core_samples_mask[dbscan.core_sample_indices_] = True #将核心对象点对应的位置置true plt.style.use(&apos;ggplot&apos;) plt.figure(figsize=(30,20)) for k,col in zip(unique_labels,colors): if k==-1: col=&apos;k&apos; class_member_mask = (labels==k) xy = x[class_member_mask &amp; core_samples_mask] plt.plot(xy[:,0],xy[:,1],&apos;o&apos;,markerfacecolor=col,markeredgecolor=&apos;k&apos;,markersize=10)]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F23%2FDBSCAN%E5%92%8CKMeans%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[DBSCAN和KMeans相关资源和理解1.DBSCANDBSCAN是是一种典型的密度聚类算法，算法主要的思是由密度可达关系导出最大的密度相连样本集合，将其作为一个类。​ 主要参数： 最小分类样本数 半径 DBSCAN算法为有参数算法，聚类的最终结果很大程度上取决于参数的设定 DBSCAN算法不需要指定聚类个数，聚类个数根据算法和数据情况自动获得 DBSCAN聚类过程 首先根据半径画每个点的邻域，当点的邻域内点的个数大于最小样本数时，该点位为核心对象（原始数据集重点的变为核心对象和一般点） 随机确定一个核心点作为初始点，将该初始点全部的最大密度相连的点作为一类。 将分好类样本从原始的样本集中除去，从新选择核心对象作为聚类中心，再进行2.3操作，直至全部核心对象都被分类 DBSCAN代码实现from sklearn.cluster import DBSCAN dbcscan = DBSCAN(min_samples=30,eps=1.8) predict = dbscan.fit_predict(imput) 2.K-MeansKMeans是一种原始性聚类算法，算法主要思想是通过迭代过程把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使生成的每个聚类内紧凑，类间独立。这一算法不适合处理离散型数据，对连续性数据具有良好的效果 Kmeans为无参数算法，算法执行过程中不需要进行调参 Kmeans算法需要指定聚类个数K，这在实际问题中是很难进行确定的 KMeans聚类过程 根据指定的K值随机寻找K个点作为初始中心，将其他样本分别分给这些中心 由分好的类计算均值作为其该类新的中心，重新对各个样本分到距离最近的中心，重复这一过程，直至中心不再变化 Kmeans代码实现from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=8) predict = kmeans.fit_predict(input) Kmeans、DBSCAN优缺点对比DBSCAN的主要优点有： 1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 DBSCAN的主要缺点有： 1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。 3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树模型之CART树和C5.0]]></title>
    <url>%2F2018%2F09%2F23%2F%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E4%B9%8BCART%E6%A0%91%E5%92%8CC5-0%2F</url>
    <content type="text"><![CDATA[决策树模型之CART树和C5.0树模型基本思想：计算结点的纯度来选择最具显著性的切分不同树模型之间的差异：差异在于衡量纯度变化的标准不同 CART树：Gini系数C5.0树：信息熵增益 1.回归树(CART树)回归树也成为分类回归树，是一种既可用于分类也可用于回归的算法。 CART树分类的主要步骤：1. 决策树的生成：递归的构建而决策树的过程，基于训练数据生成决策树，生成的决策树数量应尽量大。 自上而下的从根开始建立节点，在每个节点处选择一个最好的属性来分类，使子节点红的训练集尽可能的顿。 不同算法使用不同的指标来衡量“最好”： 分类算法：一般选择Gini系数 回归算法：使用最小二乘偏差（LSD）或最小绝对偏差（LSA） 2.决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树这时损失函数最小做为标准 分类树的生成 对每个特征A，对它所有的可能取值a，将数据集划分为A=a和A!=a两个部分计算集合D的基尼指数： 遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。 对上述两个子节点递归调用步骤(1)(2), 直到满足停止条件。 生成CART树 基尼指数： 是一种不等度的度量 是介于0~1之间的数，0-完全相等，1-完全不相等 总体内包含的类别越杂乱，Gini指数就越大 分类问题中，假设存在K个类，样本属于第k个类的概率为pk，则概率分布的Gini指数为： 样本集合D的Gini指数为：​ 当在数据集D上根据某一取值a进行分割，得到D1、D2两部分后，那么特征A下集合D的Gini指数为：​ 算法停止条件： 节点中样本个数小于预定阈值 样本的Gini指数小于阈值 没有更多特征 剪枝在完整的的决策树上，减掉一些完整的子支是决策树变小，从而防止决策树过拟合。 决策树很容易产生过拟合，改善的方式包括： 通过阈值控制终止条件，防止分支过细 对决策树进行剪枝 建立随机森林 2.C5.0节点分裂标准：信息增益比 C系列决策树发展过程： 阶段一：ID3 节点选择标准：信息增益 缺陷：1. 方法会倾向与属性值比较多的变量（如省份字段存在31个水平，性别由两个水平，信息增益会考虑选择省份做特征节点 2.构造树时不能很好地处理连续变量 阶段二：C4.5 节点选择标准：信息增益比（避免了偏袒） 缺点：运行效率很低 阶段三：C5.0 商业版的C4.5，提升了算法效率，但没有公布具体算法细节 C5.0算法特点1.C5.0是一种多叉树。 如果根节点或者中间节点为连续变量，则改变量一定会一分为二成为两个分支；如果根节点或者中间节点为离散变量，则分开离散变量水平数个分支；因此一个变量一旦被使用，后面节点都不会在使用该变量。 2.C5.0生成树采用信息增益比进行分裂节点的选择3.剪枝采用“减少误差”法和“减少损失”法进行。 减少误差法：核心思想是比较剪枝前后的误差率 误差率的计算：如果第i个节点中包含N个样本，其中预测错误的样本量为M，则该节点的错误率为f=M/N 减少损失法：该方法结合损失矩阵对树进行剪枝，核心是剪枝前后的的损失量。 4.C5.0只能解决分类问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github+hexo博客使用]]></title>
    <url>%2F2018%2F09%2F23%2FGithub-hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.创建新博文页面 创建博文页面首先要cd 到博客的根目录下，然后运行命令： 1hexo new &quot;页面名称&quot; 这样则在博客站点的source/_posts/文件夹下创建了指定的“页面名称.md”文件，可以直接对其进行编辑。完成编辑后就可已使用下面的同步命令来将修改提交到GIthub上： 1hexo d -g #生成静态文件并部署到服务器，g是generate代表生成文件，d是depoly代表部署到服务器]]></content>
      <categories>
        <category>博客相关</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F09%2F23%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[文件管理1.查看当前文件夹中文件 #显示文件夹中全部文件和目录数 ls | wc -l #显示包含指定内容的文件和目录 ls *3094 | wc -l #显示文件夹中文件的个数 find . -type f | wc -l 注：wc 是统计前面管道输出的东西，-l表示按照行统计 磁盘管理1.查看文件夹中文件的总大小 #查看当前文件夹中文件总大小 du -h # -h表示以人可理解的方式输出 #查看指定文件夹中文件的总大小 du /home/yhk/ -h ​2.查看磁盘各个分区大小及使用情况​ df -h 内存、Cpu使用情况查看1.Cpu个数 逻辑Cpu个数查看： 1.方式一： 先使用top密令进入top界面，在界面中按1，即可出现cpu个数以及使用情况 2.方式二： cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l ​ 物理CPU个数查看： cat /proc/cpuinfo |grep &quot;physical id&quot;|sort |uniq|wc -l 一个物理CPU数几核：​ cat /proc/cpuinfo |grep “cores”|uniq 2.CPU内存使用情况 top 实用程序以忽略挂起信号方式执行​ nohup command &gt; myout.file 2&gt;&amp;1 &amp; #文件产生的输出将重定向到myout.file ​​​​ ​​​]]></content>
      <categories>
        <category>Linux操作</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>服务器管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch入门学习心得]]></title>
    <url>%2F2018%2F09%2F23%2FPytorch%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[全连接1.DropoutDrop是一种现在在深度学习中使用最为广泛的防止过拟合的方式 核心思想:再训练神经网格时的时候依据概率P保留每个神经元的权重，也就是说每次训练的时候都会有一些神经元被置0，这样就保证神经网络神经网络不会过度学习 注意：我们只是在训练的时候使用dropout去使一些神经元不参与训练，但是在预测阶段会使用全部的神经元参与预测 使用情况：卷积神经网路只在最后的全连接层中使用dropout，循环神经网络一般只在不同层循环结构体之间使用dropout ２.Batch Normalization核心思想：将标准化应用的整个前向传播和反向传播的过程中。传统的标准化一般只是在输入数据之前将数据进行标准化处理，而批标准化是在神经网络的训练过程中对每层的输入数据都进行一个标准化 使用位置:线性层和非线性层之间 作用：1.加快收敛速度 2.防止过拟合 ​ ３.从神经网络角度看线性回归和逻辑回归的区别？ 丛神经网络角度上看，逻辑回归只是在线性回归的基础上计入了一层Sigmod激活函数。 ４.全连接网络设计趋势1. 使用线性层和非线性激活函数交替的结果来代替线性层交替的结构往往能大大提升准确率2. 在线性层和非线性激活函数之间加入批标准化处理加快收敛速度 CNN卷积层卷积层可以看作是多个滤波器的集合，滤波器在深度上要和输入数据保持一致，让每个滤波器在宽度和深度高度上进行滑动卷积，然后计算整个滤波器和输入数据任意一处的内积，输出数据的深度和滤波器的个数保持一致 1.卷积层为什么有效？(1).局部性判断题图片的类型并不是根据整张图片来决定的，而是由一定的局部区域决定的 (2).相同性对于不同的图片，如果他们属于同一类，他们将具有相同的特征，但这些特征可能属于图片的不同位置，但是在不同位置的检测方式几乎是一样的 (3).不变性当我们对图片进行下采样时，图片的基本性质不变 2.卷积层的参数 关键参数: in_channels ：特征的管道数，彩色图片为3，黑白图片则为1 out_channels : 输出管道个数，也就是滤波器个数 kernel_size : 卷积核大小 可选参数： padding:边界填充0的层数 stride:步长 bias: 是否使用偏置，默认是True 3.卷积层的输入输出 输入： 卷积层的输入格式为(batch,channels,width,heights) 输出： 卷积层的输出取决于输入数据大小W、卷积核大小F、步长S、0填充个数P等四个方面，计算公式如下： W-F+2P/S + 1 这里在介绍几种常用的卷积层参数设置： 卷积核大小 0填充层数 步长 卷积层输出 3 1 1 保持输入维度不变 3 0 1 输入维度减2 一般卷积核大小不超过5 4.卷积层的参数共享基于特征的相同性，因此可以使用相同的滤波器来检测不同位置的相同特征，参数共享共享机制有效的减少卷积层的参数个数，加快了卷积神经网络的训练速度。 使用参数共享机制的CNN层参数个数为： 滤波器个数（out_dim） 神经元大小（kernel_size kernel_size * input_dim） 例如：当卷积层的的输出是20 20 32，窗口大小是3 3，输入数据深度是10时，当不适用参数共享时神经元个数为20 20 32，每个神经元个数为3 3 10，总参数个数为12800 900 =11520000个参数。但当我们使用参数共享时，因为输出深度为32，所以存在32个滤波器，每个滤波器存在参数3 3 10个，而总参数个数即为90 * 32个，大大减少了参数的个数 池化层1.使用池化层有什么作用？ 有效的缩小矩阵的尺寸 加快计算速度 防止过拟合 2.池化层的参数设置 关键参数： kernel_size ：池化层的大小 池化层也可也进行0填充，但是几乎不用 池化层最常用的池化方式以及参数设置： 池化类型 卷积核大小 步长 池化层输出 MaxPooling 2 2 输入维度的一半 注意：池化层只能改变高度和宽度，不能改变深度；卷积层即可改变数据的宽度和高度也可以改变数据的深度 经典卷积设计的趋向1. 使用小滤波器2. 多个卷积层和非线性激活层交替的结构比单一的卷积层结构能更加有效的提取出更深层次的特征，并且参数个数更少 RNN1.RNN模型的超参数 关键参数： input_size:输入的维度 hidden_size：隐藏层维度，也是最终输出的维度 num_layers: RNN层数可选参数: batch_first : 将输入输出的batch放在最前面，顺序为（batch,seq,feature） bidirectional: True表示双向循环神经网络，默认为False dropout: 参数接受一个0~1之间的一个值，会将网路中出最后一层外加入dropout层 2.RNN模型的输入 RNN模型的输入为:(seq,batch,feature),这里要重点注意，在建立模型时可使用batch_first将顺序变成正常的(batch,seq,feature)． 其中的含义为: batch: 样本个数 seq: 每个样本依据附近的样本个数 feature: 每个样本特征数 其实RNN的网络中需要两个输入，上面的序列输入是主要输入，必须进行人工指定，还有一个起始状态输入，可选进行输入，不指定则默认为全0 3.RNN模型的输出 RNN的模型输出分为实际输出output和记忆状态h两部分。其中各自的形式和表达如下： 实际输出output: 维度：(seq,batch,hiddendirection)记忆状态： 维度：(layers direction,bactch,hidden) 注：使用batch_first可将batch放在最前面 4.RNN使用作词性判断 因为RNN可根据上下文进行输出，因此使用RNN模型根据上下文的词性判断某个词的词性比直接根据该单词判断效果更好。 训练集：​ 输入：给定的句子​ 标签：句子中每个单词的词性 基本原理： 首先我们使用word_embedding将句子中的每个词进行词向量化，如：The dog ate apple 转化成4 word_dim 的词向量 x = x.embedding(x) 因为lstm需要的输入形式为3维，因此我们要将其转换为1 4 word_dim x = x.unsqueeze(0) 再将其输入到lstm模型中，得到模型的实际输出维度为：batch seq * hidden_dim output 因为我们需要判断是的最后一个词的词性，因此我们只需要取最后一个seq就好了 output[:,-1,:] 因为一个单词的词性不只与其上下文关系有关，还与其单词的字母排列情况有关，因此我们可以上面的基础上增加字符角度的lstm来进行表征其字母排列情况。 完善方案： 遍历句子The dog ate apple中的每个单词： 将单词中的每个字母进行词向量表示，如apple转化成5 char_dim的词向量 char_info = nn.embedding(x) 将其转换为３维：１ 5 char_dim char_info = char_info.unsqueeze(0) 将模型输入lstm模型，但这里取记忆状态作为输出,输出状态是h0维度为(1,1,hidden_dim) _,h = char_lstm(char_info) h[0] 将各个单词的输出组合成一个向量，按照seq进行拼接,形成一个1 4 hidden_dim的向量 for word in words: char_seq = make_sequeece(word,char_to_idx) char_seq = self.char_lstm(char_seq) word_set.append(char_seq) char = torch.stack(word_set,1) 根据前面基本方法将单词进行向量化，得到1 4 word_dim维向量，将其与字符级别的lstm结果从feature维度进行拼接，得到1 4 char_hidden+word_dim维向量 x = torch.cat((x,char),dim=2) 最后将两个角度的得到的特征一起输入的最终的lstm模型，在经过全连接层得到最终结果]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
</search>
