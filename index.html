<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="AnchoretY&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="AnchoretY&#39;s blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AnchoretY&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>AnchoretY's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/AnchoretY">
	<img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_green_007200.png" alt="Fork me on GitHub">
    </a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AnchoretY's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/深度学习-BN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/深度学习-BN/" itemprop="url">深度学习-BN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-28T16:11:20+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="为什么要进行归一化？"><a href="#为什么要进行归一化？" class="headerlink" title="为什么要进行归一化？"></a>为什么要进行归一化？</h5><blockquote>
<p>​    原因在于神经网络的本身就在于学习数据的分布，一旦训练数据和测试数据分布不同，那么网络的<strong>泛化能力也将大大降低</strong>；另外一方面，再使用BSGD时一旦每批训练数据的分布不相同，那么网络在每次进行迭代时都要去适应不同的数据分布，这<strong>将大大降低网络的学习速度</strong>。</p>
</blockquote>
<h5 id="为什么要使用BN？"><a href="#为什么要使用BN？" class="headerlink" title="为什么要使用BN？"></a>为什么要使用BN？</h5><blockquote>
<p>​    这主要是因为对于一般的归一化，只是在输入网络之前对数进行了归一化，而在神经网络的训练过程中并没有对数据做任何处理，而在神经网络的的训练过程中只要网络的前面几层的数据分布发生微小的变化，那么后面的网络就会不断积累放大这个分布的变化，因此一旦有任意一层的数据发生改变，这层以及后面的网络都会需要去从新适应学习这个新的数据分布，而如果训练过程中，每一层的数据都在不断发生变化，那么更将大大影响网络的训练速度，因此需要在网络的每一层输入之前都将数据进行一次归一化，保证数据分布的相同，<strong>加快网络训练速度</strong>。</p>
<p>​    在另一方面，由于将网络的每一步都进行了标准化，数据分布一致，因此模型的泛化能力将更强。</p>
</blockquote>
<h5 id="BN的本质是什么？"><a href="#BN的本质是什么？" class="headerlink" title="BN的本质是什么？"></a>BN的本质是什么？</h5><blockquote>
<p>一个<strong>可学习</strong>、<strong>有参数（γ、β）</strong>的使每层数据之前进行归一化的网络层</p>
</blockquote>
<h5 id="BN使用位置"><a href="#BN使用位置" class="headerlink" title="BN使用位置"></a>BN使用位置</h5><blockquote>
<p>线性层后全连接层之前</p>
</blockquote>
<h5 id="BN过程"><a href="#BN过程" class="headerlink" title="BN过程"></a>BN过程</h5><blockquote>
<p>对于一般的归一化没使用下面的公式进行归一化计算：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png?raw=true" alt=""></p>
<p><strong>但是如果仅仅使用上面的公式来对某层的输出做下一层的输入做归一化，那么是会影响到前面一层学习到的特征的。</strong>例如：网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，强制把它归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被搞坏了。因此，<strong>BN引入了可学习的参数γ、β</strong>：</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/BN%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png?raw=true" alt=""></p>
<p>​    上面的公式表明，<strong>通过学习到的重构参数γ、β，是可以恢复出原始的某一层所学到的特征的。</strong></p>
</blockquote>
<h5 id="BN中为什么要在后面γ、β？不加可以吗？"><a href="#BN中为什么要在后面γ、β？不加可以吗？" class="headerlink" title="BN中为什么要在后面γ、β？不加可以吗？"></a>BN中为什么要在后面γ、β？不加可以吗？</h5><blockquote>
<p>​    不可以，因为这是BN中的最关键步骤。不使用γ、β会造成归一化的同时破坏前一层提取到的特征，而BN通过记录每个神经元上的γ、β，使前一层的特征可以通过γ、β得以还原。</p>
</blockquote>
<h5 id="BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？"><a href="#BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？" class="headerlink" title="BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？"></a>BN层是对每一个神经元归一化处理，那在CNN的BN层是怎么应用的？是不参数个数会非常多？</h5><blockquote>
<p>​    对于CNN上采用了类似权值共享的策略，<strong>将一个特征图看做一个神经元</strong>，因此参数个数并不会很多。</p>
<p>例如：如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,w,h)，m为min-batch sizes，f为特征图个数，w、h分别为特征图的宽高。在CNN中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m.w.h，于是对于每个特征图都只有一对可学习参数：γ、β，总参数个数也就是2m个。</p>
</blockquote>
<h5 id="BN的作用"><a href="#BN的作用" class="headerlink" title="BN的作用"></a>BN的作用</h5><blockquote>
<p>1.防止过拟合。有了BN，dropout和正则化的需求下降了</p>
<p>2.加速训练</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/19/机试-二叉树遍历/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/19/机试-二叉树遍历/" itemprop="url">机试-二叉树遍历</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-19T23:02:04+08:00">
                2019-03-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    二叉树最常用的遍历算法主要分为下面几种：</p>
<p>​    <strong>1.先序遍历</strong></p>
<p>​    <strong>2.中序遍历</strong></p>
<p>​    <strong>3.后序遍历</strong></p>
<p>​    <strong>4.层次遍历</strong></p>
<p>​    下面我们将针对这些遍历算法的递归与非递归实现分别给出代码实现以及特点。</p>
<blockquote>
<p>这里有一点我们需要注意:</p>
<p>​    无论是前序、中序、后续，都是指根节点访问的顺序，<strong>而左右节点的相对访问顺序永远是相同的，即先访问做节点，后访问右节点。</strong></p>
</blockquote>
<h3 id="先序遍历"><a href="#先序遍历" class="headerlink" title="先序遍历"></a>先序遍历</h3><p>​    先序遍历指在二叉树便利过程中首先输出根节点，然后再分别输出左右节点的遍历方式。</p>
<p>#####递归实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">core</span><span class="params">(result,root)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root==<span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            result.append(root.val)</span><br><span class="line">            core(result,root.left)</span><br><span class="line">            core(result,root.right)</span><br><span class="line">        </span><br><span class="line">        result = []</span><br><span class="line">        core(result,root)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h5 id="非递归实现"><a href="#非递归实现" class="headerlink" title="非递归实现"></a>非递归实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root==<span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        stack = []</span><br><span class="line">        res = []</span><br><span class="line">        stack = [root]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            node = stack.pop()</span><br><span class="line">            res.append(node.val)</span><br><span class="line">            <span class="comment">#注意这里的顺序一定是先右后左，和一般的相反</span></span><br><span class="line">            <span class="keyword">if</span> node.right!=<span class="keyword">None</span>:</span><br><span class="line">                stack.append(node.right)</span><br><span class="line">            <span class="keyword">if</span> node.left!=<span class="keyword">None</span>:</span><br><span class="line">                stack.append(node.left)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="中序遍历"><a href="#中序遍历" class="headerlink" title="中序遍历"></a>中序遍历</h3><p>​    二叉树的中序遍历是指现先遍历左节点，中间遍历根节点，最后在遍历右节点的便利方式。</p>
<h4 id="递归实现"><a href="#递归实现" class="headerlink" title="递归实现"></a>递归实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Core</span><span class="params">(root)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root==<span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> []</span><br><span class="line">            </span><br><span class="line">            Core(root.left)</span><br><span class="line">            result.append(root.val)</span><br><span class="line">            Core(root.right)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        </span><br><span class="line">        result = []</span><br><span class="line">        Core(root)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h4 id="非递归实现-1"><a href="#非递归实现-1" class="headerlink" title="非递归实现"></a>非递归实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root==<span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        stack = []</span><br><span class="line">        result = []</span><br><span class="line">        </span><br><span class="line">        pos = root</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">or</span> pos:</span><br><span class="line">            <span class="keyword">if</span> pos:</span><br><span class="line">                stack.append(pos)</span><br><span class="line">                pos = pos.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos = stack.pop()</span><br><span class="line">                result.append(pos.val)</span><br><span class="line">                pos = pos.right</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="后序遍历"><a href="#后序遍历" class="headerlink" title="后序遍历"></a>后序遍历</h3><h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><h4 id="非递归实现-2"><a href="#非递归实现-2" class="headerlink" title="非递归实现"></a>非递归实现</h4><p>​    利用<strong>队列</strong>先进先出的特点，依次将结点的左、右孩子入队，然后依次出队访问，以此为循环。当有些题目中要求按照层输出时，需要根据每层的节点个数做一个计数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        </span><br><span class="line">        queue = [root]</span><br><span class="line">        result = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            tmp = []</span><br><span class="line">            number_flag = len(queue)   <span class="comment">#层节点个数计数器</span></span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i&lt;number_flag:</span><br><span class="line">                node = queue.pop(<span class="number">0</span>)</span><br><span class="line">                tmp.append(node.val)</span><br><span class="line">                <span class="keyword">if</span> node.left:</span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            result.append(tmp)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="根据两个序列复原二叉树"><a href="#根据两个序列复原二叉树" class="headerlink" title="根据两个序列复原二叉树"></a>根据两个序列复原二叉树</h3><p>​    这种题目其实只有两个，核心是找出先根据一个序列找出根节点，然后在根据另一个序列找出其左右子树的元素，然后不断的递归这个过程即可。</p>
<h5 id="已知前序遍历中序遍历"><a href="#已知前序遍历中序遍历" class="headerlink" title="已知前序遍历中序遍历"></a>已知前序遍历中序遍历</h5><p>​    在<strong>已知前序遍历的题目中，就以前序遍历为基础，去不断地区分剩下的数据应该在左子树还是右子树即可</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(self, preorder: List[int], inorder: List[int])</span> -&gt; TreeNode:</span></span><br><span class="line">				<span class="string">"""</span></span><br><span class="line"><span class="string">					先将前序遍历的第一个节点作为根节点，然后在后序遍历中找到其对应的位置，左右分别做相同的操作</span></span><br><span class="line"><span class="string">				"""</span></span><br><span class="line">        len_pre = len(preorder)</span><br><span class="line">        len_in = len(inorder)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len_pre==<span class="number">0</span> <span class="keyword">or</span> len_in==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        tree_root = TreeNode(preorder[<span class="number">0</span>])</span><br><span class="line">        preorder = preorder[<span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        left_len = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> inorder:</span><br><span class="line">            <span class="keyword">if</span> i==tree_root.val:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left_len+=<span class="number">1</span></span><br><span class="line">        inorder.remove(tree_root.val)</span><br><span class="line">        <span class="keyword">if</span> left_len&gt;=<span class="number">1</span>:</span><br><span class="line">            tree_root.left =  self.buildTree(preorder[:left_len],inorder[:left_len])</span><br><span class="line">        <span class="keyword">if</span> len(preorder)-left_len&gt;=<span class="number">1</span>:</span><br><span class="line">            tree_root.right = self.buildTree(preorder[left_len:],inorder[left_len:])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tree_root</span><br></pre></td></tr></table></figure>
<h5 id="已知前序遍历和后序遍历"><a href="#已知前序遍历和后序遍历" class="headerlink" title="已知前序遍历和后序遍历"></a>已知前序遍历和后序遍历</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">constructFromPrePost</span><span class="params">(self, pre, post)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type pre: List[int]</span></span><br><span class="line"><span class="string">        :type post: List[int]</span></span><br><span class="line"><span class="string">        :rtype: TreeNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            前序遍历的第一个节点必定是根节点，随后的节点就是其左子树的根节点，然后再在</span></span><br><span class="line"><span class="string">        后序遍历中找到这个节点的位置就可以确定左子树中有哪些节点，右子树中有哪些节点</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        tree_root = TreeNode(pre[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        pre = pre[<span class="number">1</span>:]</span><br><span class="line">        post = post[:<span class="number">-1</span>]</span><br><span class="line">        left_len = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> post:</span><br><span class="line">            <span class="keyword">if</span> i==pre[<span class="number">0</span>]:</span><br><span class="line">                left_len+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left_len+=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> left_len&gt;=<span class="number">1</span>:</span><br><span class="line">            tree_root.left = self.constructFromPrePost(pre[:left_len],post[:left_len])</span><br><span class="line">        <span class="keyword">if</span> len(post)-left_len&gt;=<span class="number">1</span>:</span><br><span class="line">            tree_root.right = self.constructFromPrePost(pre[left_len:],post[left_len:])</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> tree_root</span><br></pre></td></tr></table></figure>
<h5 id="已知中序后序遍历构造二叉树"><a href="#已知中序后序遍历构造二叉树" class="headerlink" title="已知中序后序遍历构造二叉树"></a>已知中序后序遍历构造二叉树</h5><pre><code>没有前序遍历时，使用后序遍历定根节点     
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(self, inorder: List[int], postorder: List[int])</span> -&gt; TreeNode:</span>  </span><br><span class="line">	len_in = len(inorder)</span><br><span class="line">  len_post = len(postorder)</span><br><span class="line">  <span class="keyword">if</span> len_in==<span class="number">0</span> <span class="keyword">or</span> len_in!=len_post:</span><br><span class="line">      <span class="keyword">return</span>  <span class="keyword">None</span></span><br><span class="line">  </span><br><span class="line">  tree_root = TreeNode(postorder[<span class="number">-1</span>])</span><br><span class="line">  postorder = postorder[:<span class="number">-1</span>]</span><br><span class="line">  left_len = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> inorder:</span><br><span class="line">      <span class="keyword">if</span> i==tree_root.val:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          left_len += <span class="number">1</span></span><br><span class="line">  </span><br><span class="line">  inorder.remove(tree_root.val)</span><br><span class="line">  <span class="keyword">if</span> left_len&gt;=<span class="number">1</span>:</span><br><span class="line">      tree_root.left = self.buildTree(inorder[:left_len],postorder[:left_len])</span><br><span class="line">  <span class="keyword">if</span> len(postorder)-left_len&gt;=<span class="number">1</span>:</span><br><span class="line">      tree_root.right = self.buildTree(inorder[left_len:],postorder[left_len:])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> tree_root</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/12/机试-回文子串相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/12/机试-回文子串相关/" itemprop="url">机试-回文子串相关</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-12T21:27:59+08:00">
                2019-03-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="回文子串"><a href="#回文子串" class="headerlink" title="回文子串"></a>回文子串</h4><blockquote>
<p>例：给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。</p>
<p>具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被计为是不同的子串。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSubstrings</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        length = len(s) </span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,length+<span class="number">1</span>): <span class="comment">#这里注意循环的范围为range(i+1,length+1)</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> s[i:j]==s[i:j][::<span class="number">-1</span>]:</span><br><span class="line">                    result += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h4 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a>最长回文子串</h4><p>​    最长回文子串也是回文串中常见的一中题目，下面是例题</p>
<blockquote>
<p>例：给定一个字符串 <code>s</code>，找到 <code>s</code> 中最长的回文子串。你可以假设 <code>s</code> 的最大长度为 1000。</p>
</blockquote>
<blockquote>
<p>思路一：Manacher算法</p>
<p>​    首先先将字符串首尾以及字符和字符之间采用”#“进行补齐，补齐后的字符串总长度2n+1(n为原始字符串长度)。然后从第一个非#字符</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_length</span><span class="params">(string, index)</span>:</span></span><br><span class="line">            <span class="comment"># 循环求出index为中心的最长回文字串</span></span><br><span class="line">            length = <span class="number">0</span></span><br><span class="line">            seq = <span class="string">""</span></span><br><span class="line">            <span class="keyword">if</span> string[index]!=<span class="string">"#"</span>:</span><br><span class="line">                seq = string[index]</span><br><span class="line">                length = <span class="number">1</span></span><br><span class="line">            string_len = len(string)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,index+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> index+i&lt;string_len <span class="keyword">and</span> string[index-i]==string[index+i]:</span><br><span class="line">                    <span class="comment"># print(string[index-i],seq+string[index+i])</span></span><br><span class="line">                    <span class="keyword">if</span> string[index-i]!=<span class="string">"#"</span>:</span><br><span class="line">                        length +=<span class="number">2</span></span><br><span class="line">                        seq = string[index-i]+seq+string[index+i]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">return</span> length,seq</span><br><span class="line">        </span><br><span class="line">        s_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> s]</span><br><span class="line">        string = <span class="string">"#"</span>+<span class="string">"#"</span>.join(s)+<span class="string">"#"</span></span><br><span class="line">        </span><br><span class="line">        length = len(string)</span><br><span class="line">        max_length = <span class="number">0</span></span><br><span class="line">        max_seq = <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>,length):</span><br><span class="line">            <span class="comment"># print("====")</span></span><br><span class="line">            tmp_len,tmp_seq = get_length(string,index)</span><br><span class="line">            <span class="comment"># print(tmp_len,tmp_seq)</span></span><br><span class="line">            <span class="keyword">if</span> tmp_len&gt;max_length:</span><br><span class="line">                max_length = tmp_len</span><br><span class="line">                max_seq = tmp_seq</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> max_seq</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思路二：动态规划</p>
<p>​    这里的动态规划的核心思路就是从头开始向后进行遍历，每次想看<strong>头尾同时加入比最大之前最大回文子串的长多+1</strong>字符串是不是回文子串(注意但是首部索引不能超过0)，如果是则记录起始节点start，max_len的值+2；否则判断只在尾部进行字符串加1的字符串时不是回文子串（这里之说以不必尝试在头部加1，因为再从头开始遍历的过程中已经尝试了头部加1），如果是记录start节点，max_len的值+2</p>
<p>​    f(x+1)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        length = len(s)</span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            <span class="keyword">if</span> i-max_len&gt;=<span class="number">1</span> <span class="keyword">and</span> s[i-max_len<span class="number">-1</span>:i+<span class="number">1</span>]==s[i-max_len<span class="number">-1</span>:i+<span class="number">1</span>][::<span class="number">-1</span>]:</span><br><span class="line">                start = i-max_len<span class="number">-1</span></span><br><span class="line">                max_len += <span class="number">2</span></span><br><span class="line">            <span class="keyword">elif</span> i-max_len&gt;=<span class="number">0</span> <span class="keyword">and</span> s[i-max_len:i+<span class="number">1</span>]==s[i-max_len:i+<span class="number">1</span>][::<span class="number">-1</span>]:</span><br><span class="line">                start = i-max_len</span><br><span class="line">                max_len += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> s[start:start+max_len]</span><br></pre></td></tr></table></figure>
<h4 id="最长回文子序列516"><a href="#最长回文子序列516" class="headerlink" title="最长回文子序列516"></a>最长回文子序列516</h4><p>​    </p>
<p>z</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/12/机试-含环链表相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/12/机试-含环链表相关/" itemprop="url">机试-含环链表相关</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-12T14:48:34+08:00">
                2019-03-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    在含环的问题中，存在一些关键性的结论，在解决问题时非常有帮助，下面是一些相关的总结。</p>
<h4 id="1-判断链表是否有环"><a href="#1-判断链表是否有环" class="headerlink" title="1.判断链表是否有环"></a>1.判断链表是否有环</h4><p>​    结论：<strong>一个速度为1的low指针和一个速度为2的fast指针同时从头向前走，如果其中fast指针为None，那么则为无环，如果两个只能指向的元素相等，那么链表有环。</strong></p>
<h4 id="2-判断链表的环入口节点"><a href="#2-判断链表的环入口节点" class="headerlink" title="2.判断链表的环入口节点"></a>2.判断链表的环入口节点</h4><p>​    结论：函数一样的双指针进行遍历，如果fast指针为None,那么则为无环。如果两个指针指向的的元素相同，那么<strong>这个节点到链表入口点的长度</strong>和<strong>链表头到链表入口点的长度</strong>相等。</p>
<blockquote>
<p>推导过程：</p>
<p>​    设链表头到入口节点的长度为a</p>
<p>​           链表入口节点到相遇节点的长度为b</p>
<p>​        相遇节点到链表入口节点的长度为c</p>
<p>​    那么因为fast的速度为2，low的速度为1，因此可以认为low入环时走在前面，每次fast和low之间的距离缩小1，因此，必定会在第一圈完成之前相遇。所以有</p>
<p>​    low 在环内位置: (a+b)-a mod (b+c)  -&gt; b mod (b+c)</p>
<p>​    fast 在环内位置：2(a+b)-a mod (b+c) -&gt; a+2b mod (b+c)</p>
<p>二者应该相等，因此得出 a+b mod (b+c) = 0 即<strong>a = c</strong></p>
</blockquote>
<p>​    利用这个结论，我们可以先判断判断链表是否有环，如果有环，那么先找到相间的节点，然后再用一个新指针从头开始以速度为1和low指针从相交节点同时开始遍历，当两个点相交的节点即为环入口节点。</p>
<blockquote>
<p>例题：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 <code>null</code>.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detectCycle</span><span class="params">(head)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type head: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        low,fast = head,head</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.next <span class="keyword">and</span> fast.next:  </span><br><span class="line">            low, fast = low.next, fast.next.next</span><br><span class="line">            <span class="keyword">if</span> fast==low:</span><br><span class="line">                p = head</span><br><span class="line">                <span class="keyword">while</span> p!=low:</span><br><span class="line">                    p = p.next</span><br><span class="line">                    low = low.next</span><br><span class="line">                <span class="keyword">return</span> p</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<h4 id="3-变形型题目"><a href="#3-变形型题目" class="headerlink" title="3.变形型题目"></a>3.变形型题目</h4><p>​    有一类题目不会明显的说让解决环的问题，但是使用环来解决，往往会起到意想不到的效果。</p>
<blockquote>
<p>例题：编写一个程序，找到两个单链表相交的起始节点。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIntersectionNode</span><span class="params">(headA, headB)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type head1, head1: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> headA==<span class="keyword">None</span> <span class="keyword">or</span> headB==<span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#相判断两个是否相交</span></span><br><span class="line">        pA = headA</span><br><span class="line">        pB = headB</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> pA.next:</span><br><span class="line">            pA = pA.next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> pB.next:</span><br><span class="line">            pB = pB.next</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> pA!=pB:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将PA首尾相接</span></span><br><span class="line">        tail = pA</span><br><span class="line">        pA.next = headA</span><br><span class="line">        </span><br><span class="line">        fast = headB</span><br><span class="line">        low = headB</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            fast = fast.next.next</span><br><span class="line">            low = low.next</span><br><span class="line">            <span class="keyword">if</span> fast==low:</span><br><span class="line">                s = headB</span><br><span class="line">                <span class="keyword">while</span> s!=low:</span><br><span class="line">                    low = low.next</span><br><span class="line">                    s = s.next</span><br><span class="line">                tail.next = <span class="keyword">None</span></span><br><span class="line">                <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p>​    <strong>这道题利用了和上一道题目完全一样的规律解决</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/09/机试-搜索算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/09/机试-搜索算法总结/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-09T20:40:40+08:00">
                2019-03-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="正则化相关问题"><a href="#正则化相关问题" class="headerlink" title="正则化相关问题"></a>正则化相关问题</h3><hr>
<p>1.实现参数的稀疏有什么好处吗？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了。</span><br><span class="line">2.参数变少可以使整个模型获得更好的可解释性。</span><br></pre></td></tr></table></figure>
<p>2.参数值越小代表模型越简单吗？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是的。这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。</span><br></pre></td></tr></table></figure>
<p>3.模型简单包括什么？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.参数少</span><br><span class="line">2.参数值小</span><br></pre></td></tr></table></figure>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><hr>
<p><strong>机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作L1-norm和L2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。</strong></p>
<p>对于线性回归模型，使用L1正则化的模型建叫做<strong>Lasso回归</strong>，使用L2正则化的模型叫做<strong>Ridge回归</strong>（岭回归）</p>
<p><strong>概念</strong>：</p>
<blockquote>
<p>L1正则化是指权值向量绝对值之和，通常表示为||w||1</p>
<p>L2正则化是指全职向量w中各个元素的平方和让后再求平方根，通常表示为||w||2</p>
</blockquote>
<p>下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Lasso回归损失函数.png?raw=true" alt="img">﻿</p>
<p>下图是Python中Ridge回归的损失函数，式中加号后面一项α||w||22 即为L2正则化项</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Ridge回归损失函数.png?raw=true" alt=""></p>
<blockquote>
<p>注：<br>1.上面的两个函数前半部分可以为任意的线性函数的损失函数，组合成的函数都可以成为Lasso回归会Ridge回归<br>2.上面两个式子中的α为正则化系数，后续通过交叉验证确定</p>
</blockquote>
<p>注：上面两个式子中的α为正则化系数，后续通过交叉验证确定)</p>
<p>L1正则化与L2正则化的作用：</p>
<blockquote>
<p>L1正则化可产生稀疏权值矩阵，即<strong>产生一个稀疏模型，可用用于特征选择</strong></p>
</blockquote>
<blockquote>
<p>L2正则化主要用于<strong>防止过拟合</strong></p>
</blockquote>
<p>﻿</p>
<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><hr>
<p><strong>L1正则化的标准形式：</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/youdao/L1正则化公式.png?raw=true" alt=""></p>
<p>​    其中J0是原始的损失函数，加好后面是L1正则化项。机器学习的最终目就是找出损失函数的最小值，当我们在原本的损失函数后面加上L1正则化后，相当于对J0做了一个约束，另L1正则化项等于L，则 J=J0+L，<strong>任务转化为在L1的约束下求J0最小值的解</strong>。<br>​    考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，<strong>二者图像首次相交处即为最优解</strong>，获得下图：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/youdao/L1正则化可得到稀疏矩阵原因.png?raw=true" alt=""></p>
<p>​    从图中可看出<strong>j0与L相交于L的一个顶点处，这个顶点即为最优解</strong>。注意这个顶点的值为（w1,w2）=(0,w)，可以想象，在更多维的情况下，L将会有很多突出的角，而<strong>J与这些叫接触的几率将远大于与L其他部位接触的概率</strong>，而这些角上将会有许多权值为0，<strong>从而产生系数矩阵，进而用于特征选择</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_bostonboston=load_boston() </span><br><span class="line"></span><br><span class="line">scaler=StandardScaler() </span><br><span class="line">X=scaler.fit_transform(boston[<span class="string">"data"</span>])</span><br><span class="line">Y=boston[<span class="string">"target"</span>]</span><br><span class="line">names=boston[<span class="string">"feature_names"</span>]</span><br><span class="line">lasso=Lasso(alpha=<span class="number">.3</span>)</span><br><span class="line">lasso.fit(X,Y)</span><br><span class="line"><span class="keyword">print</span><span class="string">"Lasso model: "</span>,pretty_print_linear(lasso.coef_,names,sort=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><hr>
<p>L2正则化的标准形式</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/youdao/L2正则化公式.png?raw=true" alt=""></p>
<p>​    和L1正则化相同，<strong>任务转化为在L2的约束下求J0最小值的解</strong>。考虑二维情况，即只有两个权值w1和w2，此时L=|w1|+|w2|，对于梯度下降算法，求解j0的过程中画出等值线，同时将L1正则化的函数L也在w1、w2空间化出来，<strong>二者图像首次相交处即为最优解</strong>，获得下图：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/youdao/L2正则化不具有稀疏作用原因.png?raw=true" alt=""></p>
<h5 id="机器学习过程中权值尽可能小的原因："><a href="#机器学习过程中权值尽可能小的原因：" class="headerlink" title="机器学习过程中权值尽可能小的原因："></a>机器学习过程中权值尽可能小的原因：</h5><blockquote>
<p>试想对于一个模型，当参数很大时，只要数据偏移一点点，就会对结果造成很大的影响，如果参数较小，则数据偏移的多一点，也不会对结果产生多大的影响，<strong>抗扰动能力强</strong></p>
</blockquote>
<h4 id="为什么L2正则化可以使权值尽可能的小"><a href="#为什么L2正则化可以使权值尽可能的小" class="headerlink" title="为什么L2正则化可以使权值尽可能的小?"></a>为什么L2正则化可以使权值尽可能的小?</h4><blockquote>
<p>对于损失函数不带L2正则化项的梯度下降时参数更新公式为：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/不带正则化项的参数更新表达式.png?raw=true" alt=""></p>
<p>加入L2正则化项，参数更新公式为：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/带正则化项的参数更新公式.png?raw=true" alt=""></p>
<p>根据两个公式之间的差别，我们可以明显的看到，加入正则化以后的梯度下降在进行参数更新时，要先将原有的参数值乘以一个小于1的值，因此权值也会变得比不带的参数小</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/28/NLP-transformer模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/NLP-transformer模型/" itemprop="url">NLP-transformer模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T10:49:15+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    transformer模型来自于Google的经典论文<strong>Attention is all you need</strong>，在这篇论文中作者采用Attention来取代了全部的RNN、CNN，实现效果效率的双丰收。</p>
<p>​    现在transformer在NLP领域已经可以达到全方位吊打CNN、RNN系列的网络，网络处理时间效率高，结果稳定性可靠性都比传统的CNN、RNN以及二者的联合网络更好，因此现在已经呈现出了transformer逐步取代二者的大趋势。</p>
<p>​    下面是三者在下面四个方面的对比试验结果</p>
<p>​        <strong>1.远距离特征提取能力</strong></p>
<p>​        <strong>2.语义特征提取能力</strong></p>
<p>​        <strong>3.综合特征提取能力</strong></p>
<p>​        <strong>4.特征提取效率</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RNN%E3%80%81CNN%E3%80%81transformer%E9%95%BF%E8%B7%9D%E7%A6%BB%E7%89%B9%E5%BE%81%E6%8D%95%E8%8E%B7%E8%83%BD%E5%8A%9B%E5%AF%B9%E6%AF%94.png?raw=true" alt=""></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RNN%E3%80%81CNN%E3%80%81transformer%E8%AF%AD%E4%B9%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E8%83%BD%E5%8A%9B%E5%AF%B9%E6%AF%94.png?raw=true" alt=""></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RNN%E3%80%81CNN%E3%80%81Transformer%E7%BB%BC%E5%90%88%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E8%83%BD%E5%8A%9B%E5%AF%B9%E6%AF%94.png?raw=true" alt=""></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/RNN%E3%80%81CNN%E3%80%81Transformer%E4%B8%89%E8%80%85%E8%AE%A1%E7%AE%97%E6%95%88%E7%8E%87%E5%AF%B9%E6%AF%94.png?raw=true" alt=""></p>
<p>下面是从一系列的论文中获取到的RNN、CNN、Transformer三者的对比结论：    </p>
<p>​    <strong>1.从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。</strong></p>
<p>​    <strong>2.速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显。(主流经验上transformer和CNN速度差别不大，RNN比前两者慢3倍到几十倍)</strong></p>
<h3 id="Transformer模型具体细节"><a href="#Transformer模型具体细节" class="headerlink" title="Transformer模型具体细节"></a>Transformer模型具体细节</h3><p>​    transformer模型整体结构上主要<strong>Encoder</strong>和<strong>Decoder</strong>两部分组成，Encoder主要用来将数据进行特征提取，而Decoder主要用来实现隐向量解码出新的向量表示(原文中就是新的语言表示)，由于原文是机器翻译问题，而我们要解决的问题是类文本分类问题，因此我们直接减Transformer模型中的Encoder部分来进行特征的提取。其中主要包括下面几个核心技术模块：</p>
<p>​        <strong>1.残差连接</strong></p>
<p>​        <strong>2.Position-wise前馈网络</strong></p>
<p>​        <strong>3.多头self-attention</strong></p>
<p>​        <strong>4.位置编码</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/transformer%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.png?raw=true=20*50" alt=""></p>
<p>​    1.采用全连接层进行Embedding （Batch_size,src_vocab_size,model_dim）</p>
<p>​    2.在进行位置编码，位置编码和Embedding的结果进行累加</p>
<p>​    3.进入Encoder_layer进行编码处理(相当于特征提取)</p>
<p>​        (1)</p>
<p>​        </p>
<h4 id="1-位置编码（PositionalEncoding）"><a href="#1-位置编码（PositionalEncoding）" class="headerlink" title="1.位置编码（PositionalEncoding）"></a>1.位置编码（PositionalEncoding）</h4><p>​    大部分编码器一般都采用RNN系列模型来提取语义相关信息，但是采用RNN系列的模型来进行语序信息进行提取具有不可并行、提取效率慢等显著缺点，本文采用了一种 Positional Embedding方案来对于语序信息进行编码，主要通过正余弦函数，</p>
<p>​    <img src="/Users/yhk/Library/Application Support/typora-user-images/image-20190304162008847.png" alt="image-20190304162008847"></p>
<p>在偶数位置，使用正弦编码;在奇数位置使用余弦进行编码。</p>
<blockquote>
<p>为什么要使用三角函数来进行为之编码？</p>
<p>​    首先在上面的公式中可以看出，给定词语的pos可以很简单其表示为dmodel维的向量，也就是说位置编码的每一个位置每一个维度对应了一个波长从2π到10000*2π的等比数列的正弦曲线，也就是说可以表示各个各个位置的<strong>绝对位置</strong>。</p>
<p>​    在另一方面，词语间的相对位置也是非常重要的，这也是选用正余弦函数做位置编码的最主要原因。因为</p>
<p>​    sin(α+β) = sinαcosβ+cosαsinβ</p>
<p>​    cos(α+β) = cosαcosβ+sinαsinβ</p>
<p>​    因此对于词汇间位置偏移k，PE(pos+k)可以表示为PE(pos)和PE(k)组合的形式，也就是<strong>具有相对位置表达能力</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        位置编码层</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, max_seq_len)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            d_model: 一个标量。模型的维度，论文默认是512</span></span><br><span class="line"><span class="string">            max_seq_len: 一个标量。文本序列的最大长度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 根据论文给的公式，构造出PE矩阵</span></span><br><span class="line">        position_encoding = np.array([</span><br><span class="line">          [pos / np.power(<span class="number">10000</span>, <span class="number">2.0</span> * (j // <span class="number">2</span>) / d_model) <span class="keyword">for</span> j <span class="keyword">in</span> range(d_model)]</span><br><span class="line">          <span class="keyword">for</span> pos <span class="keyword">in</span> range(max_seq_len)])</span><br><span class="line">        <span class="comment"># 偶数列使用sin，奇数列使用cos</span></span><br><span class="line">        position_encoding[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_encoding[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">        position_encoding[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_encoding[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">        position_encoding = torch.Tensor(position_encoding)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding</span></span><br><span class="line">        <span class="comment"># 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似</span></span><br><span class="line">        <span class="comment"># 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，</span></span><br><span class="line">        <span class="comment"># 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码</span></span><br><span class="line">        pad_row = torch.zeros([<span class="number">1</span>, d_model])</span><br><span class="line">        position_encoding = torch.cat((pad_row, position_encoding))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，</span></span><br><span class="line">        <span class="comment"># Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似</span></span><br><span class="line">        self.position_encoding = nn.Embedding(max_seq_len + <span class="number">1</span>, d_model)</span><br><span class="line">        self.position_encoding.weight = nn.Parameter(position_encoding,</span><br><span class="line">                                                     requires_grad=<span class="keyword">False</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_len,max_len)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            神经网络的前向传播。</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。</span></span><br><span class="line"><span class="string">          param max_len:数值，表示当前的词的长度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          返回这一批序列的位置编码，进行了对齐。</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 找出这一批序列的最大长度</span></span><br><span class="line">        tensor = torch.cuda.LongTensor <span class="keyword">if</span> input_len.is_cuda <span class="keyword">else</span> torch.LongTensor</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对每一个序列的位置进行对齐，在原序列位置的后面补上0</span></span><br><span class="line">        <span class="comment"># 这里range从1开始也是因为要避开PAD(0)的位置</span></span><br><span class="line">        input_pos = tensor(</span><br><span class="line">          [list(range(<span class="number">1</span>, len + <span class="number">1</span>)) + [<span class="number">0</span>] * (max_len - len) <span class="keyword">for</span> len <span class="keyword">in</span> input_len.tolist()])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.position_encoding(input_pos)</span><br></pre></td></tr></table></figure>
<h4 id="2-scaled-Dot-Product-Attention"><a href="#2-scaled-Dot-Product-Attention" class="headerlink" title="2.scaled Dot-Product Attention"></a>2.scaled Dot-Product Attention</h4><p>​    <strong>scaled</strong>代表着在原来的dot-product Attention的基础上加入了缩放因子1/sqrt(dk)，dk表示Key的维度，默认用64.</p>
<blockquote>
<p>为什么要加入缩放因子？</p>
<p>​    在dk(key的维度)很大时，点积得到的结果维度很大，使的结果处于softmax函数梯度很小的区域，这是后乘以一个缩放因子，可以缓解这种情况的发生。</p>
</blockquote>
<p>​    <strong>Dot-Produc</strong>代表乘性attention，attention计算主要分为加性attention和乘性attention两种。加性 Attention 对于输入的隐状态 ht 和输出的隐状态 st直接做 concat 操作，得到 [ht:st] ，乘性 Attention 则是对输入和输出做 dot 操作。</p>
<p>​    <strong>Attention</strong>又是什么呢？通俗的解释Attention机制就是通过query和key的相似度确定value的权重。论文中具体的Attention计算公式为：</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/atttion%E8%AE%A1%E7%AE%97%E8%A1%A8%E8%BE%BE%E5%BC%8F.png?raw=true" alt=""></p>
<p>​    在这里采用的scaled Dot-Product Attention是self-attention的一种，self-attention是指Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。就是下面来具体说一下K、Q、V具体含义：</p>
<blockquote>
<ol>
<li>在一般的Attention模型中，Query代表要进行和其他各个位置的词做点乘运算来计算相关度的节点，Key代表Query亚进行查询的各个节点，每个Query都要遍历全部的K节点，计算点乘计算相关度，然后经过缩放和softmax进行归一化的到当前Query和各个Key的attention score，然后再使用这些attention score与Value相乘得到attention加权向量</li>
<li>在self-attention模型中，Key、Query、Value均来自相同的输入</li>
<li>在transformer的encoder中的Key、Query、Value都来自encoder上一层的输入，对于第一层encoder layer，他们就是word embedding的输出和positial encoder的加和。</li>
</ol>
</blockquote>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/scaled%20dot-product%20attention.png?raw=true" alt=""></p>
<blockquote>
<p>query、key、value来源：</p>
<p>​    他们三个是由原始的词向量X乘以三个权值不同的嵌入向量Wq、Wk、Wv得到的，三个矩阵尺寸相同</p>
<p><strong>Attention计算步骤：</strong></p>
<ol>
<li>如上文，将输入单词转化成嵌入向量；</li>
<li>根据嵌入向量得到 q、k、v三个向量；</li>
<li>为每个向量计算一个score： score = q*k</li>
<li>为了梯度的稳定，Transformer使用了score归一化，即除以 sqrt(dk)；</li>
<li>对score施以softmax激活函数；</li>
<li>softmax点乘Value值 v ，得到加权的每个输入向量的评分 v；</li>
<li>相加之后得到最终的输出结果Sum(z) ：  。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        标准的scaled点乘attention层</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, attention_dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(attention_dropout)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, scale=None, attn_mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        前向传播.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        	q: Queries张量，形状为[B, L_q, D_q]</span></span><br><span class="line"><span class="string">        	k: Keys张量，形状为[B, L_k, D_k]</span></span><br><span class="line"><span class="string">        	v: Values张量，形状为[B, L_v, D_v]，一般来说就是k</span></span><br><span class="line"><span class="string">        	scale: 缩放因子，一个浮点标量</span></span><br><span class="line"><span class="string">        	attn_mask: Masking张量，形状为[B, L_q, L_k]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        	上下文张量和attention张量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        attention = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> scale:</span><br><span class="line">            attention = attention * scale</span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># 给需要 mask 的地方设置一个负无穷</span></span><br><span class="line">            attention = attention.masked_fill(attn_mask,<span class="number">-1e9</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算softmax</span></span><br><span class="line">        attention = self.softmax(attention)</span><br><span class="line">        <span class="comment"># 添加dropout</span></span><br><span class="line">        attention = self.dropout(attention)</span><br><span class="line">        <span class="comment"># 和V做点积</span></span><br><span class="line">        context = torch.bmm(attention, v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context, attention</span><br></pre></td></tr></table></figure>
<h4 id="3-多头Attention"><a href="#3-多头Attention" class="headerlink" title="3.多头Attention"></a>3.多头Attention</h4><p>​    论文作者发现<strong>将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention</strong> 效果更好。<strong>然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出</strong>。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。</p>
<p>​    这里采用了四个全连接层+有个dot_product_attention层(也可以说是8个)+layer_norm实现。</p>
<blockquote>
<p>为什么要使用多头Attention？</p>
<p>​    1.”多头机制“能让模型考虑到不同位置的Attention</p>
<p>​    2.”多头“Attention可以在不同的足空间表达不一样的关联</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        多头Attention层</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_dim=<span class="number">512</span>, num_heads=<span class="number">8</span>, dropout=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.dim_per_head = model_dim // num_heads</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line"></span><br><span class="line">        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class="line">        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class="line">        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class="line"></span><br><span class="line">        self.dot_product_attention = ScaledDotProductAttention(dropout)</span><br><span class="line">        self.linear_final = nn.Linear(model_dim, model_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.layer_norm = nn.LayerNorm(model_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, key, value, query, attn_mask=None)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        residual = query</span><br><span class="line">        dim_per_head = self.dim_per_head</span><br><span class="line">        num_heads = self.num_heads</span><br><span class="line">        batch_size = key.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 线性层 (batch_size,word_nums,model_dim)</span></span><br><span class="line">        key = self.linear_k(key)</span><br><span class="line">        value = self.linear_v(value)</span><br><span class="line">        query = self.linear_q(query)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将一个切分成多个(batch_size*num_headers,word_nums,word//num_headers)</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            这里用到了一个trick就是将key、value、query值要进行切分不需要进行真正的切分，直接将其维度整合到batch_size上，效果等同于真正的切分。过完scaled dot-product attention 再将其维度恢复即可</span></span><br><span class="line"><span class="string">        """</span>       </span><br><span class="line">        key = key.view(batch_size * num_heads, <span class="number">-1</span>, dim_per_head)</span><br><span class="line">        value = value.view(batch_size * num_heads, <span class="number">-1</span>, dim_per_head)</span><br><span class="line">        query = query.view(batch_size * num_heads, <span class="number">-1</span>, dim_per_head)</span><br><span class="line">        <span class="comment">#将mask也复制多份和key、value、query相匹配  （batch_size*num_headers,word_nums_k,word_nums_q）</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            attn_mask = attn_mask.repeat(num_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用scaled-dot attention来进行向量表达</span></span><br><span class="line">        <span class="comment">#context:(batch_size*num_headers,word_nums,word//num_headers)</span></span><br><span class="line">        <span class="comment">#attention:(batch_size*num_headers,word_nums_k,word_nums_q)</span></span><br><span class="line">        scale = (key.size(<span class="number">-1</span>)) ** <span class="number">-0.5</span></span><br><span class="line">        context, attention = self.dot_product_attention(</span><br><span class="line">          query, key, value, scale, attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># concat heads</span></span><br><span class="line">        context = context.view(batch_size, <span class="number">-1</span>, dim_per_head * num_heads)</span><br><span class="line">        <span class="comment"># final linear projection</span></span><br><span class="line">        output = self.linear_final(context)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        output = self.dropout(output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用了残差连接和LN</span></span><br><span class="line">        output = self.layer_norm(residual + output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attention</span><br></pre></td></tr></table></figure>
<h4 id="4-残差连接"><a href="#4-残差连接" class="headerlink" title="4.残差连接"></a>4.残差连接</h4><p>​    在上面的多头的Attnetion中，还采用了残差连接机制来保证网络深度过深从而导致的精度下降问题。这里的思想主要来源于深度残差网络(ResNet)，残差连接指在模型通过一层将结果输入到下一层时也同时直接将不通过该层的结果一同输入到下一层，从而达到解决网络深度过深时出现精确率不升反降的情况。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/res-net.png?raw=true" alt=""></p>
<blockquote>
<p><strong>为什么残差连接可以在网络很深的时候防止出现加深深度而精确率下降的情况？</strong></p>
<p>​    神经网络随着深度的加深会会出现训练集loss逐渐下贱，趋于饱和，然后你再加深网络深度，训练集loss不降反升的情况。这是因为随着网络深度的增加，在深层的有效信息可能变得更加模糊，不利于最终的决策网络做出正确的决策，因此残差网络提出，建立残差连接的方式来将低层的信息也能传到高层，因此这样实现的深层网络至少不会比浅层网络差。</p>
</blockquote>
<h4 id="5-Layer-normalization"><a href="#5-Layer-normalization" class="headerlink" title="5.Layer normalization"></a>5.Layer normalization</h4><h5 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h5><p>​    Normalization 有很多种，但是它们都有一个<strong>共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据</strong>。我们在把数据送入激活函数之前进行 normalization（归一化），<strong>因为我们不希望输入数据落在激活函数的饱和区。</strong></p>
<p>#####Batch Normalization(BN)</p>
<p>​    应用最广泛的Normalization就是Batch Normalization，其主要思想是:<strong>在每一层的每一批数据上进行归一化</strong>。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。<strong>随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。</strong></p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Batch%20normalization%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt=""></p>
<h5 id="Layer-normalization-LN"><a href="#Layer-normalization-LN" class="headerlink" title="Layer normalization(LN)"></a>Layer normalization(LN)</h5><p>​    LN 是<strong>在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差</strong>.</p>
<blockquote>
<p>Layer normalization在pytorch 0.4版本以后可以直接使用nn.LayerNorm进行调用</p>
</blockquote>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/Batch%20normalization%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt=""></p>
<h4 id="6-Mask"><a href="#6-Mask" class="headerlink" title="6.Mask"></a>6.Mask</h4><p>​    <strong>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果</strong>。Transformer 模型里面涉及两种 mask，分别是 <strong>padding mask</strong> 和 <strong>sequence mask</strong>。</p>
<p>​    在我们使用的Encoder部分，只是用了padding mask因此本文重点介绍padding mask。 </p>
<h5 id="padding-mask"><a href="#padding-mask" class="headerlink" title="padding mask"></a>padding mask</h5><p>​    什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给<strong>在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上</strong>，所以我们需要进行一些处理。<strong>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</strong>而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span><span class="params">(seq_k, seq_q)</span>:</span></span><br><span class="line">    <span class="string">"""        </span></span><br><span class="line"><span class="string">        param seq_q:(batch_size,word_nums_q)</span></span><br><span class="line"><span class="string">        param seq_k:(batch_size,word_nums_k)</span></span><br><span class="line"><span class="string">        return padding_mask:(batch_size,word_nums_q,word_nums_k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seq_k和seq_q 的形状都是 (batch_size,word_nums_k)</span></span><br><span class="line">    len_q = seq_q.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 找到被pad填充为0的位置(batch_size,word_nums_k)</span></span><br><span class="line">    pad_mask = seq_k.eq(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#(batch_size,word_nums_q,word_nums_k)</span></span><br><span class="line">    pad_mask = pad_mask.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, len_q, <span class="number">-1</span>)  <span class="comment"># shape [B, L_q, L_k]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pad_mask</span><br></pre></td></tr></table></figure>
<p>p</p>
<p>####3.Position-wise 前馈网络</p>
<p>​    这是一个全连接网络，包含两个线性变换和一个非线性函数(实际上就是 ReLU)</p>
<p>​    <img src="https://github.com/AnchoretY/images/blob/master/blog/Position-wise%20Feed-Forward%20network%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F.png?raw=true" alt=""></p>
<p><strong>这里实现上用到了两个一维卷积。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class PositionalWiseFeedForward(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">        前向编码，使用两层一维卷积层实现</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):</span><br><span class="line">        super(PositionalWiseFeedForward, self).__init__()</span><br><span class="line">        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)</span><br><span class="line">        self.w2 = nn.Conv1d(ffn_dim, model_dim, 1)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(model_dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        output = x.transpose(1, 2)</span><br><span class="line">        output = self.w2(F.relu(self.w1(output)))</span><br><span class="line">        output = self.dropout(output.transpose(1, 2))</span><br><span class="line"></span><br><span class="line">        # add residual and norm layer</span><br><span class="line">        output = self.layer_norm(x + output)</span><br><span class="line">        return output</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/27/面试总结-Python和C语言中的一些不同/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/面试总结-Python和C语言中的一些不同/" itemprop="url">面试总结-Python和C语言中的一些不同</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T21:18:33+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###1.python在除法和C语言中的一点区别</p>
<p>​    在Python3中，除法有 “/” 以及 “//” 两种，这两个有着明显的区别，具体区别看代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="number">12</span>//<span class="number">10</span>)</span><br><span class="line">print(<span class="number">12</span>/<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>这两行代码的输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1.2</span></span><br></pre></td></tr></table></figure>
<p>当被除数是负数的时候，又是另一种情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="number">-12</span>/<span class="number">10</span>)      <span class="comment">#不补整</span></span><br><span class="line">print(int(<span class="number">-12</span>/<span class="number">10</span>)) <span class="comment">#向正方向进行补整</span></span><br><span class="line">print(<span class="number">-13</span>//<span class="number">10</span>)     <span class="comment">#向负方向进行补整</span></span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">    <span class="number">-1.2</span></span><br><span class="line">    <span class="number">-1</span></span><br><span class="line">    <span class="number">-2</span></span><br></pre></td></tr></table></figure>
<p>​    因此，综合前面的正负两种情况，我们可以看出<strong>当我们想要达到和C++同样的向上取整，只能使用int(a/b)方式。</strong></p>
<h3 id="2-python在求余时和C的一点区别"><a href="#2-python在求余时和C的一点区别" class="headerlink" title="2.python在求余时和C的一点区别"></a>2.python在求余时和C的一点区别</h3><p>​    对于正数求余运算，python和C++完全相同，但是对于负数求余运算，python和C++存在着较大的差别，下面我们通过例子来说明二者的差别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#C++</span></span><br><span class="line">count&gt;&gt;<span class="number">-123</span>%<span class="number">10</span>;</span><br><span class="line">output:</span><br><span class="line">    <span class="number">-3</span></span><br><span class="line"><span class="comment">#python</span></span><br><span class="line">print(<span class="number">-123</span>%<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">    <span class="number">-7</span>    <span class="comment">#这里是向下取10的余数</span></span><br></pre></td></tr></table></figure>
<p>​    为了实现和C++相同效果的取余运算，我们只能采用如下方式进行取余运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> a&gt;=<span class="number">0</span></span><br><span class="line">	print(a%<span class="number">10</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	print(a%<span class="number">-10</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/10/机试——排序算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/10/机试——排序算法总结/" itemprop="url">机试——排序算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-10T09:16:10+08:00">
                2019-02-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​    机试中，排序算法是主要面临的一类算法，很久都没有接触机试的题了，解决的时候感觉有点思路不是很清楚了，因此写了这一片博客，来整理下常见的排序算法以及各种常见算法的效率稳定性等特点。</p>
<blockquote>
<p>在机试中常用的排序算法主要包含下面几种：</p>
<p>​    1.插入排序</p>
<p>​    2.选择排序</p>
<p>​    3.快速排序(最常用的排序)</p>
<p>​    4.冒泡排序</p>
<p>​    5.归并排序</p>
<p>​    6.桶排序</p>
</blockquote>
<p>下面我将具体介绍各种排序算法：</p>
<h4 id="1-插入排序"><a href="#1-插入排序" class="headerlink" title="1.插入排序"></a>1.插入排序</h4><p>​    每次从头到尾选择一个元素，并且将这个元素和整个数组中的所有已经排序的元素进行比较，然后插入到合适的位置。</p>
<p>​    注意：插入排序的核心点就是两两比较时从后向前进行比较，如果比插入值大，那么将其向后移动，直到找到比插入值小的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertion_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    length = len(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,length):     <span class="comment">#从第一个元素开始依次进行排序</span></span><br><span class="line">        tmp = arr[i]</span><br><span class="line">        j = i</span><br><span class="line">        <span class="keyword">while</span> arr[j<span class="number">-1</span>]&gt;tmp <span class="keyword">and</span> j&gt;<span class="number">0</span>:  <span class="comment">#从当前元素从后向前向前开始遍历，寻找第一个比当前元素更小的元素</span></span><br><span class="line">            arr[j] = arr[j<span class="number">-1</span>]		<span class="comment">#再找比当前小的元素位置的同时，只要扫描到的位置比当前元素大，那么将该元素后移一维</span></span><br><span class="line">            j -= <span class="number">1</span></span><br><span class="line">        arr[j] = tmp</span><br><span class="line">   <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<p><strong>稳定性：稳定</strong></p>
<p><strong>时间复杂度：O(n^2)</strong></p>
<p><strong>空间复杂度：O(1)</strong></p>
<blockquote>
<p>为什么插入排序是稳定的排序算法？</p>
<p>​    当前从头到尾选择元素进行排序时，当选择到第i个元素时，前i-1个元素已经排好了续，取出第i个元素，从i-1开始向前开始比较，如果小于，则将该位置元素向后移动，继续先前的比较，如果不小于，那么将第i个元素放在当前比较的元素之后。</p>
</blockquote>
<h4 id="2-选择排序"><a href="#2-选择排序" class="headerlink" title="2.选择排序"></a>2.选择排序</h4><p>​    选择排序主要采用了从头到尾依次确定各个位置的方式来进行排序，首先遍历一次整个数组，如果遇到比第一个元素小的元素那么交换位置，一次遍历完成那么第一个位置就已经是整个数组中最小的元素了，经过n次遍历，确定全部位置的元素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    length = len(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i,length):</span><br><span class="line">            <span class="keyword">if</span> arr[i]&gt;arr[j]:</span><br><span class="line">                tmp = arr[i]</span><br><span class="line">                arr[i] = arr[j]</span><br><span class="line">                arr[j] = tmp</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<p><strong>稳定性：不稳定</strong></p>
<p><strong>时间复杂度：O(n^2)</strong></p>
<p><strong>空间复杂度：O(1)</strong></p>
<h4 id="3-冒泡排序"><a href="#3-冒泡排序" class="headerlink" title="3.冒泡排序"></a>3.冒泡排序</h4><p>​    冒泡排序额是实现是不停地进行两两比较，将较大的元素换到右侧，然后继续进行两两比较，直到比较完全全部元素，<strong>每进行完一轮两两比较，确定一个元素的位置</strong>。例如：第一轮两两比较确定最大的值，第二轮比较确定次大元素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    length = len(arr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,length):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,length-i):</span><br><span class="line">            <span class="keyword">if</span> arr[j]&lt;arr[j<span class="number">-1</span>]:</span><br><span class="line">                tmp = arr[j]</span><br><span class="line">                arr[j] = arr[j<span class="number">-1</span>]</span><br><span class="line">                arr[j<span class="number">-1</span>] = tmp</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<p><strong>稳定性：稳定</strong></p>
<p><strong>时间复杂度：O(n^2)</strong></p>
<p><strong>空间复杂度：O(1)</strong></p>
<blockquote>
<p>冒泡排序在原始冒泡排序算法的基础上还能做哪些优化？</p>
<p>​    1.设置是否已经排好序的flag。如果在某一轮的便利中没有出现任何的交换发生，这说明已经都排好序,那么直接将flag置True，每轮结束时检测flag，如果为True则直接返回</p>
<p>​    2.某一轮的结束为止为j，但这一轮最后一次交换发生在lastSwap位置，那么说明lastSwap到j之间已经排好序，下次遍历的结束点就不需要再到j—而是直接到lastSwap即可。</p>
</blockquote>
<h4 id="4-快速排序"><a href="#4-快速排序" class="headerlink" title="4.快速排序"></a>4.快速排序</h4><p>​    快速排序的的主要思想是先找到一个任意一个元素作为基准元素pivot（一般都采用第一个元素作为基准），然后从右向左搜索，如果发现比pivot小，那么和pivot交换,然后从右向左进行搜索，如果发现比pviot大，那么进行交换，遍历一轮后pivot左边的元素都比它小，右边的元素都比他大，<strong>此时pivot的位置就是排好序后他也应该在的位置。</strong>然后继续用递归算法分别处理pivot左边的元素和右边的元素。</p>
<h5 id="对于大的乱序数据快速排序被认为是最快速的排序方式"><a href="#对于大的乱序数据快速排序被认为是最快速的排序方式" class="headerlink" title="对于大的乱序数据快速排序被认为是最快速的排序方式"></a>对于大的乱序数据快速排序被认为是最快速的排序方式</h5><p>d</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式一：递归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(arr,l,r)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(l&lt;r):</span><br><span class="line">        q = mpartition(arr,l,r)</span><br><span class="line">        quick_sort(arr,l,q<span class="number">-1</span>)    <span class="comment">#前面经过一次mpartion后q位置已经排好序，因此递归时两部分跳过q位置</span></span><br><span class="line">        quick_sort(arr,q+<span class="number">1</span>,r)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mpartition</span><span class="params">(arr,l,r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    	递归子函数，povit放到指定位置</span></span><br><span class="line"><span class="string">    	return l:最终标志元素被放置的位置，本轮确定了的元素位置</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    poviot = arr[l]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> l&lt;r:</span><br><span class="line">        <span class="keyword">while</span> l&lt;r <span class="keyword">and</span> arr[r]&gt;=poviot:</span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> l&lt;r:</span><br><span class="line">            arr[l] = arr[r]</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> l&lt;r <span class="keyword">and</span> arr[l]&lt;poviot:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> l&lt;r:</span><br><span class="line">            arr[r] = arr[l]</span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    arr[l]  = poviot</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式二：非递归，利用栈</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(nums,low,high)</span>:</span></span><br><span class="line">    <span class="comment">#确定nums数组中指定部分low元素的位置，左边都比它小，右边都比他大</span></span><br><span class="line">    </span><br><span class="line">    pivot = nums[low]</span><br><span class="line">    high_flag = <span class="keyword">True</span>   <span class="comment">#这里之所以设置这两个flag是为了确保交叉进行，否则可能会出现最大索引值处没有值或者最大索引值处一直付给各个low</span></span><br><span class="line">    low_flag = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> low&lt;high <span class="keyword">and</span> low&lt;len(nums) <span class="keyword">and</span> high&lt;len(nums):</span><br><span class="line">        <span class="keyword">if</span> high_flag:</span><br><span class="line">            <span class="keyword">if</span> nums[high]&lt;pivot:</span><br><span class="line">                nums[low]=nums[high]</span><br><span class="line">                high_flag = <span class="keyword">False</span></span><br><span class="line">                low_flag = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> low_flag:</span><br><span class="line">            <span class="keyword">if</span> nums[low]&gt;pivot:</span><br><span class="line">                nums[high] = nums[low]</span><br><span class="line">                low_flag = <span class="keyword">False</span></span><br><span class="line">                high_flag = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low += <span class="number">1</span></span><br><span class="line">    nums[low] = pivot     </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> low</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = len(nums)<span class="number">-1</span></span><br><span class="line">    stack = []    <span class="comment">#存储每次遍历起始索引和结束索引</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> low&lt;high:</span><br><span class="line">        <span class="comment">#先手动将找到第一个节点的最终位置，将原数组分为左右两个数组，分别左右索引入栈</span></span><br><span class="line">        mid = partition(nums,low,high)</span><br><span class="line">        <span class="keyword">if</span> low&lt;mid<span class="number">-1</span>:</span><br><span class="line">            stack.append(low)</span><br><span class="line">            stack.append(mid<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> high&gt;mid+<span class="number">1</span>:</span><br><span class="line">            stack.append(mid+<span class="number">1</span>)</span><br><span class="line">            stack.append(high)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#取出之前入栈的一个数组，来进行确定最终位置，分为左右两个子数组，分别左右索引入栈的操作，重复直到所有元素都已经排好序</span></span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            <span class="comment">#这里写的是属于右半部都排好后左半部</span></span><br><span class="line">            r = stack.pop()</span><br><span class="line">            l = stack.pop()</span><br><span class="line">            mid = partition(nums,l,r)</span><br><span class="line">            <span class="keyword">if</span> l&lt;mid<span class="number">-1</span>:</span><br><span class="line">                stack.append(l)</span><br><span class="line">                stack.append(mid<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">if</span> r&gt;mid+<span class="number">1</span>:</span><br><span class="line">                stack.append(mid+<span class="number">1</span>)</span><br><span class="line">                stack.append(r)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<p><strong>稳定性：不稳定</strong>（排序过程中不停地交换元素位置造成了排序算法不稳定）</p>
<p><strong>时间复杂度：</strong></p>
<p>​    <strong>平均时间O(nlogn)</strong></p>
<p>​    <strong>最坏情况：O(n^2)</strong></p>
<p><strong>空间复杂度：O(nlogn)</strong></p>
<h4 id="5-归并排序"><a href="#5-归并排序" class="headerlink" title="5.归并排序"></a>5.归并排序</h4><p>​    </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/21/深度学习——Attion相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/21/深度学习——Attion相关/" itemprop="url">深度学习——Attion相关</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-21T14:54:55+08:00">
                2019-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>通过encoder，把 <img src="https://www.zhihu.com/equation?tex=X%3D%28x_0%2Cx_1%2Cx_2%2Cx_3%29" alt="X=(x_0,x_1,x_2,x_3)"> 映射为一个隐层状态 <img src="https://www.zhihu.com/equation?tex=H%3D%28h_0%2Ch_1%2Ch_2%2Ch_3%29" alt="H=(h_0,h_1,h_2,h_3)"> ，再经由decoder将 <img src="https://www.zhihu.com/equation?tex=H%3D%28h_0%2Ch_1%2Ch_2%2Ch_3%29" alt="H=(h_0,h_1,h_2,h_3)"> 映射为 <img src="https://www.zhihu.com/equation?tex=Y%3D%28y_0%2Cy_1%2Cy_2%29" alt="Y=(y_0,y_1,y_2)"> 。这里精妙的地方就在于，Y中的每一个元素都与H中的所有元素相连，而<strong>每个元素通过不同的权值给与Y不同的贡献</strong>。</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/attention.png?raw=true" alt=""></p>
<p>1.关键在于红框里面的部分，即attention，后面再讲。</p>
<p>2.红框下面是encoder，输入 <img src="https://www.zhihu.com/equation?tex=X%3D%28x_0%2Cx_1%2Cx_2%2C...%2Cx_%7BT_x%7D%29" alt="X=(x_0,x_1,x_2,...,x_{T_x})">，通过一个双向LSTM得到两组<img src="https://www.zhihu.com/equation?tex=h%5E%5Cleftarrow" alt="h^\leftarrow">和 <img src="https://www.zhihu.com/equation?tex=h%5E%5Crightarrow" alt="h^\rightarrow">（向左和向右），然后concat起来得到最终的 <img src="https://www.zhihu.com/equation?tex=H%3D%28h_0%2Ch_1%2Ch_2%2C...%2C+h_%7BT_x%7D%29" alt="H=(h_0,h_1,h_2,..., h_{T_x})">。</p>
<p>3.红框上面是decoder。以 <img src="https://www.zhihu.com/equation?tex=t" alt="t">时刻为例，输入共有三个： <img src="https://www.zhihu.com/equation?tex=s_%7Bt-1%7D" alt="s_{t-1}">， <img src="https://www.zhihu.com/equation?tex=y_%7Bt-1%7D" alt="y_{t-1}">， <img src="https://www.zhihu.com/equation?tex=c_t" alt="c_t">。</p>
<p>​    其中,<img src="https://www.zhihu.com/equation?tex=s_%7Bt-1%7D" alt="s_{t-1}">是上一个时刻的hidden state（一般用 <img src="https://www.zhihu.com/equation?tex=h" alt="h">表示encoder的hidden state，用 <img src="https://www.zhihu.com/equation?tex=s" alt="s">表示decoder的hidden state）；</p>
<p>​     <img src="https://www.zhihu.com/equation?tex=y_%7Bt-1%7D+" alt="y_{t-1} ">是上一个时刻的输出，用来当做这个时刻的输入； </p>
<p>​    <img src="https://www.zhihu.com/equation?tex=c_t+" alt="c_t ">在图中没有，就是红框里得到的加权和，叫做context，即<strong>由所有的encoder output（即 <img src="https://www.zhihu.com/equation?tex=h" alt="h">，不定长）得到一个定长的向量，代表输入序列的全局信息</strong>，作为当前decoder step的context（上下文）。</p>
<p>​        计算方法为： <img src="https://www.zhihu.com/equation?tex=c_i%3D%5Csum_%7Bj%3D1%7D%5E%7BT_x%7D%5Calpha_%7Bij%7Dh_j" alt="c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j">，其中<img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D" alt="\alpha_{ij}">是权重，又称为<strong>alignment</strong>；</p>
<p><img src="https://www.zhihu.com/equation?tex=h" alt="h">就是encoder所有step的hidden state，又叫做<strong>value</strong>或者<strong>memory</strong>； <img src="https://www.zhihu.com/equation?tex=i" alt="i">代表decoder step， <img src="https://www.zhihu.com/equation?tex=j" alt="j">代表encoder step。</p>
<p>那么<strong>权重矩阵又是怎么来的</strong>呢？</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bij%7D%3D%5Cfrac%7B%5Cexp%28e_%7Bij%7D%29%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BT_x%7D%5Cexp%28e_%7Bik%7D%29%7D" alt="\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{k=1}^{T_x}\exp(e_{ik})}"></p>
<p>​    其中 <img src="https://www.zhihu.com/equation?tex=e_%7Bij%7D%3Da%28s_%7Bi-1%7D%2Ch_j%29" alt="e_{ij}=a(s_{i-1},h_j)"></p>
<p>其实上面的公式就是对e的求softmax，因为我们要计算一组权重，而这组权重的和为1；下面的e的表达式就是代表着对<img src="https://www.zhihu.com/equation?tex=s_%7Bi-1%7D" alt="s_{i-1}"> 和 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 的<strong>相关程度</strong>进行计算的函数，<strong>即对于某个既定的decoder step，计算上个时刻的hidden state和所有encoder step的输出的相关程度，并且用softmax归一化；这样，相关度大的 <img src="https://www.zhihu.com/equation?tex=h" alt="h">权重就大，在整个context里占得比重就多，decoder在当前step做解码的时候就越重视它（这就是attention的思想）</strong>。</p>
<p>具体的相关程度的计算，主要实现方式</p>
<p>对 <img src="https://www.zhihu.com/equation?tex=s_%7Bi-1%7D" alt="s_{i-1}"> 做一个线性映射，得到的向量叫做<strong>query</strong>，记做 <img src="https://www.zhihu.com/equation?tex=q_i" alt="q_i"> ；</p>
<p>对 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j"> 做一个线性映射，得到的向量叫做<strong>key</strong>，记做 <img src="https://www.zhihu.com/equation?tex=k_j" alt="k_j"> ；</p>
<p><img src="https://www.zhihu.com/equation?tex=e_%7Bij%7D%3Dv%5ET+%5Ccdot+%28q_i%2Bk_j%29" alt="e_{ij}=v^T \cdot (q_i+k_j)"> 。<img src="https://www.zhihu.com/equation?tex=k_j" alt="k_j">和<img src="https://www.zhihu.com/equation?tex=q_i" alt="q_i">的维度必须相同，记为 <img src="https://www.zhihu.com/equation?tex=d" alt="d"> ； <img src="https://www.zhihu.com/equation?tex=v" alt="v"> 是一个 <img src="https://www.zhihu.com/equation?tex=d+%5Ctimes+1" alt="d \times 1"> 的向量。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/08/python进阶-面向对象编程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AnchoretY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AnchoretY's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/08/python进阶-面向对象编程/" itemprop="url">python进阶-面向对象编程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-08T19:24:57+08:00">
                2019-01-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>1.__slots__</p>
<p>​    用于指定class 实例能够指定的属性</p>
<blockquote>
<p>注意：__slots__只对当前类起作用，对其子类无效</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Myclass</span><span class="params">(object)</span>:</span></span><br><span class="line">	__slots__ = [<span class="string">"name"</span>,<span class="string">"set_name]</span></span><br><span class="line"><span class="string">	</span></span><br><span class="line"><span class="string">s = MyClass()</span></span><br><span class="line"><span class="string">s.name = "</span>john<span class="string">" #这里可以进行正常的赋值，因为包含在__slots__中</span></span><br><span class="line"><span class="string">try:</span></span><br><span class="line"><span class="string">	s.age = 2	#这里不能进行正常赋值</span></span><br><span class="line"><span class="string">except AttributeError:</span></span><br><span class="line"><span class="string">	traceback.print_exc()</span></span><br></pre></td></tr></table></figure>
<p>Output:</p>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/__slots__.png?raw=true" alt=""></p>
<p><strong>2.@property属性</strong></p>
<p>​    @property 可以实现比较方便的属性set、get设置</p>
<blockquote>
<p>1.使用@property相当于讲将一个函数变为get某个属性值<br>2.@属性名称.setter可以实现设置一个属性的set条件</p>
</blockquote>
<p>​    使用上面的两种修饰符，可以实现</p>
<p>​        1.对写入属性的限制，只有符合规范的才允许写入</p>
<p>​        2.设置只读属性，只能够读取，不能写入，只能从其他属性处计算出</p>
<p>下面的就是对score属性的写操作进行了一些限制，将double_score属性设置为只读属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._score</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @score.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self,value)</span>:</span></span><br><span class="line">        <span class="comment">#不是int类型时引发异常</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(value,int):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"not int"</span>)    <span class="comment">#raise的作用是显示的引发异常</span></span><br><span class="line">        <span class="comment">#超出范围时引发异常</span></span><br><span class="line">        <span class="keyword">elif</span> (value&lt;<span class="number">0</span>) <span class="keyword">or</span> (value&gt;<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"score must in 0 to 100"</span>)</span><br><span class="line">        </span><br><span class="line">        self._score = value</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">double_score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._score*<span class="number">2</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">s = MyClass()</span><br><span class="line">s.score = <span class="number">3</span></span><br><span class="line">print(s.score)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    s.score = <span class="number">2300</span></span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    traceback.print_exc()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    s.score = <span class="string">"dfsd"</span></span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    traceback.print_exc()</span><br><span class="line">    </span><br><span class="line">print(s.double_score)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    s.double_score = <span class="number">2</span></span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    traceback.print_exc()</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/AnchoretY/images/blob/master/blog/@property_2.png?raw=true" alt=""></p>
<p>描述器，主要是用来读写删除类的行为</p>
<p>  函数可以直接使用__name__属性来获取函数名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">now</span><span class="params">()</span>:</span></span><br><span class="line">	print(<span class="string">"2012"</span>)</span><br><span class="line"></span><br><span class="line">print(now.__name__)</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">	<span class="string">"now"</span></span><br></pre></td></tr></table></figure>
<p>​    </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">AnchoretY</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <!-- 底部导航栏相关 -->
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AnchoretY</span>

  
</div>

<!-- 添加底部导航栏-->
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
